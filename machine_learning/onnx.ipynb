{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Výroba a nasazení ONNX modelů  \n",
    "Ve vedlejším notebooku **machine_learning** jsme se věnovali vytváření scikit-learnovských modelů. Jen okrajově jsme ale zmínili, jak vlastně natrénovaný model posléze používat. Nejjednodušší možností je jeho uložení s pomocí balíčku pickle (resp. joblib) a následné nahrání ať již obyčejným skriptem, anebo flaskovou webovou servisou. Nicméně co dělat, když vyvstane nutnost model nativně použít v javovské webaplikaci? Tehdy nezbývá než převést scikit-learnovský model do nějakého obecnějšího formátu, se kterým se dá pracovat ve více programovacích jazycích. Takovým formátem může být ONNX alias Open Neural Network Exchange. Právě otázce, jak zkonvertovat scikit-learnovký model na ONNX model a jak s takovýmto modelem pracovat jak v Pythonu, tak v Javě, se budě věnovat naše dnešní povídání.  \n",
    "Aby nedošlo k nedorozumění - i když ta dvě písmena N v ONNX znamenají Neural Network, neuronové sítě zde dnes řešit nebudeme. Ne že by u nich tento problém neexistoval - koneckonců základní pytorchí ukládání modelů stojí na picklu (viz [zde](https://pytorch.org/tutorials/beginner/saving_loading_models.html)). Tensorflow pravda tohle asi tolik netrápí, neboť pro ukládání modelů používá na jazyku nezávislý formát [HFD5](https://www.tensorflow.org/tutorials/keras/save_and_load) a koneckonců má i svou [javovskou mutaci](https://www.tensorflow.org/install/lang_java?hl=en). Každopádně modely vyrobené jak Pytorchem, tak Tensorflowem lze na ONNX také převádět. Nicméně jelikož nepočítám s tím, že bych v nejbližší budoucnosti běh neuronových sítí v Javě musel řešit, nemá ani smysl, abych zde k tomu něco psal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jednoduchý model - vytvoření  v Pythonu\n",
    "Zkusme pro začátek do ONNX převést jeden z nejprovařenějších modelů - klasifikaci kosatců. Zdůrazněme, že zde existují čtyři prediktory stejného datového typu, což - jak se později ukáže - situaci zjednodušuje.  \n",
    "Nejprve začneme importem potřebných balíčků. Krom obvyklých podezřelých se nám zde objevují dvě nová jména - **onnxruntime** a **skl2onnx**. Jak už název napovídá, skl2onnx převádí scikit-learnový model na ONNX model. Balíček onnxruntime pak bude sloužit k práci s takto vzniklým modelem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T14:49:30.473308Z",
     "start_time": "2021-03-28T14:48:50.083757Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import skl2onnx\n",
    "import onnxruntime\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kosatcová data vezmeme z scikit-learnu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T19:49:27.046244Z",
     "start_time": "2021-03-27T19:49:27.014787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "73                 6.1               2.8                4.7               1.2   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "118                7.7               2.6                6.9               2.3   \n",
       "\n",
       "     target  \n",
       "73        1  \n",
       "18        0  \n",
       "118       2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data_heap = load_iris()\n",
    "iris_dataframe = pd.DataFrame(iris_data_heap[\"data\"], columns=iris_data_heap[\"feature_names\"])\n",
    "iris_dataframe[\"target\"] = pd.Series(iris_data_heap[\"target\"])\n",
    "iris_dataframe.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby ale situace nebyla od reality odtržená až tolik, náš model bude fakticky pipelina složená ze tří kroků včetně u těchto dat fakticky nepotřebného imputování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T19:52:36.084136Z",
     "start_time": "2021-03-27T19:52:35.813670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('minmaxscaler', MinMaxScaler()),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    MinMaxScaler(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "final_pipeline.fit(\n",
    "    iris_dataframe.drop(\"target\", axis=1), \n",
    "    iris_dataframe[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T19:54:15.273753Z",
     "start_time": "2021-03-27T19:54:15.258549Z"
    }
   },
   "source": [
    "Jelikož by mohlo být zajímavé srovnat složitost práce s obyčejným picklem a ONNXem, uložme model v obou formátech.  \n",
    "U picklu je tato operace triviální:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T19:54:39.161835Z",
     "start_time": "2021-03-27T19:54:39.139658Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"iris_pickle.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(final_pipeline, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ONNX musíme před uložením použít konverzi. Konverzní funkce **skl2onnx.convert_sklearn** kromě původního modelu potřebuje i datový typ prediktorů. Přesněji potřebuje dvojici jméno vstupu a vstupní tenzor o specifikovaném datovém typu a rozměrech. Konkrétně tedy u rozměru má smysl uvést (coby druhý element listu) počet prediktorů.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:22:06.992860Z",
     "start_time": "2021-03-27T20:22:06.861169Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_features = 4\n",
    "initial_type = [\n",
    "    (\"float_input\", skl2onnx.common.data_types.FloatTensorType([None, number_of_features]))\n",
    "]\n",
    "converted_model = skl2onnx.convert_sklearn(final_pipeline, initial_types=initial_type)\n",
    "with open(\"iris_onnx.onnx\", \"wb\") as model_file:\n",
    "    model_file.write(converted_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načtěme si nyní zapicklovaný model a zkusme ho použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:11:47.191874Z",
     "start_time": "2021-03-27T20:11:47.169738Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"iris_pickle.pkl\", \"rb\") as model_file:\n",
    "    loaded_iris_pickle_model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve na jeden záznam..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:14:27.530291Z",
     "start_time": "2021-03-27T20:14:27.501757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled model prediction for one flower: [1]\n"
     ]
    }
   ],
   "source": [
    "one_flower = np.array([[6.1, 2.8, 4.7, 1.2]])\n",
    "\n",
    "one_flower_pickle_pred = loaded_iris_pickle_model.predict(one_flower)\n",
    "print(f\"Pickled model prediction for one flower: {one_flower_pickle_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pak na několik záznamů najednou..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:17:15.870557Z",
     "start_time": "2021-03-27T20:17:15.854931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled model prediction for several flowers: [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "several_flowers = iris_dataframe.sample(3, random_state=42).drop(\"target\", axis=1)\n",
    "\n",
    "several_flowers_pickle_pred = loaded_iris_pickle_model.predict(several_flowers)\n",
    "print(f\"Pickled model prediction for several flowers: {several_flowers_pickle_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívejme se na pravděpodobnosti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:19:01.608358Z",
     "start_time": "2021-03-27T20:19:01.592775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled model probability prediction for several flowers:\n",
      "[[8.73950214e-02 6.29196011e-01 2.83408968e-01]\n",
      " [8.73572675e-01 1.19048866e-01 7.37845927e-03]\n",
      " [6.53072296e-04 1.23856107e-01 8.75490820e-01]]\n"
     ]
    }
   ],
   "source": [
    "several_flowers_pickle_prob = loaded_iris_pickle_model.predict_proba(several_flowers)\n",
    "print(f\"Pickled model probability prediction for several flowers:\\n{several_flowers_pickle_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To samé nyní provedeme pro ONNX model. Nejprve ho načteme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:22:10.541720Z",
     "start_time": "2021-03-27T20:22:09.387952Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_iris_onnx_model = onnxruntime.InferenceSession(\"iris_onnx.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro predikování použijeme metodu **run**. Té musíme podhodit jednak jméno výstupu (lze získat pomocí metody **get_outputs**), jednak jméno vstupu (lze získat skrze metodu **get_inputs**) a v neposlední řadě vstupní data zkonvertovaná do očekávaného formátu vstupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:25:40.687046Z",
     "start_time": "2021-03-27T20:25:40.301626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_name = float_input, output_name = output_label\n",
      "ONNX model prediction for one flower: [1]\n"
     ]
    }
   ],
   "source": [
    "input_name = loaded_iris_onnx_model.get_inputs()[0].name\n",
    "output_name = loaded_iris_onnx_model.get_outputs()[0].name\n",
    "print(f\"input_name = {input_name}, output_name = {output_name}\")\n",
    "\n",
    "one_flower_onnx_pred = loaded_iris_onnx_model.run(\n",
    "    [output_name], \n",
    "    {input_name: one_flower.astype(np.float32)}\n",
    ")[0]\n",
    "print(f\"ONNX model prediction for one flower: {one_flower_onnx_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predikce více záznamů vypadá úplně stejně. Jen tedy musíme napřed zkonvertovat pandí dataframe na numpoidní pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:34:03.796736Z",
     "start_time": "2021-03-27T20:34:03.765459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model prediction for several flowers: [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "several_flowers_numpy = several_flowers.to_numpy()\n",
    "\n",
    "several_flowers_onnx_pred = loaded_iris_onnx_model.run(\n",
    "    [output_name], \n",
    "    {input_name: several_flowers_numpy.astype(np.float32)}\n",
    ")[0]\n",
    "print(f\"ONNX model prediction for several flowers: {several_flowers_onnx_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro získání pravděpodobnosti musíme použít druhou část dvousložkové návratové hodnoty metody get_outputs. Jinak ale kód vypadá stejně jako předtím."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T20:37:57.783199Z",
     "start_time": "2021-03-27T20:37:57.767578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proba_name = output_probability\n",
      "ONNX model probability prediction for several flowers:\n",
      "[{0: 0.08739500492811203, 1: 0.6291959881782532, 2: 0.2834089994430542}, {0: 0.8735726475715637, 1: 0.11904889345169067, 2: 0.0073784636333584785}, {0: 0.0006530721439048648, 1: 0.12385611236095428, 2: 0.8754908442497253}]\n"
     ]
    }
   ],
   "source": [
    "proba_name = loaded_iris_onnx_model.get_outputs()[1].name\n",
    "print(f\"proba_name = {proba_name}\")\n",
    "\n",
    "several_flowers_onnx_pred = loaded_iris_onnx_model.run(\n",
    "    [prob_name], \n",
    "    {input_name: several_flowers_numpy.astype(np.float32)}\n",
    ")[0]\n",
    "print(f\"ONNX model probability prediction for several flowers:\\n{several_flowers_onnx_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jednoduchý model - použití v Javě"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provolávání ONNX modelu v Pythonu je sice hezké, nicméně cílem je mít možnost ho používat v rámci javovského kódu. Ten samozřejmě v Jupyteru odpálit nemůžeme - proto by se měly na Githubu vedle tohoto spisku nacházet odpovídající java soubory. No a jelikož tento typ problémů člověk obvykle řeší v pracovním softwarovém prostředí, které nemusí vždy úplně zářit novotou, bude kód kompatibilní s Javou 1.8.     \n",
    "Prerekvizitou pro práci s ONNX je odpovídající knihovna. Tu si k sobě natáhneme pomocí Mavenu. To v době psaní těchto řádek (březen 2021) znamená přidat do pom.xml následující řádky:\n",
    "```\n",
    "<dependency>\n",
    "  <groupId>com.microsoft.onnxruntime</groupId>\n",
    "  <artifactId>onnxruntime</artifactId>\n",
    "  <version>1.7.0</version>\n",
    "</dependency>\n",
    "```\n",
    "Nicméně až toto budete číst, tak se raději podívejte [sem](https://search.maven.org/artifact/com.microsoft.onnxruntime/onnxruntime), zda se v mezičase neobjevily nové verze.  \n",
    "Kód pro \"jednokosatcový\" problém vypadá následovně:\n",
    "```java\n",
    "package onnx_test;\n",
    "\n",
    "import ai.onnxruntime.OnnxTensor;\n",
    "import ai.onnxruntime.OrtEnvironment;\n",
    "import ai.onnxruntime.OrtSession;\n",
    "import ai.onnxruntime.OrtUtil;\n",
    "\n",
    "import java.util.HashMap;\n",
    "\n",
    "public class iris_one_flower {\n",
    "\n",
    "    public static void main(String[] args){\n",
    "        try{\n",
    "            OrtEnvironment environment = OrtEnvironment.getEnvironment();\n",
    "            OrtSession session = environment.createSession(\n",
    "                    \"c:\\\\vs\\\\programovani\\\\python\\\\workshopy\\\\repozitar\\\\machine_learning\\\\iris_onnx.onnx\",\n",
    "                    new OrtSession.SessionOptions()\n",
    "            );\n",
    "\n",
    "            float[] oneFlowerInputData = {6.1f, 2.8f, 4.7f, 1.2f};\n",
    "            long[] oneFlowerInputShape = {1,4};\n",
    "            Object oneFlowerReshaped = OrtUtil.reshape(oneFlowerInputData, oneFlowerInputShape);\n",
    "            OnnxTensor oneFlowerTensor = OnnxTensor.createTensor(environment, oneFlowerReshaped);\n",
    "\n",
    "            HashMap<String, OnnxTensor> inputData = new HashMap<>();\n",
    "            inputData.put(\"float_input\",oneFlowerTensor);\n",
    "\n",
    "            OrtSession.Result results = session.run(inputData);\n",
    "            System.out.println(\"Predicted class: \" + ((long[])results.get(0).getValue())[0]);\n",
    "            System.out.println(\"Predicted probability: \" + results.get(1).getValue());\n",
    "        }catch(Exception e){\n",
    "            System.out.println(\"Following error has occurred:\");\n",
    "            System.out.println(e);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "Výstupem bude\n",
    "```\n",
    "Predicted class: 1\n",
    "Predicted probability: [{0=0.087395005, 1=0.629196, 2=0.283409}]\n",
    "```\n",
    "Co se tady vlastně děje? Nejprve se vytvoří environemnt objekt. To by měl být hostitelský objekt, kde může bydlet více modelů naráz. Přesněji řečeno v něm může koexistovat naráz několik session objektů, které každý obsahují právě jeden model. Sešna se vytvoří metodou *createSession*, která je nakrmena jednak cestou k modelu, jednak objektem s nastavením dané sessiony. Zde by se například dalo nastavit, že chceme použít GPU. Nicméně mi si zde vystačíme s defaultním nastavením, kdy veškerou práci obstará CPU.  \n",
    "Následně je potřeba vyrobit vstupní data, resp. je přetransformovat do vhodné podoby. Nejprve hodnoty prediktorů umístíme do pole floatů. Opravdu se musí jednat o floaty a nikoli o defaultní javovský desetinnočíselný formát double, se kterým by si ONNX neporadil. Do pole longů se zase musí umístit tvar vstupních dat - v našem případě se jedná o jeden řádek se čtyřmi sloupci. Data se reshapují a zkonvertují do podoby tenzoru. Následně se umístí do mapy.  \n",
    "Predikce je podobně jako v Pythonu realizovaná metodou *run*. V případě predikování třidy se nesmí zapomenout na konverzi výsledku, u pravděpodobnosti tento krok potřeba není.  \n",
    "Kód zpracovávající více záznamů najednou se od předchozího příliš neliší. Pouze v poli zastupující tvar vstupu bude na prvním místě namísto jedničky trojka (na vstupu máme tři záznamy) a polem s výsledkem klasifikace musíme iterovat.\n",
    "```java\n",
    "package onnx_test;\n",
    "\n",
    "import ai.onnxruntime.OnnxTensor;\n",
    "import ai.onnxruntime.OrtEnvironment;\n",
    "import ai.onnxruntime.OrtSession;\n",
    "import ai.onnxruntime.OrtUtil;\n",
    "\n",
    "import java.util.HashMap;\n",
    "\n",
    "public class iris_more_flowers {\n",
    "    public static void main (String[] args){\n",
    "        try{\n",
    "            OrtEnvironment environment = OrtEnvironment.getEnvironment();\n",
    "            OrtSession session = environment.createSession(\n",
    "                    \"c:\\\\vs\\\\programovani\\\\python\\\\workshopy\\\\repozitar\\\\machine_learning\\\\iris_onnx.onnx\",\n",
    "                    new OrtSession.SessionOptions()\n",
    "            );\n",
    "\n",
    "            float[] moreFlowersInputData = {\n",
    "                    6.1f, 2.8f, 4.7f, 1.2f,\n",
    "                    5.7f, 3.8f, 1.7f, 0.3f,\n",
    "                    7.7f, 2.6f, 6.9f, 2.3f\n",
    "            };\n",
    "            long[] moreFlowersInputShape = {3,4};\n",
    "            Object moreFlowersReshaped = OrtUtil.reshape(moreFlowersInputData, moreFlowersInputShape);\n",
    "            OnnxTensor moreFlowersTensor = OnnxTensor.createTensor(environment, moreFlowersReshaped);\n",
    "\n",
    "            HashMap<String, OnnxTensor> inputData = new HashMap<>();\n",
    "            inputData.put(\"float_input\",moreFlowersTensor);\n",
    "\n",
    "            OrtSession.Result results = session.run(inputData);\n",
    "            long[] resultsArray = ((long[])results.get(0).getValue());\n",
    "            for (long oneResult:resultsArray){\n",
    "                System.out.println(\"Predicted class: \" + oneResult);\n",
    "            }\n",
    "            System.out.println(\"Predicted probabilities: \" + results.get(1).getValue());\n",
    "        }catch(Exception e){\n",
    "            System.out.println(\"Following error has occurred:\");\n",
    "            System.out.println(e);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "V konzoli uvidíme následující:\n",
    "```\n",
    "Predicted class: 1\n",
    "Predicted class: 0\n",
    "Predicted class: 2\n",
    "Predicted probabilities: [{0=0.087395005, 1=0.629196, 2=0.283409}, {0=0.87357265, 1=0.11904889, 2=0.0073784636}, {0=6.5307214E-4, 1=0.12385611, 2=0.87549084}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Složitější model - vytvoření v Pythonu\n",
    "Představme si, že máme úlohu vyžadující model složitější. Co se zde ale vyšší složitostí myslí? Sice přejdeme od logistické regrese k regresnímu lesu, ale to nás netrápí. Složitost zde vnese komplikovanější pipelina a skutečnost, že v datech budeme mít prediktory více typů.  \n",
    "Použijeme kagglovská data z úlohy na [ceny nemovitostí](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Nicméně pozorujeme, že prediktorů je pro naše účel snad až moc - omezíme se proto jen na první čtyři - MSSubClass, MSZoning, LotFrontage a LotArea. Přesnost modelu sice půjde pod kytičky, nicméně o to nám zde ani nejde - cílem je vytvořit ONNX model a porovnat jeho výstupy s modelem normálním."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:01:41.614392Z",
     "start_time": "2021-03-28T15:01:41.573805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea  SalePrice\n",
       "0          60       RL         65.0     8450     208500\n",
       "1          20       RL         80.0     9600     181500\n",
       "2          60       RL         68.0    11250     223500\n",
       "3          70       RL         60.0     9550     140000\n",
       "4          60       RL         84.0    14260     250000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_columns = [\"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\", \"SalePrice\"]\n",
    "houses_dataset = pd.read_csv(\"train.csv\", header=0, sep=\",\")[wanted_columns]\n",
    "houses_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívejme se na datové typy prediktorů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:01:44.109055Z",
     "start_time": "2021-03-28T15:01:44.093869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       int64\n",
       "MSZoning        object\n",
       "LotFrontage    float64\n",
       "LotArea          int64\n",
       "SalePrice        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z pohledu ONNX by se hodilo, kdyby všechny one-hot encodované sloupce měly stejný typ a kdyby všechny numerické sloupce byly floaty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:01:45.414027Z",
     "start_time": "2021-03-28T15:01:45.383928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass      object\n",
       "MSZoning        object\n",
       "LotFrontage    float64\n",
       "LotArea        float64\n",
       "SalePrice        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_dataset[\"MSSubClass\"] = houses_dataset[\"MSSubClass\"].astype(str)\n",
    "houses_dataset[\"LotArea\"] = houses_dataset[\"LotArea\"].astype(float)\n",
    "houses_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořme si pipelinu. Zde začnou nastávat problémy. Ne každá věc v scikit-learnu je převeditelná do ONNXu. Například použití FunctionTransformeru vede k vzniku chyby \n",
    "```\n",
    "RuntimeError: FunctionTransformer is not supported unless the transform function is None (= identity). You may raise an issue at https://github.com/onnx/sklearn-onnx/issues.\n",
    "```\n",
    "Věc se má totiž tak, že by se skrze tento transformer mohla dál dostat funkce, se kterou si ONNX nedokáže [poradit](https://github.com/onnx/sklearn-onnx/issues/278). V našem konkrétním případě jsme přišli o logaritmování. To by v běžném provozu znamenalo, že logaritmování bychom museli při trénování v Pythonu i predikci v Javě provést ručně. Jelikož v tomto příkladu na kvalitě predikce nebazírujeme, necháme to být. Další věcí, která bude muset zmizet, je imputování pro stringy (viz [zde](http://onnx.ai/sklearn-onnx/auto_examples/plot_complex_pipeline.html#example-complex-pipeline))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:01:49.652348Z",
     "start_time": "2021-03-28T15:01:48.174111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('scaler',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('robustscaler',\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  ['LotFrontage', 'LotArea']),\n",
       "                                                 ('one_hot',\n",
       "                                                  Pipeline(steps=[('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['MSSubClass',\n",
       "                                                   'MSZoning'])])),\n",
       "                ('randomforestregressor', RandomForestRegressor())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_one_hot = [\"MSSubClass\", \"MSZoning\"]\n",
    "col_scaler = [\"LotFrontage\", \"LotArea\"]\n",
    "\n",
    "one_hot_pipeline = make_pipeline(\n",
    "    #SimpleImputer(strategy=\"constant\", fill_value=\"NotPresent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "scaler_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=0),\n",
    "    #FunctionTransformer(func=np.log1p),\n",
    "    RobustScaler()\n",
    ")\n",
    "\n",
    "joined_pipeline = ColumnTransformer([\n",
    "    (\"scaler\", scaler_pipeline, col_scaler),\n",
    "    (\"one_hot\", one_hot_pipeline, col_one_hot)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "final_pipeline_forest = make_pipeline(\n",
    "    joined_pipeline,\n",
    "    RandomForestRegressor()\n",
    ")\n",
    "\n",
    "final_pipeline_forest.fit(\n",
    "    houses_dataset.drop(\"SalePrice\", axis=1), \n",
    "    houses_dataset[\"SalePrice\"]\n",
    ")\n",
    "final_pipeline_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplikujme scikit-learnovský model na několik záznamů..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:26:16.134471Z",
     "start_time": "2021-03-28T15:26:16.114796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>98.0</td>\n",
       "      <td>12256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>56.0</td>\n",
       "      <td>8960.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass MSZoning  LotFrontage  LotArea\n",
       "892          20       RL         70.0   8414.0\n",
       "1105         60       RL         98.0  12256.0\n",
       "413          30       RM         56.0   8960.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_houses = houses_dataset.sample(3, random_state=42).drop(\"SalePrice\", axis=1)\n",
    "more_houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:08:19.434343Z",
     "start_time": "2021-03-28T15:08:19.393998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([153684.83333333, 331668.93      , 106657.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipeline_forest.predict(more_houses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V kontrastu ke kosatcům musíme v initial_type vyrobit pro každý prediktor separátní tuple o jednom sloupci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:12:35.564088Z",
     "start_time": "2021-03-28T15:12:30.144553Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_type = [\n",
    "    (\"MSSubClass\", skl2onnx.common.data_types.StringTensorType([None, 1])),\n",
    "    (\"MSZoning\", skl2onnx.common.data_types.StringTensorType([None, 1])),  \n",
    "    (\"LotFrontage\", skl2onnx.common.data_types.FloatTensorType([None, 1])),\n",
    "    (\"LotArea\", skl2onnx.common.data_types.FloatTensorType([None, 1]))\n",
    "\n",
    "]\n",
    "converted_model = skl2onnx.convert_sklearn(final_pipeline_forest, initial_types=initial_type)\n",
    "with open(\"houses_onnx.onnx\", \"wb\") as model_file:\n",
    "    model_file.write(converted_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zpracujme data k ohodnocení do podoby, ve které jim bude na chvíli načtený ONNX model rozumět. Tj. hodnoty číselných sloupců převedeme na float a všechny sloupce reshapujeme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:17:37.264285Z",
     "start_time": "2021-03-28T15:17:37.244191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSSubClass': array([['20'],\n",
       "        ['60'],\n",
       "        ['30']], dtype=object),\n",
       " 'MSZoning': array([['RL'],\n",
       "        ['RL'],\n",
       "        ['RM']], dtype=object),\n",
       " 'LotFrontage': array([[70.],\n",
       "        [98.],\n",
       "        [56.]], dtype=float32),\n",
       " 'LotArea': array([[ 8414.],\n",
       "        [12256.],\n",
       "        [ 8960.]], dtype=float32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {col_name: more_houses[col_name].values for col_name in more_houses.columns}\n",
    "for col_name in [\"LotFrontage\", \"LotArea\"]:\n",
    "    inputs[col_name] = inputs[col_name].astype(np.float32)\n",
    "for col_name in inputs:\n",
    "    inputs[col_name] = inputs[col_name].reshape((inputs[col_name].shape[0], 1))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:16:04.764465Z",
     "start_time": "2021-03-28T15:16:04.734416Z"
    }
   },
   "source": [
    "Predikci v Pythonu pak realizujeme následujícím způsobem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:21:32.264533Z",
     "start_time": "2021-03-28T15:21:31.954501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted prices:\n",
      "[[153684.83]\n",
      " [330689.47]\n",
      " [106657.  ]]\n"
     ]
    }
   ],
   "source": [
    "loaded_houses_onnx_model = onnxruntime.InferenceSession(\"houses_onnx.onnx\")\n",
    "predicted_prices = loaded_houses_onnx_model.run(\n",
    "    None, inputs\n",
    ")[0]\n",
    "print(f\"Predicted prices:\\n{predicted_prices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Složitější model - použití v Javě  \n",
    "Javovský kód pro složitější model vypadá následovně:\n",
    "```java\n",
    "package onnx_test;\n",
    "\n",
    "import ai.onnxruntime.OnnxTensor;\n",
    "import ai.onnxruntime.OrtEnvironment;\n",
    "import ai.onnxruntime.OrtSession;\n",
    "import ai.onnxruntime.OrtUtil;\n",
    "\n",
    "import java.util.HashMap;\n",
    "\n",
    "public class houses_more_houses {\n",
    "    public static void main(String[] args){\n",
    "        try{\n",
    "            OrtEnvironment environment = OrtEnvironment.getEnvironment();\n",
    "            OrtSession session = environment.createSession(\n",
    "                    \"c:\\\\vs\\\\programovani\\\\python\\\\workshopy\\\\repozitar\\\\machine_learning\\\\houses_onnx.onnx\",\n",
    "                    new OrtSession.SessionOptions()\n",
    "            );\n",
    "\n",
    "            HashMap<String, OnnxTensor> inputData = new HashMap<>();\n",
    "            long[] housesInputShape = {3,1};\n",
    "\n",
    "            String[] dataMSSubClass = {\"20\", \"60\", \"30\"};\n",
    "            OnnxTensor tensorMSSubClassData = OnnxTensor.createTensor(\n",
    "                    environment, dataMSSubClass, housesInputShape\n",
    "            );\n",
    "            inputData.put(\"MSSubClass\",tensorMSSubClassData);\n",
    "\n",
    "            String[] dataMSZoningData = {\"RL\", \"RL\", \"RM\"};\n",
    "            OnnxTensor tensorMSZoning = OnnxTensor.createTensor(\n",
    "                    environment, dataMSZoningData, housesInputShape\n",
    "            );\n",
    "            inputData.put(\"MSZoning\",tensorMSZoning);\n",
    "\n",
    "            float[] dataLotFrontage = {70.0f, 98.0f, 56.0f};\n",
    "            Object reshapedLotFrontage = OrtUtil.reshape(dataLotFrontage, housesInputShape);\n",
    "            OnnxTensor tensorLotFrontage = OnnxTensor.createTensor(environment, reshapedLotFrontage);\n",
    "            inputData.put(\"LotFrontage\",tensorLotFrontage);\n",
    "\n",
    "            float[] dataLotArea = {8414.0f, 12256.0f, 8960.0f};\n",
    "            Object reshapedLotArea = OrtUtil.reshape(dataLotArea, housesInputShape);\n",
    "            OnnxTensor tensorLotArea = OnnxTensor.createTensor(environment, reshapedLotArea);\n",
    "            inputData.put(\"LotArea\",tensorLotArea);\n",
    "\n",
    "            OrtSession.Result results = session.run(inputData);\n",
    "            float[][] resultsArray = (float[][])results.get(0).getValue();\n",
    "            for (float[] oneResult:resultsArray){\n",
    "                System.out.println(oneResult[0]);\n",
    "            }\n",
    "        }catch (Exception e){\n",
    "            System.out.println(\"Following error has occurred:\");\n",
    "            System.out.println(e);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "Výstupem bude\n",
    "```\n",
    "153684.83\n",
    "330689.47\n",
    "106657.0\n",
    "```\n",
    "Čím se liší od prvotního javovského kódu? Nyní má každý prediktor svůj vlastní tenzor a svůj vlastní záznam v mapě inputData. Za zmínku stojí skutečnost, že u stringů neprovádíme reshapování. Další rozdíl spočívá v tom, že se výsledek nepřevádí na jednorozměrné, nýbrž na dvojrozměrné pole. Nicméně ve výsledku se koukáme na praktický identický kód."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingový model - použití v Pythonu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedním z možných využití velkých jazykových modelů jsou tzv. RAGy (retrieval augmented generation). Jedná se de facto o dotazovače nad uživatelovými dokumenty. Fungují zhruba tak, že se uživatelova otázka převede na vektor a pro tento vektor se následně hledají nejpodobnější vektory reprezentující útržky prohledávaných dokumentů. Následně se uživatelova otázka i vybrané útržky dokumentů pošlou na syntézu do LLMka.  \n",
    "Neřešme nyní LLMko jako takové a podívejme se na způsob, jak nějaký text převést na vektor. K tomu slouží embeddingové modely. Z hlediska češtiny je z open sourcu v dnešní době asi nejlepší rodina e5 modelů (k nalezení [zde](https://huggingface.co/intfloat)). Defaultně se jedná o pythoní záležitost, nicméně pro některé z modelů existuje i konverze do ONNX (viz třeba [zde](https://huggingface.co/intfloat/multilingual-e5-small/tree/main/onnx)). Problém nicméně spočívá ve faktu, že převod původních modelů na ONNX neudělal původní autor, nýbrž sám Higging Face. To má za následek skutečnost, že v kartě modelu není ukázáno, jak by se ONNX model vlastně měl používat. Popravdě i na internetu se moc návodů nenalézá. Nicméně to neznamená, že by návody neexistovaly - pro účel této podkapitoly čerpám (resp. místy přímo na 100% kód přejímám) [odtud](https://www.philschmid.de/optimize-sentence-transformers).  \n",
    "Pozn.: po rozběhnutí následujícího kódu byste měli mít v environmentu balíčky [transformers](https://pypi.org/project/transformers/), [optimum](https://pypi.org/project/optimum/), [onnxruntime](https://pypi.org/project/onnxruntime/) a [onnx](https://pypi.org/project/onnx/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\primary\\programovani\\workshops\\environment\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kód 1:1 převzat z https://www.philschmid.de/optimize-sentence-transformers\n",
    "from transformers import Pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    " \n",
    "# copied from the model card\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    " \n",
    " \n",
    "class SentenceEmbeddingPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        # we don't have any hyperameters to sanitize\n",
    "        preprocess_kwargs = {}\n",
    "        return preprocess_kwargs, {}, {}\n",
    " \n",
    "    def preprocess(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "        return encoded_inputs\n",
    " \n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n",
    " \n",
    "    def postprocess(self, model_outputs):\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_outputs[\"outputs\"], model_outputs['attention_mask'])\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro tuto ukázku jsem stáhnul ONNX variantu [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large/tree/main/onnx) a to sice do adresáře \"e5_onnx_large\". Následně jsem se setkal s errorem na chybějící config.json. Ten jsem vyrobil tak, že jsem pod tímto názvem vytvořil duplikát souboru onnx_config.json.  \n",
    "Původně jsem test prováděl se small variantou modelu, avšak ta z nějakého důvodu nefungovala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ORTModelForFeatureExtraction.from_pretrained(\"e5_onnx_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"e5_onnx_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0097, -0.0032, -0.0154,  ..., -0.0236, -0.0341,  0.0105]])\n"
     ]
    }
   ],
   "source": [
    "onnx_emb_instance = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "new_vector = onnx_emb_instance(\"query: králík chroupá mrkev.\")\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
