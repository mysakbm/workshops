{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2493642a-1caa-406c-9eb6-e6d8066411e6",
   "metadata": {},
   "source": [
    "Velkých jazykových modelů (large language models, LLM) dnes existuje  velké množství. Variabilita je možná už tak velká, že se v nabídce člověk snadno ztratí. Aby se to nestalo i mně, začal jsem si dělat poznámky, které potenciální zájemce nalezne v tomto souboru. Přirozeně ale vyvstává otázka, zda zastarají za 6 nebo 12 měsíců...  \n",
    "První dělení LLM jde po ose proprietární modely, které lze použít jen skrze API jejich mateřské firmy (GPT, Claud, Gemini atd.), versus modely více či méně otevřené. Věnovat se zde budeme pouze druhé skupině.\n",
    "\n",
    "## Dělení dle quantization\n",
    "\n",
    "### \"Čisté\" modely\n",
    "Jedná se o model v té podobě, v jaké byl natrénován. Papírově má lepší (současně přesvědčivější a pravdivější) výkon než dále zmíněné možnosti, což je ale vykoupeno většími hardwarovými nároky. Dřív měly tyto modely podobu adresáře s hromadou malých souborů a jedním velkým pytorchím picklem. Dnes byl ve většině případů pickle nahrazen safetensorem. Safetensory jsou údajně o [něco rychlejší než pickly](https://huggingface.co/docs/safetensors/speed) a použitelné i v jiných jazycích než Python. Nejdůležitější roli pro nás ale hraje skutečnost, že jsou bezpečnější - oproti picklům si lze jen obtížně představit, že by do nich někdo vložil škodlivý kód, který by se při jejich načtení spustil. Více informací například [zde](https://github.com/huggingface/safetensors/discussions/111).\n",
    "\n",
    "### GGUF\n",
    "Zkratka znamená GPT Generated Unified Format. Ty první dvě Gčka možná též představují iniciály autora - Georgi Greganov. Jedná se o stejného člověka, který je autorem llamma.cpp (viz dále). Výhoda formátu tkví ve faktu, že dokáže pracovat jak s GPU (s tím, že se model načte do VRAM), tak s CPU (model se načte do RAM). Možná je i kombinace obojího, rychlost v takovém případě ale dle internetu závratná nebude (i v porovnání s čístým CPU přístupem).  \n",
    "Běh na CPU je nepřekvapivě pomalejší než na GPU. Nicméně zde se dostáváme k další výhodě GGUF modelů - dovolují použít menší datový typ pro uložení vah, než byl v modelu původním. V takovém případě mluvíme o \"quantization\". De facto se tak provede něco na způsob zaokrouhlení. Čím je toto zaokrouhlení agresivnější, tím má GGUF model menší velikost a pracuje rychleji, bohužel je ale také hloupější.\n",
    "Oproti čistým modelům tvoří GGUF model jeden jediný soubor. Z jeho jména pak můžeme odvodit, jak silná quantizace byla. Se seznamem sufixů i s technický vysvětlením se můžeme seznámit [na spodku této stránky](https://huggingface.co/docs/hub/gguf), trochu srozumitelnější vysvětlení se nalézá [tady](https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/). Obecně Q8 je z hlediska výkonnosti téměř na úrovni původního modelu, Q2 bývá obvykle hodně dada, Q4/Q5 zase zlatý střed (objevily se i vědecké články o Q1, ale na vlastní PC jsem ještě s něčím podobným do styku nepřišel). Písmeno \"K\" představuje novější typ quantizace, S, M a L (small, medium, large) zase podrobnější úroveň quantizace na (z hlediska primární úrovně quantizace) - pro S model byla quantizace silnější než pro L model.  \n",
    "GGUF formát by měl být rozhodně bezpečnější než pickle. Nicméně co se týče implementace v llama.cpp, svého času se objevila zpráva o [security díře](https://www.reddit.com/r/LocalLLaMA/comments/1bist4o/alert_gguf_security_advisory/). Ta je už naštěstí dávno patchnutá a co se ví, nikdo tuto díry nevyužil.  \n",
    "V dnešní době se obvykle GGUF mutace modelů objeví současně s modelem původním, obvykle na stejném hugging face repozitáři.\n",
    "\n",
    "### GGML\n",
    "Zkratka znamená GPT-Generated Model Language (anebo možná Georgi Greganov Model Language). Jedná se o předchůzce GGUF, tj. dnes se už nepoužívá.\n",
    "\n",
    "### GPTQ, AWQ, EXL2\n",
    "Jedná se o quantizované modely využívající pouze GPU. Nejstarší bylo GPTQ, nejnovější EXL2. Obecně čím novější formát, tím větší rychlost - prý. Jelikož používám primárně GGUF, tak o výhodách a problémech těchto GPU-only mutací modelů příliš nevím. Zdá se, že hodně aktivním konvertorem modelů do EXL2 je [LoneStriker](https://huggingface.co/LoneStriker)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d7c3a-bb0b-49ea-8d33-446b9814d6cd",
   "metadata": {},
   "source": [
    "## Další dělení \n",
    "### Dělení dle velikosti\n",
    "Ve jménech modelů se těsně na názvem rodiny objevuje něco v duchu 7b, 13b, 70b atd. Jedná se o celkový počet parametrů (vah) modelu, obvykle v miliardách (od toho pochází i písmeno b - billion). Větší číslo znamená větší sílu modelu (minimálně v rámci dané generace modelu), vykoupenou ale většími HW nároky. \n",
    "\n",
    "### Dělení dle délky kontextu\n",
    "Velikost kontextu de facto udává velikost paměti modelu. Jedná se o maximální možný součet počtu tokenů do modelu vstupujících a z modelu vystupujících. Pokud by byl počet tokenů větší než toto číslo, bude odpověď modelu uříznutá. Měli bychom zdůraznit, že délka kontextu není totožná s pamětí u chatbotů založených na LLM - tam jde nejčastěji o počet zapamatovaných výměn uživatel - AI.  \n",
    "Velikost kontextu se ne vždy objevuje ve jméně modelu. Pokud tam je, udává se v tisících a má za sebou písmeno \"k\". Nejtypičtější je asi velikost 4k. Existují metody, jak toto číslo navýšit, to si ale vybírá daň v podobě HW nároků.\n",
    "\n",
    "### Dělení dle podoby promptu\n",
    "Různé modely vyžadují různé formáty promptu alias tagy oddělující systémový prompt, uživatelský prompt a odpověď modelu (srovnejme např. sekci \"Chat Format\" [odtud](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) se sekcí \"Prompt Format\" [zde](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-70B)). Sice formátů promtů existuje mnohem méně než modelů, ale když člověk použije formát špatný, dostane namísto rozumné odpovědi nesmysly anebo vůbec nic. Proto je vždy vhodné podívat se na kartu modelu.\n",
    "\n",
    "### Dělení dle (ne)cenzurovanosti\n",
    "Většina modelů od korporací je cenzurovaných, tj. modely odmítnou odpovídat na otázky s kriminální či NSFW tématikou. Necenzurované modely vznikly fine-tuningem těchto cenzurovaných modelů.\n",
    "\n",
    "### Nevešlo se jinam\n",
    "Existuje řada benchmarků, s pomocí kterých se mezi sebou jednotlivé modely porovnávají (např. [MMLU](https://en.wikipedia.org/wiki/MMLU)). Bohužel často dochází k tomu, že (vědomky či nevědomky) otázky z benchmarků proniknou do trénovacích dat, čímž je taková metrika poněkud znehodnocena. Též se objevily pokusy v rámci jednoho benchmarku srovnávat hrušky s jablky, popřípadě hledat specifický způsob dotazování, při kterém se model A jeví lépe než model B, přestože při nomrálním testování by srovnávání skončilo naopak. Existuje ale jeden benchmark, který není založený na odpovídání na předem známé otázky, nýbrž na hodnocení člověka - [LMSYS Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard). Uživatel položí ve webovém rozhraní otázku a dostaně odpověď ode dvou náhodně vybraných modelů. Jeden označí jako lepší (resp. řekne, že výsledky byly ekvivaletní) a až poté uvidí, jaké že modely mu vlastně odpovídaly.  \n",
    "\n",
    "Za pozornost stojí i žebříčky, které na Redditu dělá (možná spíše dělal - dlouho se nic nového neobjevilo) uživatel [WolframRavenwolf](https://www.reddit.com/r/LocalLLaMA/comments/1cal17l/llm_comparisontest_llama_3_instruct_70b_8b/). Otázky jsou neveřejné, přičemž položeny jsou v němčině (metologie bývá na začátku příspěvku). Lze ocenit, že k jednotlivým hodnocením modelů je připojen vždy i krátký komentář.  \n",
    "\n",
    "V názvu některých modelů člověk občas objeví písmena DPO. Jedná se o zkratku pro [\"Direct Preference Optimization\"](https://arxiv.org/abs/2305.18290). Víceméně se jedná o způsob, jak se zbavit pomocného modelu, který v rámci RLHF (Reinforcement Learning from Human Feedback) dotrénovává velký jazykový model s cílem produkovat člověku libé výstupy. Onen pomocný model prý totiž bývá nestabilní a poměrně komplikovaný na vytvoření. V rámci DPO dochází k RLHF v rámci trénování samotného jazykového modelu. \n",
    "\n",
    "Na kartách některých modelů člověk nalezne informaci, že se jedná o \"frankenmerge\". Víceméně jde o to, že se vrstvy několika modelů (anebo třeba i toho samého modelu) postaví na sebe, případně se trochu zdrcnou. Výsledek by údajně měl být lepší co se přirozeného toku řeči týče, avšak horší z hlediska faktografické správnosti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a0405-f2c7-4baa-91c8-53e4b5cefe53",
   "metadata": {},
   "source": [
    "## Llama cpp\n",
    "Pro práci s GGUF soubory použijeme balíček [llama-cpp-python](https://pypi.org/project/llama-cpp-python/). Jedná se o pythoní wrapper kolem knihovny [llama.cpp](https://github.com/ggerganov/llama.cpp). Pokud budeme na práci s modely používat puze CPU, má instalace klasickou podobu \"pip install balíček\". Pro využití GPU je však potřeba\n",
    "- mít NVIDIA grafickou kartu\n",
    "- nainstalovat odpovídající verzi CUDY\n",
    "- v příkazovém řádku postupně pustit 'set LLAMA_CUBLAS=1', 'set FORCE_CMAKE=1', 'set CMAKE_ARGS=-DCMAKE_GENERATOR_TOOLSET=\"cuda=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\"' (resp. obdobu závisející na adresáři a verzi použité CUDY)\n",
    "- samotnou instalaci realizovat s příkazem 'pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir'\n",
    "\n",
    "Testování provedeme na [GGUF mutaci modelu Phi-3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf).   \n",
    "Nejprve si vytvoříme objekt reprezentující velký jazykový model. V něm nejdůležitějším parametrem je *model_path*, kam vkládáme název souboru s modelem.  \n",
    "Defaultně se předpokládá, že model poběží pouze na CPU. V takovém případe je *n_gpu_layers* položeno rovno nule. Tento parametr totiž říká, kolik vrstev (ve smyslu layers) modelu se má nahrát do GPU. Pokud chceme nahrát všechny, položíme parametr rovný -1.  \n",
    "Pro specifikaci, kolik CPU vláken se má dát modelu k dispozici použijeme parametr *n_threads*. Hádám, že defaultní None znamená všechny. Bacha - jako max číslo pro tohle nemůžeme brát \"Threads\" z task managera, nýbrž spíše půlku z počtu logických procesorů - viz následující řádek z llama-cpp-python:\n",
    "```python\n",
    "self.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\n",
    "```\n",
    "Člověk by si myslel, že aby model na stejný uživatelův vstup reagoval vždy stejně, museli bychom zafixovat hodnotu parametru *seed*. Defaultně je tato hodnota fixovaná skrze *llama_cpp.LLAMA_DEFAULT_SEED*. Pokud bychom chtěli chceme náhodný seed, museli bychom parametr položit rovný -1. Fakticky ale i pro seed fixovaný *llama_cpp.LLAMA_DEFAULT_SEED* dostáváme různé výsledky (a i když celou dobu kontrolujeme, zda má proměnná opravdu stejnou hodnotu)   \n",
    "Parametr *n_ctx* by měl udávat maximální povolený součet délky vstupu a výstupu modelu v tokenech. Defaultní hodnota je 512. Když hodnotu příliš snížíme, můžeme dostat error v podobě \"IndexError: index 10 is out of bounds for axis 0 with size 10\" (to když jsem nastavil *n_ctx* = 10).    \n",
    "Parametry mající v názvů *rope* souvisí v umělém prodloužení kontextu modelu. Nikdy jsem to nezkoušel použít (slabý HW).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a613d7-3bac-49c8-bb5d-c10be2099e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    83.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'phi3.embedding_length': '3072', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "path_to_model = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=path_to_model,  \n",
    "  n_gpu_layers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f1927-4124-4373-a581-334c20c16193",
   "metadata": {},
   "source": [
    "Samotné použití modelu nastává až při jeho provolání. Nejdůležitějším parametrem je *prompt*, do kterého napíšeme uživatelům dotaz/příkaz. Obvykle je ale potřeba jej obohatit o tagy specifikující, kde že to vlasntě uživatelův vstup začíná a končí a kde by měla začínat odpověď modelu. Bez takovýchto věcí může odpověď obsahovat nesmysly anebo naopak bude zcela prázdná. Správná podoba tagů se liší model od modelu, přičemž ji najdeme v kartě modelu na Hugging facu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fdc2fe5-6e36-4ff4-bb32-f6b2197f116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     651.74 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    16 runs   (    0.14 ms per token,  7165.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     651.65 ms /     9 tokens (   72.41 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:        eval time =     775.28 ms /    15 runs   (   51.69 ms per token,    19.35 tokens per second)\n",
      "llama_print_timings:       total time =    1436.38 ms /    24 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-d6734890-1db3-43bf-bd5a-e99824d0fab6',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1718978920,\n",
       " 'model': 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'choices': [{'text': ' A hamster is a small rodent belonging to the subfamily Cricet',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 9, 'completion_tokens': 16, 'total_tokens': 25}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\n",
    "  prompt=\"<|user|>\\nWhat is hamster?<|end|>\\n<|assistant|>\"\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae6c72-142f-4421-b7b6-e0f1070a9b8f",
   "metadata": {},
   "source": [
    "Níže se nalézá ukázka výstupu bez tagů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b577bb-f4d6-4894-b92d-dcdb40e0403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  170174.67 ms\n",
      "llama_print_timings:      sample time =       0.12 ms /     1 runs   (    0.12 ms per token,  8474.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  152432.03 ms /     5 tokens (30486.41 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  152433.38 ms /     6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-fbd009a5-540a-427d-8a58-dc72b837927f',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1717940277,\n",
       " 'model': 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'choices': [{'text': '',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 6, 'completion_tokens': 0, 'total_tokens': 6}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\n",
    "  prompt=\"What is hamster?\"\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e9db2-9805-43b8-8c2f-eaea6cfaaddb",
   "metadata": {},
   "source": [
    "Důležitým parametrem je **max_tokens** říkající, kolik tokenů může maximálně zabrat odpověď. Defaultní hodnota je rovna 16 (opravdu tak málo). Pokud jí položíme rovnou None či číslu menšímu než 1, bude maximální počet vygenerovaných tokenů neomezený, resp. omezený pouze hodnotou **n_ctx** z konstruktoru modelu. V případě, že se odpověď do počtu tokenů nevejde, bude useklá a **finish_reason** se namísto klasického **stop** bude rovnat **lenght**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927896f3-80f7-412e-8427-79cc4a9867f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  170174.67 ms\n",
      "llama_print_timings:      sample time =       0.64 ms /     5 runs   (    0.13 ms per token,  7824.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  162739.85 ms /     8 tokens (20342.48 ms per token,     0.05 tokens per second)\n",
      "llama_print_timings:        eval time =     299.64 ms /     4 runs   (   74.91 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =  163044.52 ms /    12 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-7f93153f-cc58-41b3-bb6a-7f7876ef40ca',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1717940630,\n",
       " 'model': 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'choices': [{'text': ' A hamster is a',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 9, 'completion_tokens': 5, 'total_tokens': 14}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\n",
    "  prompt=\"<|user|>\\nWhat is hamster?<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=5,\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d0495-9527-4acc-9ff9-e3aad7bfd139",
   "metadata": {},
   "source": [
    "Pokud v odpovědi chceme mít i původní prompt, použijeme parametr **echo** s hodnotou True (defaultně je tento parametr roven False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bde07b-b465-4a30-a4de-581e342e28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  170174.67 ms\n",
      "llama_print_timings:      sample time =      67.73 ms /   377 runs   (    0.18 ms per token,  5566.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =   64274.95 ms /   377 runs   (  170.49 ms per token,     5.87 tokens per second)\n",
      "llama_print_timings:       total time =   64768.95 ms /   377 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-3ea16bdc-6ce0-404a-b433-eada18ca98cc',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1717940910,\n",
       " 'model': 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'choices': [{'text': \"<|user|>\\nWhat is hamster?<|end|>\\n<|assistant|> A hamster is a small, rodent mammal belonging to the family Cricetidae. There are approximately 19 different species of hamsters found naturally in various regions around the world, with one species, the golden hamster (Campbell's dwarf hamster), being commonly kept as pets globally due to its docile nature and manageable size.\\n\\nHamsters have round bodies, short limbs, and large cheek pouches, which are used for storing food. They come in a variety of colors (including golden, white, black, brown, or even multi-colored) and sizes but typically range from 5 to 12 inches long with an additional tail length ranging between 0.3 to 0.8 inches.\\n\\nIn the wild, hamsters live in burrows they dig underground for shelter from predators and extreme temperatures. Their diets mainly consist of seeds, nuts, fruits, vegetables, and insects. Hamsters are nocturnal creatures that sleep during the daytime and become more active at night.\\n\\nAs pets, hamsters require proper care including a suitable habitat with bedding material for burrowing, food specifically designed for their species, fresh water, toys, and regular handling by trusted individuals. Hamster owners must also be mindful of the appropriate size and type of cage, as well as socialization needs since some hamster species can live in pairs or small groups (depending on the specific breed).\\n\\nIn summary, a hamster is a small rodent animal often kept as a pet due to its gentle temperament. They have distinctive physical features and behaviors that make them unique animals, requiring special care if they are domesticated.\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 9, 'completion_tokens': 376, 'total_tokens': 385}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\n",
    "  prompt=\"<|user|>\\nWhat is hamster?<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=512,\n",
    "  echo=True\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798c7ad-1f7d-460d-af6a-6fd86a360ab2",
   "metadata": {},
   "source": [
    "Přidat lze i parametr **stop**. Do něj vložíme list s tagy, které - objeví-li se ve výstupu modelu - povedou k ukončení generování. Obvykle se jedná o \"</s>\" či \"<|end|>\" - konkrétní tag člověk opět najde v kartě modelu. Defaultní hodnotou je prázdný list.  \n",
    "Míru chaotičnosti výstupu můžeme ovlivnit s parametry **temperature** (default 0.8, čím větší hodnota, tím větší chaotičnost a **top_p** (default 0.95, čím menší hodnota, tím větší chaotičnost).  \n",
    "I zde se objevuje parametr **seed**, který tentokrát opravdu vede k fixaci výstupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9550-2cc7-4c24-9d4d-2f1623a2b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
