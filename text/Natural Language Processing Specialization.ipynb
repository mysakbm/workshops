{
 "cells": [
  {
   "cell_type": "code",
   "id": "d892b1f4-9b8a-408b-8a96-8142b99f0040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:11:30.079499Z",
     "start_time": "2024-05-27T21:11:30.077119Z"
    }
   },
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import re # regular expression library; for tokenization of words\n",
    "from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
    "from pomocne_soubory.utils_pos import get_word_tag, preprocess"
   ],
   "outputs": [],
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "id": "e5412230414e7119",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Natural Language Processing with Classification and Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09a7243269df5c",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48de073088b7a1f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330dfc0cdd9dd93",
   "metadata": {},
   "source": [
    "## Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e505452a3535c89",
   "metadata": {},
   "source": [
    "## Machine Translation and Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b9fa3dfffc32d",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing with Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194824742d9d58a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ae3d-9882-4c37-a625-8803edcd8947",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e7833-d86b-43b9-b3a3-1b88a44ec02b",
   "metadata": {},
   "source": [
    "1. Zkontroluj slovo ve slovniku. Pokud tam neni, je to asi typo.\n",
    "2. Najdi vsechny stringy vzdaleny \"n edits\". (Edit je operace insert (add a letter), delete (delete letter), switch (prohod dve pismena), replace (nahrad jedno pismeno)\n",
    "3. Profiltruj stringy, ktery jsou ve slovnku\n",
    "4. Napocitej pravdepodobnosti slov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8df5c1-a18d-4fc5-8408-2ca165351899",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LAB: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecbeefe-3185-4aa6-8dc9-28f318362ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:15.974731Z",
     "start_time": "2024-05-24T13:05:15.972386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow ORANGE BLUE BLUE PINK\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# the tiny corpus of text ! \n",
    "text = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # ðŸŒˆ\n",
    "print(text)\n",
    "print('string length : ',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e68bd03-a564-4b98-ba2d-df941f6ea2ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:15.977635Z",
     "start_time": "2024-05-24T13:05:15.975560Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow orange blue blue pink\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# Preprocessnig\n",
    "# convert all letters to lower case\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n",
    "print('string length : ',len(text_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6efced-82af-4e54-ab88-5646f73418f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:16.045812Z",
     "start_time": "2024-05-24T13:05:16.043290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\n",
      "count :  10\n"
     ]
    }
   ],
   "source": [
    "# some regex to tokenize the string to words and return them in a list\n",
    "words = re.findall(r'\\w+', text_lowercase)\n",
    "print(words)\n",
    "print('count : ',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea78c8bb-7a47-48d8-99c6-093f5d431150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:16.204923Z",
     "start_time": "2024-05-24T13:05:16.202908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue', 'yellow', 'orange', 'pink', 'red'}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 1\n",
    "# create vocab\n",
    "vocab = set(words)\n",
    "print(vocab)\n",
    "print('count : ',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d73246-52b1-4fa4-8607-9d9ba7a9782d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:16.381971Z",
     "start_time": "2024-05-24T13:05:16.379463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 2\n",
    "# create vocab including word count\n",
    "counts_a = dict()\n",
    "for w in words:\n",
    "    counts_a[w] = counts_a.get(w,0)+1\n",
    "print(counts_a)\n",
    "print('count : ',len(counts_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9027e924-8b7b-482a-a3d1-27938f03559a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:17.172200Z",
     "start_time": "2024-05-24T13:05:17.169784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 3\n",
    "# create vocab including word count using collections.Counter\n",
    "counts_b = dict()\n",
    "counts_b = Counter(words)\n",
    "print(counts_b)\n",
    "print('count : ',len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff81c0d-e9a9-4830-9766-ea318938b703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:17.438649Z",
     "start_time": "2024-05-24T13:05:17.371371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barchart of sorted word counts\n",
    "d = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\n",
    "plt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n",
    "_ = plt.xticks(range(len(d)), list(d.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4794de4-1dcc-48e2-ad0c-26ba30197009",
   "metadata": {},
   "source": [
    "### Napocitej pravdepodobnosti slov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26125c45-7be6-42a8-b78e-d8a461cd25db",
   "metadata": {},
   "source": [
    "Model je zalozeny na predikci nejpravdepodobnejsiho slova, takze je treba napocitat pravdepodobnosti slov:\n",
    "$$\n",
    "    P(w) = \\frac{C(w)}{V}\n",
    "$$\n",
    "kde $ P(w) $ je pravdepodobnost slova, $V$ je velikost corpusu, $C(w)$ je pocet slov v nem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ba1b2-ec18-460a-aa72-978efdece329",
   "metadata": {},
   "source": [
    "O neco sofistikovanejsi je moznost pocitat dve slova po sobe jdouci a pouzit pak prvni slovo jak zjistit vyssi pravdepodbnost comba. Napriklad: jestli je \"there friends\" nebo \"their friends\" pravdepodobnejsi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61dadd-65e7-4c57-9d51-a6841690ebd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LAB: String Edits Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceba8e9-3ca0-4f66-a9e0-b6e9904ad65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:38.430149Z",
     "start_time": "2024-05-24T13:05:38.427260Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "word = 'dearz' # ðŸ¦Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76648087-1dc3-41ff-8d79-bce707801fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:38.604563Z",
     "start_time": "2024-05-24T13:05:38.602220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'dearz']\n",
      "['d', 'earz']\n",
      "['de', 'arz']\n",
      "['dea', 'rz']\n",
      "['dear', 'z']\n",
      "['dearz', '']\n"
     ]
    }
   ],
   "source": [
    "# splits with a loop\n",
    "splits_a = []\n",
    "for i in range(len(word)+1):\n",
    "    splits_a.append([word[:i],word[i:]])\n",
    "\n",
    "for i in splits_a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cf44d5-aca9-4260-863a-5d5ddbf75d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:38.797Z",
     "start_time": "2024-05-24T13:05:38.794898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'dearz')\n",
      "('d', 'earz')\n",
      "('de', 'arz')\n",
      "('dea', 'rz')\n",
      "('dear', 'z')\n",
      "('dearz', '')\n"
     ]
    }
   ],
   "source": [
    "# same splits, done using a list comprehension\n",
    "splits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "for i in splits_b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e26461-395c-4245-826f-5462b14273f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:38.974280Z",
     "start_time": "2024-05-24T13:05:38.971988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "earz  <-- delete  d\n",
      "darz  <-- delete  e\n",
      "derz  <-- delete  a\n",
      "deaz  <-- delete  r\n",
      "dear  <-- delete  z\n"
     ]
    }
   ],
   "source": [
    "# Delete Edit\n",
    "# deletes with a loop\n",
    "splits = splits_a\n",
    "deletes = []\n",
    "\n",
    "print('word : ', word)\n",
    "for L,R in splits:\n",
    "    if R:\n",
    "        print(L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f286ea8-d08b-4a11-94ed-df17a877a22b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:39.149115Z",
     "start_time": "2024-05-24T13:05:39.146407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "first item from the splits list :  ['', 'dearz']\n",
      "L :  \n",
      "R :  dearz\n",
      "*** now implicit delete by excluding the leading letter ***\n",
      "L + R[1:] :  earz  <-- delete  d\n"
     ]
    }
   ],
   "source": [
    "# breaking it down\n",
    "print('word : ', word)\n",
    "one_split = splits[0]\n",
    "print('first item from the splits list : ', one_split)\n",
    "L = one_split[0]\n",
    "R = one_split[1]\n",
    "print('L : ', L)\n",
    "print('R : ', R)\n",
    "print('*** now implicit delete by excluding the leading letter ***')\n",
    "print('L + R[1:] : ',L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6387d3df-ba80-4169-968e-e0abaada3993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:39.325805Z",
     "start_time": "2024-05-24T13:05:39.322422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "*** which is the same as ***\n",
      "earz\n",
      "darz\n",
      "derz\n",
      "deaz\n",
      "dear\n"
     ]
    }
   ],
   "source": [
    "# deletes with a list comprehension\n",
    "splits = splits_a\n",
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "\n",
    "print(deletes)\n",
    "print('*** which is the same as ***')\n",
    "for i in deletes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27aa2b90-84f8-4af6-9f24-c2510821d2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T13:05:39.504801Z",
     "start_time": "2024-05-24T13:05:39.502282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\n",
      "edits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "candidate words :  {'dear'}\n"
     ]
    }
   ],
   "source": [
    "vocab = ['dean','deer','dear','fries','and','coke']\n",
    "edits = list(deletes)\n",
    "\n",
    "print('vocab : ', vocab)\n",
    "print('edits : ', edits)\n",
    "\n",
    "candidates=[]\n",
    "\n",
    "### START CODE HERE ###\n",
    "candidates = set.intersection(set(vocab), set(edits))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print('candidate words : ', candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fccaa-2fde-4592-afba-aead4d8483e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b1fd1-866e-4f16-95e2-5e67486f9880",
   "metadata": {},
   "source": [
    "Je mira, ktera popisuje podobnost dvou stringu. Jedna se o aplikaci dynamickeho programovani. Do tabulky si zapisujeme zdrojove slovo a cilove slovo a ukladame do ni cenu ruznych uprav."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f667acc-46e7-4451-b61a-23fa88249209",
   "metadata": {},
   "source": [
    "![image](./pomocne_soubory/dyn_prog.png)\n",
    "\n",
    "Cena z # do # je 0.\n",
    "Z P do # je 1, protoze je potreba smazat P.\n",
    "Z # do P je 1, protoze je potreba pridat P.\n",
    "Z P do P je 0, protoze neni potreba nic delat.\n",
    "Z P do S je 2, protoze je potreba nahradit P za S (minimum cost z P do S)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827c9a6-94d5-4ab3-83d0-b7ba7060bc03",
   "metadata": {},
   "source": [
    "Obecny vzorec na vypocet ceny je: \n",
    "$$\n",
    "    D[i,j] = min \\begin{cases}\n",
    "        D[i-1,j] + del\\_cost \\\\\n",
    "        D[i,j-1] + ins\\_cost \\\\\n",
    "        D[i-1,j-1] + c\n",
    "    \\end{cases}       \n",
    "$$\n",
    "kde $D[i,j]$ je cena zmeny z i-teho znaku zdrojoveho slova na j-ty znak ciloveho slova, $del\\_cost$ je cena smazani, \n",
    "$ins\\_cost$ je cena pridani, a $c$: \n",
    "\n",
    "$$ \n",
    "    c = \\begin{cases}\n",
    "        0 & \\text{ if } source[i] = target[j] \\\\\n",
    "        rep\\_cost & \\text{ if } source[i] \\neq target[j]\n",
    "    \\end{cases}\n",
    "$$\n",
    "a $rep\\_cost$ je cena nahrazeni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d231ff06a4b51",
   "metadata": {},
   "source": [
    "- $D[i,j] = D[i-1,j] + del\\_cost$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i-1,j) a pricte se cena smazani.\n",
    "- $D[i,j] = D[i,j-1] + ins\\_cost$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i,j-1) a pricte se cena pridani.\n",
    "- $D[i,j] = D[i-1,j-1] + c$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i-1,j-1) a pricte se $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb47f8a32696377",
   "metadata": {},
   "source": [
    "![image](./pomocne_soubory/dyn_prog_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cffb7d752f9dc0",
   "metadata": {},
   "source": [
    "Z obrazku je patrne, ze kdyz mame string \"P\" a chceme prazdny string, cena za smazeni je 1. Kdyz mame string \"PL\" a \n",
    "chceme prazdny string, cena za smazeni je 2. Kdyz mame string \"PL\" a chceme string \"P\", cena za smazeni je 1 a za \n",
    "dalsi smazani je taky 1. Analogicky pro pridavani stringu. Pro vyplneni bunky (1,1) je treba si spocitat cenu z\n",
    "bunky (0,0) a pricist cenu za nahrazeni. Nebo si spocitat cenu z bunky (0,1), coz je string \"P\" a smazani \"P\" a k \n",
    "tomu pridat cenu za pridani \"S\". Nebo analogicky pro bunku (1,0). Vezmeme minumum z techto cisel, tj. 2. A takto\n",
    "postupujeme pro vsechny bunky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480e8698004a18",
   "metadata": {},
   "source": [
    "#### LAB: Assignment 1: Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fae2ff1fd1e59",
   "metadata": {},
   "source": [
    "##### Exercise 1 - process_data\n",
    "Implement the function `process_data` which \n",
    "\n",
    "1) Reads in a corpus (text file)\n",
    "\n",
    "2) Changes everything to lowercase\n",
    "\n",
    "3) Returns a list of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a145d8b4e422ab",
   "metadata": {},
   "source": [
    "##### Options and Hints\n",
    "- If you would like more of a real-life practice, don't open the 'Hints' below (yet) and try searching the web to derive your answer.\n",
    "- If you want a little help, click on the green \"General Hints\" section by clicking on it with your mouse.\n",
    "- If you get stuck or are not getting the expected results, click on the green 'Detailed Hints' section to get hints for each step that you'll take to complete this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06bc691f80d482",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>General Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "General Hints to get started\n",
    "<ul>\n",
    "    <li>Python <a href=\"https://docs.python.org/3/tutorial/inputoutput.html\">input and output<a></li>\n",
    "    <li>Python <a href=\"https://docs.python.org/3/library/re.html\" >'re' documentation </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba559ca5891b21",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Detailed Hints</b></font>\n",
    "</summary>\n",
    "<p>     \n",
    "Detailed hints if you're stuck\n",
    "<ul>\n",
    "    <li>Use 'with' syntax to read a file</li>\n",
    "    <li>Decide whether to use 'read()' or 'readline().  What's the difference?</li>\n",
    "    <li>You can use str.lower() to convert to lowercase.</li>\n",
    "    <li>Use re.findall(pattern, string)</li>\n",
    "    <li>Look for the \"Raw String Notation\" section in the Python 're' documentation to understand the difference between r'\\W', r'\\W' and '\\\\W'. </li>\n",
    "    <li>For the pattern, decide between using '\\s', '\\w', '\\s+' or '\\w+'.  What do you think are the differences?</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f544e1a0fd58b345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:57:53.570098Z",
     "start_time": "2024-05-26T08:57:53.551650Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        text = file.read()\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Split text into words based on whitespace\n",
    "        words = re.findall(r'\\w+', text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d637233433437daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:57:54.115798Z",
     "start_time": "2024-05-26T08:57:54.103980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
      "There are 6116 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('./pomocne_soubory/shakespeare.txt')\n",
    "vocab = set(word_l)  # this will be your new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587930b575af45",
   "metadata": {},
   "source": [
    "##### Exercise 2 - get_count\n",
    "\n",
    "Implement a `get_count` function that returns a dictionary\n",
    "- The dictionary's keys are words\n",
    "- The value for each word is the number of times that word appears in the corpus. \n",
    "\n",
    "For example, given the following sentence: **\"I am happy because I am learning\"**, your dictionary should return the following: \n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b>Key </b>  </td>\n",
    "    <td> <b>Value </b> </td> \n",
    "\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> I  </td>\n",
    "    <td> 2</td> \n",
    " \n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td>am</td>\n",
    "    <td>2</td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>happy</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Instructions**: \n",
    "Implement a `get_count` which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba591c2191e9b7",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Try implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions</li>\n",
    "    <li>You can also use defaultdict instead of a regular dictionary, along with the for loop</li>\n",
    "    <li>Otherwise, to skip using a `for` loop, you can use Python's <a href=\"https://docs.python.org/3.7/library/collections.html#collections.Counter\" > Counter class</a> </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d8f0359fba8a86f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:58:41.582306Z",
     "start_time": "2024-05-26T08:58:41.577508Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # This will hold the word counts\n",
    "\n",
    "    for word in word_l:\n",
    "        if word in word_count_dict:\n",
    "            word_count_dict[word] += 1  # Increment the count of the word\n",
    "        else:\n",
    "            word_count_dict[word] = 1   # Add the word with a count of 1 if it's not already in the dictionary\n",
    "\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdee0fb0ad45d5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T08:58:52.193155Z",
     "start_time": "2024-05-26T08:58:52.183808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6116 key values pairs\n",
      "The count for the word 'thee' is 240\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef087e2c898ab5b9",
   "metadata": {},
   "source": [
    "##### Exercise 3 - get_probs\n",
    "Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2}$$\n",
    "where \n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus.\n",
    "\n",
    "For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'** is:\n",
    "\n",
    "$$P(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.$$\n",
    "\n",
    "**Instructions:** Implement `get_probs` function which gives you the probability \n",
    "that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b926e16a04ada",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "General advice\n",
    "<ul>\n",
    "    <li> Use dictionary.values() </li>\n",
    "    <li> Use sum() </li>\n",
    "    <li> The cardinality (number of words in the corpus should be equal to len(word_l).  You will calculate this same number, but using the word count dictionary.</li>\n",
    "</ul>\n",
    "    \n",
    "If you're using a for loop:\n",
    "<ul>\n",
    "    <li> Use dictionary.keys() </li>\n",
    "</ul>\n",
    "    \n",
    "If you're using a dictionary comprehension:\n",
    "<ul>\n",
    "    <li>Use dictionary.items() </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0b71a9756796cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:00:55.551390Z",
     "start_time": "2024-05-26T09:00:55.546816Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    M = sum(word_count_dict.values())\n",
    "\n",
    "    # Calculate the probability for each word\n",
    "    for word, count in word_count_dict.items():\n",
    "        probs[word] = count / M\n",
    "    ### END CODE HERE ###\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cfed09373edb488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:02:02.886622Z",
     "start_time": "2024-05-26T09:02:02.880266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 6116\n",
      "P('thee') is 0.0045\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b056d522c7a294",
   "metadata": {},
   "source": [
    "##### 2 - String Manipulations\n",
    "\n",
    "Now that you have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions: \n",
    "\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d61b52a05d092",
   "metadata": {},
   "source": [
    "##### Exercise 4 - delete_letter\n",
    "\n",
    "**Instructions for delete_letter():** Implement a `delete_letter()` function that, given a word, returns a list of strings with one character deleted. \n",
    "\n",
    "For example, given the word **nice**, it would return the set: {'ice', 'nce', 'nic', 'nie'}. \n",
    "\n",
    "**Step 1:** Create a list of 'splits'. This is all the ways you can split a word into Left and Right: For example,   \n",
    "'nice is split into : `[('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')]`\n",
    "This is common to all four functions (delete, replace, switch, insert).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29779ffd00aecb08",
   "metadata": {},
   "source": [
    "**Step 2:** This is specific to `delete_letter`. Here, we are generating all words that result from deleting one character.  \n",
    "This can be done in a single line with a list comprehension. You can make use of this type of syntax:  \n",
    "`[f(a,b) for a, b in splits if condition]`  \n",
    "\n",
    "For our 'nice' example you get: \n",
    "['ice', 'nce', 'nie', 'nic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd3f80b46e96fc",
   "metadata": {},
   "source": [
    "##### Levels of assistance\n",
    "\n",
    "Try this exercise with these levels of assistance.  \n",
    "- We hope that this will make it both a meaningful experience but also not a frustrating experience. \n",
    "- Start with level 1, then move onto level 2, and 3 as needed.\n",
    "\n",
    "    - Level 1. Try to think this through and implement this yourself.\n",
    "    - Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n",
    "    - Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n",
    "    \n",
    "- If you are still stuck, look at the images in the \"list comprehensions\" section above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ea04029933ecf",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Level 3 Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>\n",
    "    <li>Do this in a loop or list comprehension, so that you have a list of tuples.\n",
    "    <li> For example, \"cake\" can get split into \"ca\" and \"ke\". They're stored in a tuple (\"ca\",\"ke\"), and the tuple is appended to a list.  We'll refer to these as L and R, so the tuple is (L,R)</li>\n",
    "    <li>When choosing the range for your loop, if you input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) </li>\n",
    "    <li>deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings</li>\n",
    "    <li>When combining the tuples, make sure that you leave out a middle character.</li>\n",
    "    <li>Use array slicing to leave out the first character of the right substring.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fdd0c9a5ebb52df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:04:45.992067Z",
     "start_time": "2024-05-26T09:04:45.986034Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i+1:]) for i in range(len(word))]\n",
    "    delete_l = [y[0] + y[1] for y in split_l]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "    return  delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bd65541d6a3cb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:04:56.845964Z",
     "start_time": "2024-05-26T09:04:56.841985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word cans, \n",
      "split_l = [('', 'ans'), ('c', 'ns'), ('ca', 's'), ('can', '')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6653da22992a8b",
   "metadata": {},
   "source": [
    "##### Note 1\n",
    "- Notice how it has the extra tuple `('cans', '')`.\n",
    "- This will be fine as long as you have checked the size of the right-side substring in tuple (L,R).\n",
    "- Can you explain why this will give you the same result for the list of deletion strings (delete_l)?\n",
    "\n",
    "```CPP\n",
    "input word cans, \n",
    "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \n",
    "delete_l = ['ans', 'cns', 'cas', 'can']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecc3d0aff2b6c2",
   "metadata": {},
   "source": [
    "##### Note 2\n",
    "If you end up getting the same word as your input word, like this:\n",
    "\n",
    "```Python\n",
    "input word cans, \n",
    "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \n",
    "delete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n",
    "```\n",
    "\n",
    "- Check how you set the `range`.\n",
    "- See if you check the length of the string on the right-side of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa4dcc17ead9269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:05:30.305493Z",
     "start_time": "2024-05-26T09:05:30.301570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs of delete_letter('at') is 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3bfebbaabe3d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5b80222404a07f",
   "metadata": {},
   "source": [
    "##### Exercise 5 - switch_letter\n",
    "\n",
    "**Instructions for switch_letter()**: Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters **that are adjacent to each other**. \n",
    "- For example, given the word 'eta', it returns {'eat', 'tea'}, but does not return 'ate'.\n",
    "\n",
    "**Step 1:** is the same as in delete_letter()  \n",
    "**Step 2:** A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:  \n",
    "`[f(L,R) for L, R in splits if condition]`  where 'condition' will test the length of R in a given iteration. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739f39b3074125e",
   "metadata": {},
   "source": [
    "##### Levels of difficulty\n",
    "\n",
    "Try this exercise with these levels of difficulty.  \n",
    "- Level 1. Try to think this through and implement this yourself.\n",
    "- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n",
    "- Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440297fab5d1e782",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b16e38f4c7549c",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Level 3 Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>\n",
    "    <li>Splitting is the same as for delete_letter</li>\n",
    "    <li>To perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings</li>\n",
    "    <li>The four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.</li>\n",
    "    <li>Unlike delete_letter, you will want to check that your right substring is at least a minimum length.  To see why, review the previous hint bullet point (directly before this one).</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10b10e602be93cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:07:26.345188Z",
     "start_time": "2024-05-26T09:07:26.339696Z"
    }
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(1, len(word))]\n",
    "    switch_l = [L[:-1] + R[0] + L[-1] + R[1:] for L,R in split_l]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "    \n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68c5e1cdf1bd3f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:07:33.933183Z",
     "start_time": "2024-05-26T09:07:33.930432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41ced86a5cb78a",
   "metadata": {},
   "source": [
    "##### Note 1\n",
    "\n",
    "You may get this:\n",
    "```Python\n",
    "Input word = eta \n",
    "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \n",
    "switch_l = ['tea', 'eat']\n",
    "```\n",
    "- Notice how it has the extra tuple `('eta', '')`.\n",
    "- This is also correct.\n",
    "- Can you think of why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705ff51a3ebecf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547960415c1a52f0",
   "metadata": {},
   "source": [
    "##### Exercise 6 - replace_letter\n",
    "**Instructions for replace_letter()**: Now implement a function that takes in a word and returns a list of strings with one **replaced letter** from the original word. \n",
    "\n",
    "**Step 1:** is the same as in `delete_letter()`\n",
    "\n",
    "**Step 2:** A list comprehension or for loop which form strings by replacing letters.  This can be of the form:  \n",
    "`[f(a,b,c) for a, b in splits if condition for c in string]`   Note the use of the second for loop.  \n",
    "It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of 'ear' with 'e' will return 'ear'.\n",
    "\n",
    "**Step 3:** Remove the original input letter from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004c4ab77b3e4b",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>To remove a word from a list, first store its contents inside a set()</li>\n",
    "    <li>Use set.discard('the_word') to remove a word in a set.  Using set.remove('the_word') throws a KeyError if the word does not exist in the set. </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b1731ab9812cbf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:08:54.408936Z",
     "start_time": "2024-05-26T09:08:54.401350Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "\n",
    "    replace_set = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    # Step 1: Create splits\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # Step 2: Replace letters\n",
    "    for a, b in split_l:\n",
    "        if b:  # Ensure there is something to replace\n",
    "            for c in letters:\n",
    "                if b[0] != c:  # Condition to ensure it's actually a replacement\n",
    "                    replace_set.add(a + c + b[1:])\n",
    "\n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47e037caefa104bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:09:33.274574Z",
     "start_time": "2024-05-26T09:09:33.258420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0d72a18c678cc",
   "metadata": {},
   "source": [
    "##### Exercise 7 - insert_letter\n",
    "\n",
    "**Instructions for insert_letter()**: Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\n",
    "\n",
    "**Step 1:** is the same as in `delete_letter()`\n",
    "\n",
    "**Step 2:** This can be a list comprehension of the form:  \n",
    "`[f(a,b,c) for a, b in splits if condition for c in string]`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71baa0b1e7652a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:09:50.550969Z",
     "start_time": "2024-05-26T09:09:50.539262Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Insert each letter of the alphabet into every split position\n",
    "    insert_l = [a + c + b for a, b in split_l for c in letters]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5e69d4ed6b1cda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:10:01.481227Z",
     "start_time": "2024-05-26T09:10:01.476305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6f06dd284f8b",
   "metadata": {},
   "source": [
    "##### 3 - Combining the Edits\n",
    "\n",
    "Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be `edit_one_letter()` and `edit_two_letters()`.\n",
    "\n",
    "##### 3.1 - Edit One Letter\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "##### Exercise 8 - edit_one_letter\n",
    "\n",
    "**Instructions**: Implement the `edit_one_letter` function to get all the possible edits that are one edit away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The 'switch' function  is a less common edit function, so its use will be selected by an \"allow_switches\" input argument.\n",
    "\n",
    "Note that those functions return *lists* while this function should return a *python set*. Utilizing a set eliminates any duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a11d8534fe4fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:11:17.617836Z",
     "start_time": "2024-05-26T09:11:17.612743Z"
    }
   },
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "\n",
    "    # Generate all possible words with one letter replaced\n",
    "    replace_set = set(replace_letter(word))\n",
    "    edit_one_set.update(replace_set)\n",
    "    \n",
    "    # Generate all possible words with one letter inserted\n",
    "    insert_set = set(insert_letter(word))\n",
    "    edit_one_set.update(insert_set)\n",
    "    \n",
    "    # Generate all possible words with one letter deleted\n",
    "    delete_set = set(delete_letter(word))\n",
    "    edit_one_set.update(delete_set)\n",
    "    \n",
    "    # Optionally generate all possible words with two adjacent letters switched\n",
    "    if allow_switches:\n",
    "        switch_set = set(switch_letter(word))\n",
    "        edit_one_set.update(switch_set)\n",
    "    \n",
    "    # return this as a set and not a list\n",
    "    return set(edit_one_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e3516422cbcbaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:11:27.863382Z",
     "start_time": "2024-05-26T09:11:27.858774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf05627a0be8af3",
   "metadata": {},
   "source": [
    "##### 3.2 - Edit Two Letters\n",
    "\n",
    "<a name='ex-9'></a>\n",
    "##### Exercise 9 - edit_two_letters\n",
    "\n",
    "Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again. \n",
    "\n",
    "**Instructions**: Implement the `edit_two_letters` function that returns a set of words that are two edits away. Note that creating additional edits based on the `edit_one_letter` function may 'restore' some one_edits to zero or one edits. That is allowed here. This is accounted for in get_corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75134bf19b649de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:12:25.637323Z",
     "start_time": "2024-05-26T09:12:25.631440Z"
    }
   },
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "\n",
    "    # Generate all possible words with one edit\n",
    "    edit_one_set = edit_one_letter(word, allow_switches)\n",
    "    \n",
    "    # For each word in the edit_one_set, generate all possible words with another edit\n",
    "    for word_one_edit in edit_one_set:\n",
    "        edit_two_set.update(edit_one_letter(word_one_edit, allow_switches))  \n",
    "    \n",
    "    # return this as a set instead of a list\n",
    "    return set(edit_two_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76e1837b2acd0935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:12:34.325760Z",
     "start_time": "2024-05-26T09:12:34.312934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997649757cc9168",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84e2b2dcd78678c",
   "metadata": {},
   "source": [
    "##### 3.3 - Suggest Spelling Suggestions\n",
    "\n",
    "Now you will use your `edit_two_letters` function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type a.k.a your typing suggestion.\n",
    "\n",
    "##### Exercise 10 - get_corrections\n",
    "**Instructions**: Implement `get_corrections`, which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word). \n",
    "\n",
    "**Step 1:** Generate suggestions for a supplied word: You'll use the edit functions you have developed. The 'suggestion algorithm' should follow this logic: \n",
    "* If the word is in the vocabulary, suggest the word. \n",
    "* Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary, use those. \n",
    "* Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary, use those. \n",
    "* Otherwise, suggest the input word.*  \n",
    "* The idea is that words generated from fewer edits are more likely than words with more edits.\n",
    "\n",
    "\n",
    "Note: \n",
    "- Edits of two letters may 'restore' strings to either zero or one edit. This algorithm accounts for this by preferentially selecting lower distance edits first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb63386e557b84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef5b6e85bc481a0f",
   "metadata": {},
   "source": [
    "##### Short circuit\n",
    "In Python, logical operations such as `and` and `or` have two useful properties. They can operate on lists and they have ['short-circuit' behavior](https://docs.python.org/3/library/stdtypes.html). Try these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4fb6d319dcec88b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:14:53.360735Z",
     "start_time": "2024-05-26T09:14:53.355776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['a', 'b']\n",
      "['Most', 'Likely']\n",
      "['least', 'of', 'all']\n"
     ]
    }
   ],
   "source": [
    "# example of logical operation on lists or sets\n",
    "print( [] and [\"a\",\"b\"] )\n",
    "print( [] or [\"a\",\"b\"] )\n",
    "#example of Short circuit behavior\n",
    "val1 =  [\"Most\",\"Likely\"] or [\"Less\",\"so\"] or [\"least\",\"of\",\"all\"]  # selects first, does not evalute remainder\n",
    "print(val1)\n",
    "val2 =  [] or [] or [\"least\",\"of\",\"all\"] # continues evaluation until there is a non-empty list\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a20114b13f93f7",
   "metadata": {},
   "source": [
    "The logical `or` could be used to implement the suggestion algorithm very compactly. Alternately, if/elif/else constructs could be used.\n",
    " \n",
    "**Step 2**: Create a 'best_words' dictionary where the 'key' is a suggestion and the 'value' is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.\n",
    "\n",
    "**Step 3**: Select the n best suggestions. There may be fewer than n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b1d859af86d95",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>edit_one_letter and edit_two_letters return *python sets*. </li>\n",
    "    <li> Sets have a handy <a href=\"https://docs.python.org/2/library/sets.html\" > set.intersection </a> feature</li>\n",
    "    <li>To find the keys that have the highest values in a dictionary, you can use the Counter dictionary to create a Counter object from a regular dictionary.  Then you can use Counter.most_common(n) to get the n most common keys.\n",
    "    </li>\n",
    "    <li>To find the intersection of two sets, you can use set.intersection or the & operator.</li>\n",
    "    <li>If you are not as familiar with short circuit syntax (as shown above), feel free to use if else statements instead.</li>\n",
    "    <li>To use an if statement to check of a set is empty, use 'if not x:' syntax </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc2d30da4320ef06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:15:42.598763Z",
     "start_time": "2024-05-26T09:15:42.592861Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    # If the word is in the vocabulary, suggest the word\n",
    "    if word in vocab:\n",
    "        suggestions = [(word, probs[word])]\n",
    "    \n",
    "    # Otherwise, if there are suggestions from edit_one_letter that are in the vocabulary, use those\n",
    "    else:\n",
    "        edit_one_set = edit_one_letter(word)\n",
    "        edit_one_suggestions = [(w, probs[w]) for w in edit_one_set if w in vocab]\n",
    "        suggestions = edit_one_suggestions or suggestions\n",
    "    \n",
    "    # Otherwise, if there are suggestions from edit_two_letters that are in the vocabulary, use those\n",
    "    if not suggestions:\n",
    "        edit_two_set = edit_two_letters(word)\n",
    "        edit_two_suggestions = [(w, probs[w]) for w in edit_two_set if w in vocab]\n",
    "        suggestions = edit_two_suggestions or suggestions\n",
    "    \n",
    "    # Otherwise, suggest the input word\n",
    "    if not suggestions:\n",
    "        suggestions = [(word, 0)]\n",
    "    \n",
    "    # Get the n best suggestions based on probability\n",
    "    n_best = sorted(suggestions, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f585439972de8880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:15:50.802316Z",
     "start_time": "2024-05-26T09:15:50.797595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  dys \n",
      "suggestions =  [('dye', 1.865184466743761e-05), ('days', 0.0004103405826836274)]\n",
      "word 0: days, probability 0.000410\n",
      "word 1: dye, probability 0.000019\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38dd42016fff79",
   "metadata": {},
   "source": [
    "##### 4 - Minimum Edit Distance\n",
    "\n",
    "Now that you have implemented your auto-correct, how do you evaluate the similarity between two strings? For example: 'waht' and 'what'\n",
    "\n",
    "Also how do you efficiently find the shortest path to go from the word, 'waht' to the word 'what'?\n",
    "\n",
    "You will implement a dynamic programming system that will tell you the minimum number of edits required to convert a string into another string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7708d7be24263680",
   "metadata": {},
   "source": [
    "#### 4.1 - Dynamic Programming\n",
    "\n",
    "Dynamic Programming breaks a problem down into subproblems which can be combined to form the final solution. Here, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.\n",
    "\n",
    "You have to create a matrix and update each element in the matrix as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72162f883e9ecc8a",
   "metadata": {},
   "source": [
    "The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string \"EER\" to \"\". The first row is initialized to represent the cumulative cost of inserting the target characters to convert from \"\" to \"NEAR\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae0394d7829207",
   "metadata": {},
   "source": [
    "![image](./pomocne_soubory/EditDistInit4.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9c023cdc607cb",
   "metadata": {},
   "source": [
    "![image](./pomocne_soubory/EditDistExample1.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b96fe4948d2b1",
   "metadata": {},
   "source": [
    "#### Exercise 11 - min_edit_distance\n",
    "\n",
    "Again, the word \"substitution\" appears in the figure, but think of this as \"replacement\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f61a2c1f56d92",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The range(start, stop, step) function excludes 'stop' from its output</li>\n",
    "    <li><a href=\"\" > words </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c0859e8ec28d75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:21:56.952361Z",
     "start_time": "2024-05-26T09:21:56.944802Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        D[row, 0] = row * del_cost  # cost of deleting characters from source\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1, n+1):\n",
    "        D[0, col] = col * ins_cost  # cost of inserting characters to match target\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1, n+1):\n",
    "            \n",
    "            # Initialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column\n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation to calculate for D[row, col] (the minimum of three calculated costs)\n",
    "            D[row, col] = min(D[row-1, col] + del_cost,   # Deletion\n",
    "                              D[row, col-1] + ins_cost,   # Insertion\n",
    "                              D[row-1, col-1] + r_cost)  # Replacement or no change\n",
    "            \n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m, n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4187c64a87b13d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:22:11.270501Z",
     "start_time": "2024-05-26T09:22:11.253407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "# testing your implementation \n",
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a97191c9fb8e20f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:22:20.897920Z",
     "start_time": "2024-05-26T09:22:20.892903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  3 \n",
      "\n",
      "   #  n  e  a  r\n",
      "#  0  1  2  3  4\n",
      "e  1  2  1  2  3\n",
      "e  2  3  2  3  4\n",
      "r  3  4  3  4  3\n"
     ]
    }
   ],
   "source": [
    "# testing your implementation \n",
    "source =  'eer'\n",
    "target = 'near'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36db5ae789c3af9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:22:31.701290Z",
     "start_time": "2024-05-26T09:22:31.696705Z"
    }
   },
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a634f8b794215b",
   "metadata": {},
   "source": [
    "Expected Results\n",
    "(empty)\n",
    "The 'replace()' routine utilizes all letters a-z one of which returns the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ca00dc608f2466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:22:40.423940Z",
     "start_time": "2024-05-26T09:22:40.282254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer eer 0\n"
     ]
    }
   ],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 2 and min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5038d41cf09de04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c23e1f933cdc5e2",
   "metadata": {},
   "source": [
    "##### 5 - Backtrace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd4e3fb9932013d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:25:31.163622Z",
     "start_time": "2024-05-26T09:25:31.158122Z"
    }
   },
   "outputs": [],
   "source": [
    "def backtrace(matrix, source, target, i, j):\n",
    "    if i == 0 and j == 0:\n",
    "        return []\n",
    "    elif i == 0:\n",
    "        return [f\"INSERT {target[j-1]}\"] + backtrace(matrix, source, target, i, j-1)\n",
    "    elif j == 0:\n",
    "        return [f\"DELETE {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j)\n",
    "    else:\n",
    "        if matrix[i][j] == matrix[i-1][j-1] and source[i-1] == target[j-1]:\n",
    "            return [f\"KEEP {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j-1)\n",
    "        elif matrix[i][j] == matrix[i-1][j-1] + 1:\n",
    "            return [f\"REPLACE {source[i-1]} with {target[j-1]}\"] + backtrace(matrix, source, target, i-1, j-1)\n",
    "        elif matrix[i][j] == matrix[i-1][j] + 1:\n",
    "            return [f\"DELETE {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j)\n",
    "        else:\n",
    "            return [f\"INSERT {target[j-1]}\"] + backtrace(matrix, source, target, i, j-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0497c45c1c269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcda28-0296-4d08-9632-57b292f1c888",
   "metadata": {},
   "source": [
    "Part of Speech Tagging (POS) je proces, kdy kazdemu slovu priradime nejaky \"part of speech\", jako je podstatne jmeno, sloveso, atp. Jeden z moznych modelu jsou markovske processy, hidden markov chains a Viterbi Algoritmus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ba29d-bac5-4b5d-8b99-8f7895dd3529",
   "metadata": {},
   "source": [
    "POS se pouziva k taggovani \n",
    "- jmen,\n",
    "- speech recognition,\n",
    "- coreference resolution (na co ukazuji zajmena ve vete). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5fb07-40b1-4dba-b25e-eac30095e6d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LAB: POS, Prace se textem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cb41b1-af3b-458f-9804-a15ce962cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lines from 'WSJ_02-21.pos' file and save them into the 'lines' variable\n",
    "with open(\"./pomocne_soubory/WSJ_02-21.pos\", 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fe47e1-4d26-4ed6-807b-483ede6550a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tWord \tTag\n",
      "\n",
      "line number 1: In\tIN\n",
      "\n",
      "line number 2: an\tDT\n",
      "\n",
      "line number 3: Oct.\tNNP\n",
      "\n",
      "line number 4: 19\tCD\n",
      "\n",
      "line number 5: review\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print columns for reference\n",
    "print(\"\\t\\tWord\", \"\\tTag\\n\")\n",
    "\n",
    "# Print first five lines of the dataset\n",
    "for i in range(5):\n",
    "    print(f'line number {i+1}: {lines[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47933-1502-4c63-86d7-30e8b8a766d6",
   "metadata": {},
   "source": [
    "Vyznam zkratek pro Tag je mozne najit [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ed89df-35ba-4138-9bfb-584c018f407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In\\tIN\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first line (unformatted)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247d44a-4216-45c0-a0c1-943db9f4fc02",
   "metadata": {},
   "source": [
    "Indeed there is a tab between the word and the tag and a newline at the end of each line.\n",
    "\n",
    "#### Creating a vocabulary\n",
    "\n",
    "Now that you understand how the dataset is structured, you will create a vocabulary out of it. A vocabulary is made up of every word that appeared at least 2 times in the dataset. \n",
    "For this, follow these steps:\n",
    "- Get only the words from the dataset\n",
    "- Use a defaultdict to count the number of times each word appears\n",
    "- Filter the dict to only include words that appeared at least 2 times\n",
    "- Create a list out of the filtered dict\n",
    "- Sort the list\n",
    "\n",
    "For step 1 you can use the fact that every word and tag are separated by a tab and that words always come first. Using list comprehension the words list can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927b0de2-1d4f-4d22-822d-b31b8b988aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words from each line in the dataset\n",
    "words = [line.split('\\t')[0] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df4c89-6d51-4e9d-bb60-bbab21c1d0a8",
   "metadata": {},
   "source": [
    "Step 2 can be done easily by leveraging `defaultdict`. In case you aren't familiar with defaultdicts they are a special kind of dictionaries that **return the \"zero\" value of a type if you try to access a key that does not exist**. Since you want the frequencies of words, you should define the defaultdict with a type of `int`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415351ee-5b83-4751-b0f1-91bd8a9622d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define defaultdict of type 'int'\n",
    "freq = defaultdict(int)\n",
    "\n",
    "# Count frequency of ocurrence for each word in the dataset\n",
    "for word in words:\n",
    "    freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4739fb44-28e1-4d3a-8492-502365abf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary by filtering the 'freq' dictionary\n",
    "vocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3740cc89-759d-4479-aa95-6eee5e1f4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early\n",
      "Earnings\n",
      "Earth\n",
      "Earthquake\n",
      "East\n"
     ]
    }
   ],
   "source": [
    "# Sort the vocabulary\n",
    "vocab.sort()\n",
    "\n",
    "# Print some random values of the vocabulary\n",
    "for i in range(4000, 4005):\n",
    "    print(vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80272f84-7182-450e-bdde-d2c03682a313",
   "metadata": {},
   "source": [
    "#### Processing new text sources\n",
    "##### Dealing with unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f4451-7d59-46ad-b3b1-578dad915612",
   "metadata": {},
   "source": [
    "Now that you have a vocabulary, you will use it when processing new text sources. **A new text will have words that do not appear in the current vocabulary**. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding `unknown token`. \n",
    "\n",
    "This function will do the following checks and return an appropriate token:\n",
    "\n",
    "   - Check if the unknown word contains any character that is a digit \n",
    "       - return `--unk_digit--`\n",
    "   - Check if the unknown word contains any punctuation character \n",
    "       - return `--unk_punct--`\n",
    "   - Check if the unknown word contains any upper-case character \n",
    "       - return `--unk_upper--`\n",
    "   - Check if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb \n",
    "        - return `--unk_noun--`, `--unk_verb--`, `--unk_adj--`, `--unk_adv--` respectively\n",
    "\n",
    "If a word fails to fall under any condition then its token will be a plain `--unk--`. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns. \n",
    "\n",
    "This function is implemented next. Notice that the `any()` function is being heavily used. It returns `True` if at least one of the cases it evaluates is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99c70d5-8790-49a0-889f-6b0b917ad5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "    \"\"\"\n",
    "    Assign tokens to unknown words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Punctuation characters\n",
    "    # Try printing them out in a new cell!\n",
    "    punct = set(string.punctuation)\n",
    "    \n",
    "    # Suffixes\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    # Loop the characters in the word, check if any is a digit\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is a punctuation character\n",
    "    elif any(char in punct for char in word):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is an upper case character\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Check if word ends with any noun suffix\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Check if word ends with any verb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Check if word ends with any adjective suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Check if word ends with any adverb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    \n",
    "    # If none of the previous criteria is met, return plain unknown\n",
    "    return \"--unk--\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680919d3-1142-4bb0-a929-5acd47397f2b",
   "metadata": {},
   "source": [
    "##### Getting the correct tag for a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84855568-c660-4f71-9541-cdbb980d9c46",
   "metadata": {},
   "source": [
    "All that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\n",
    "\n",
    "This function should check if a line is empty and if so, it should return a placeholder word and tag, `--n--` and `--s--` respectively. \n",
    "\n",
    "If not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function `assign_unk()` should be used.\n",
    "\n",
    "The function is implemented next. Notice that the `split()` method can be used without specifying the delimiter, in which case it will default to any whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3ece5b-f93c-4740-8fa8-959a7519fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # If line is empty return placeholders for word and tag\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "    else:\n",
    "        # Split line to separate word and tag\n",
    "        word, tag = line.split()\n",
    "        # Check if word is not in vocabulary\n",
    "        if word not in vocab: \n",
    "            # Handle unknown word\n",
    "            tag = assign_unk(word)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64a8660-e097-4b63-bdf2-a58903829dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--n--', '--s--')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b0b375-bc40-4276-9bc9-0dfbc23ef7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In', 'IN')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('In\\tIN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86d72e4-8a42-4b99-b042-e9be087cea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tardigrade', '--unk--')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('tardigrade\\tNN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e88f67-e775-4438-b461-d62ed228f87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scrutinize', '--unk_verb--')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('scrutinize\\tVB\\n', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ff3e1-a193-4a4b-8f89-1a14a13427da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Markovsky Retezce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70623ef4-1f87-4037-a63d-e1feb98ca133",
   "metadata": {},
   "source": [
    "Tady asi neni poradne o cem. Proste klasicky markovsky retezce. Stavovy prostor, prechazi se mezi stavy pomoci matice prechodu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823527b343c91382",
   "metadata": {},
   "source": [
    "!<img src=\"./pomocne_soubory/markov_chains.png\"  width=\"920\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21e6a7eb3d92c7",
   "metadata": {},
   "source": [
    "Skryte maarkovske procesy se lisi tim, ze maji jeste jednu vrstvu, kterou pozorujeme, tzv. emission probabilities. \n",
    "Transition probabilities nevidime a nemuzeme je pozorovat. Nejcasteji se parametry modelu odhaduji pomoci maximalni \n",
    "verohodnosti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d1c749ec9fd1b",
   "metadata": {},
   "source": [
    "!<img src=\"./pomocne_soubory/hidden_mc_1.png\"  width=\"920\">\n",
    "!<img src=\"./pomocne_soubory/hidden_mc_2.png\"  width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a21ab6d4514de",
   "metadata": {},
   "source": [
    "Vzhledem k tomu, ze Markovsky retezec je zalozeny na predpokladu, ze soucasny stav $t_i$ zavisi jen na predcozim \n",
    "stavu v case $t_{i-1}$, tak potrebujeme napocitat pravdepodobnostni distribuci pro vsechny stavy $t_i$ v zavislosti na\n",
    "vsech predchozich stavech $t_{i-1}$. Tuto distribuci muzeme zapsat jako:\n",
    "$$P(t_i|t_{i-1}, t_{i-2}, ..., t_1) = P(t_i|t_{i-1})$$\n",
    "Tuto distribuci muzeme zapsat jako matici prechodu $A$, kde $A_{ij} = P(t_j|t_i)$.\n",
    "Pravdepodobnosti prechodu $A$ muzeme odhadnout pomoci maximalni verohodnosti. V nasem pripade pak \n",
    "$$ P(t_i|t_{i-1}) = \n",
    "\\frac{C(t_{i-1}, t_i)} {\\sum_{j=1}^N C(t_{i-1}, t_j))}\n",
    "$$\n",
    "kde $C(t_{i-1}, t_i)$ je pocet prechodu kolikrat se tag $t_{i-1}$ objevil pred tagem $t_i$ a $\\sum_{j=1}^N C(t_{i-1},\n",
    " t_j) = C(t_{i-1})$ je pocet vyskytu tagu $t_{i-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5abd0a426d326f",
   "metadata": {},
   "source": [
    "Abychom se vyhli problemum, kdy pro nejakou pripad $t_{i-1, i}$ nebyl v trenovacim datasetu, tak se pouziva Laplace \n",
    "smoothing, kdy se ke kazdemu $C(t_{i-1}, t_i)$ pricita male $\\epsilon$. Vysledny vzorec pak vypada:\n",
    "$$ P(t_i|t_{i-1}) =\n",
    "\\frac{C(t_{i-1}, t_i) + \\epsilon} {\\sum_{j=1}^N C(t_{i-1}, t_j) + \\epsilon \\cdot N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa922b74f2a837a",
   "metadata": {},
   "source": [
    "Analogicky se pak vypocitava matice emisi $B$, kde $B_{ij} = P(w_j|t_i)$, kde $w_j$ je slovo a $t_i$ je tag. Vysledny vzorec\n",
    "pak vypada:\n",
    "$$ P(w_j|t_i) =\n",
    "\\frac{C(t_i, w_j) + \\epsilon} {\\sum_{j=1}^N C(t_i, w_j) + \\epsilon \\cdot N} = \n",
    "\\frac{C(t_i, w_j) + \\epsilon} {C(t_i) + \\epsilon \\cdot V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed44531a498d17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LAB: Parts-of-Speech Tagging with Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc09ee1b9fb6a5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T11:51:55.566241Z",
     "start_time": "2024-05-27T11:51:55.554637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define tags for Adverb, Noun and To (the preposition) , respectively\n",
    "tags = ['RB', 'NN', 'TO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f947f7c0c8857ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:00:44.142414Z",
     "start_time": "2024-05-27T12:00:44.136903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define 'transition_counts' dictionary\n",
    "# Note: values are the same as the ones in the assignment\n",
    "transition_counts = {\n",
    "    ('NN', 'NN'): 16241,\n",
    "    ('RB', 'RB'): 2263,\n",
    "    ('TO', 'TO'): 2,\n",
    "    ('NN', 'TO'): 5256,\n",
    "    ('RB', 'TO'): 855,\n",
    "    ('TO', 'NN'): 734,\n",
    "    ('NN', 'RB'): 2431,\n",
    "    ('RB', 'NN'): 358,\n",
    "    ('TO', 'RB'): 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b86ab8275d26b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:01:10.699921Z",
     "start_time": "2024-05-27T12:01:10.696820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the number of tags in the 'num_tags' variable\n",
    "num_tags = len(tags)\n",
    "\n",
    "# Initialize a 3X3 numpy array with zeros\n",
    "transition_matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8128d246e19947fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:01:12.719274Z",
     "start_time": "2024-05-27T12:01:12.716221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of the matrix\n",
    "transition_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc24029dab788ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:01:24.455425Z",
     "start_time": "2024-05-27T12:01:24.452812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'RB', 'TO']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sorted version of the tag's list\n",
    "sorted_tags = sorted(tags)\n",
    "\n",
    "# Print sorted list\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8de51ee7991e385b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:01:31.218068Z",
     "start_time": "2024-05-27T12:01:31.210217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n",
       "       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n",
       "       [7.3400e+02, 2.0000e+02, 2.0000e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop rows\n",
    "for i in range(num_tags):\n",
    "    # Loop columns\n",
    "    for j in range(num_tags):\n",
    "        # Define tag pair\n",
    "        tag_tuple = (sorted_tags[i], sorted_tags[j])\n",
    "        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n",
    "        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41968b61e4f71964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:02:17.369647Z",
     "start_time": "2024-05-27T12:02:17.367677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define 'print_matrix' function\n",
    "def print_matrix(matrix):\n",
    "    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc2d23bc21102dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:02:26.166968Z",
     "start_time": "2024-05-27T12:02:26.146545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NN      RB      TO\n",
      "NN  16241.0  2431.0  5256.0\n",
      "RB    358.0  2263.0   855.0\n",
      "TO    734.0   200.0     2.0\n"
     ]
    }
   ],
   "source": [
    "# Print the 'transition_matrix' by calling the 'print_matrix' function\n",
    "print_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc261134e50eaaad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:02:51.880533Z",
     "start_time": "2024-05-27T12:02:51.876845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23928.],\n",
       "       [ 3476.],\n",
       "       [  936.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sum of row for each row\n",
    "rows_sum = transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Print sum of rows\n",
    "rows_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d894ec5ae6e4e50d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:03:01.560925Z",
     "start_time": "2024-05-27T12:03:01.556055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NN        RB        TO\n",
      "NN  0.678745  0.101596  0.219659\n",
      "RB  0.102992  0.651036  0.245972\n",
      "TO  0.784188  0.213675  0.002137\n"
     ]
    }
   ],
   "source": [
    "# Normalize transition matrix\n",
    "transition_matrix = transition_matrix / rows_sum\n",
    "\n",
    "# Print normalized matrix\n",
    "print_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b04ee99a1e024ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:03:14.169104Z",
     "start_time": "2024-05-27T12:03:14.166060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "942dde20f2e0bada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:03:44.897879Z",
     "start_time": "2024-05-27T12:03:44.893567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy transition matrix for for-loop example\n",
    "t_matrix_for = np.copy(transition_matrix)\n",
    "\n",
    "# Copy transition matrix for numpy functions example\n",
    "t_matrix_np = np.copy(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a83774e15fe9d8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:04:06.411578Z",
     "start_time": "2024-05-27T12:04:06.404789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NN        RB        TO\n",
      "NN  10.761549  0.101596  0.219659\n",
      "RB   0.102992  8.804673  0.245972\n",
      "TO   0.784188  0.213675  6.843752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_76077/158205974.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n"
     ]
    }
   ],
   "source": [
    "# Loop values in the diagonal\n",
    "for i in range(num_tags):\n",
    "    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n",
    "\n",
    "# Print matrix\n",
    "print_matrix(t_matrix_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bc5d0a7bedd811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:04:13.172152Z",
     "start_time": "2024-05-27T12:04:13.166940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save diagonal in a numpy array\n",
    "d = np.diag(t_matrix_np)\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1be31c2b43653ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:04:18.815915Z",
     "start_time": "2024-05-27T12:04:18.812605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape diagonal numpy array\n",
    "d = np.reshape(d, (3,1))\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1dc794582c133445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:04:24.339344Z",
     "start_time": "2024-05-27T12:04:24.332740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NN        RB        TO\n",
      "NN  10.761549  0.101596  0.219659\n",
      "RB   0.102992  8.804673  0.245972\n",
      "TO   0.784188  0.213675  6.843752\n"
     ]
    }
   ],
   "source": [
    "# Perform the vectorized operation\n",
    "d = d + np.vectorize(math.log)(rows_sum)\n",
    "\n",
    "# Use numpy's 'fill_diagonal' function to update the diagonal\n",
    "np.fill_diagonal(t_matrix_np, d)\n",
    "\n",
    "# Print the matrix\n",
    "print_matrix(t_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f015842d509725c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T12:04:29.526867Z",
     "start_time": "2024-05-27T12:04:29.524071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for equality\n",
    "t_matrix_for == t_matrix_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d22cbb7f36c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b163ccae62b455",
   "metadata": {},
   "source": [
    "Viterbiho algoritmus je dynamicky programovaci algoritmus, ktery se pouziva k nalezeni nejpravdepodobnejsi \n",
    "posloupnosti skrytych stavu (take zvana Viterbiho cesta), ktera vede k posloupnosti pozorovanych udalosti, zejmena v\n",
    " kontextu skrytych Markovovych modelu (HMM). Tento algoritmus je siroce pouzivan v ruznych aplikacich, jako je \n",
    " rozpoznavani reci, bioinformatika a dekodovani konvolucnich kodu. \n",
    "V nasem pripade budeme pouzivat Viterbiho algoritmus k tagovani slov v textu. \n",
    "Zjednodusene: napocitavame pravdepoodbnosti prechodu pro kazde pozorovane slovo a ukladame si indexy tagu z kterych \n",
    "jdeme. A zaroven si ukladame indexy tagu z kterych jdeme. Jakmile mame plnou matici vezmeme nejpravdepodonejsi cestu \n",
    "a vracime se pres indexy v D matici, abychom nasli tagy vsech predchozich slov. \n",
    "Ilustrativne zde: https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/9BigR/viterbi-forward-pass a \n",
    "tady: https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/4OAHP/viterbi-backward-pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ca8fbfc874dd8",
   "metadata": {},
   "source": [
    "The Viterbi algorithm is a dynamic programming algorithm used to find the most probable sequence of hidden states \n",
    "(also known as the Viterbi path) that results in a sequence of observed events, especially in the context of Hidden Markov Models (HMMs). This algorithm is widely used in various applications such as speech recognition, bioinformatics, and decoding of convolutional codes.\n",
    "\n",
    "#### Key Components of the Viterbi Algorithm\n",
    "\n",
    "1. **States**: $ S = \\{s_1, s_2, \\ldots, s_N\\} $ - Set of possible states in the HMM.\n",
    "2. **Observations**: $ O = \\{o_1, o_2, \\ldots, o_T\\} $ - Sequence of observed events.\n",
    "3. **Initial Probabilities**: $ \\pi = \\{\\pi_i\\} $ where $ \\pi_i $ is the probability of starting in state $ s_i $.\n",
    "4. **Transition Probabilities**: $ A = \\{a_{ij}\\} $ where $ a_{ij} $ is the probability of transitioning from state $ s_i $ to state $ s_j $.\n",
    "5. **Emission Probabilities**: $ B = \\{b_j(o_t)\\} $ where $ b_j(o_t) $ is the probability of observing $ o_t $ from state $ s_j $.\n",
    "\n",
    "#### Viterbi Algorithm Steps\n",
    "\n",
    "1. **Initialization**:\n",
    "    For each state $ s_i $:\n",
    "    $$\n",
    "    \\delta_1(i) = \\pi_i b_i(o_1)\n",
    "    $$\n",
    "    $$\n",
    "    \\psi_1(i) = 0\n",
    "    $$\n",
    "Where $ \\delta_1(i) $ is the highest probability of any path that reaches state $ s_i $ at time $ t = 1 $, and $ \\psi_1(i) $ is the backpointer to reconstruct the path.\n",
    "\n",
    "2. **Recursion**:\n",
    "    For each time step $ t = 2, 3, \\ldots, T $:\n",
    "    For each state $ s_j $:\n",
    "    $$\n",
    "    \\delta_t(j) = \\max_{i} [\\delta_{t-1}(i) a_{ij}] b_j(o_t)\n",
    "    $$\n",
    "    $$\n",
    "    \\psi_t(j) = \\arg\\max_{i} [\\delta_{t-1}(i) a_{ij}]\n",
    "    $$\n",
    "Where $ \\delta_t(j) $ is the highest probability of any path that reaches state $ s_j $ at time $ t $, and $ \\psi_t(j) $ is the backpointer.\n",
    "\n",
    "3. **Termination**:\n",
    "    $$\n",
    "    P^* = \\max_{i} [\\delta_T(i)]\n",
    "    $$\n",
    "    $$\n",
    "    q_T^* = \\arg\\max_{i} [\\delta_T(i)]\n",
    "    $$\n",
    "Where $ P^* $ is the highest probability of the most probable path, and $ q_T^* $ is the final state of this path.\n",
    "\n",
    "4. **Path Backtracking**:\n",
    "    For $ t = T-1, T-2, \\ldots, 1 $:\n",
    "    $$\n",
    "    q_t^* = \\psi_{t+1}(q_{t+1}^*)\n",
    "    $$\n",
    "This step reconstructs the most probable path by backtracking from the final state.\n",
    "\n",
    "#### Example Illustration\n",
    "\n",
    "Suppose we have an HMM with 2 states (Rainy and Sunny) and 3 possible observations (Walk, Shop, Clean). Let:\n",
    "\n",
    "- Initial probabilities: $ \\pi = \\{0.6, 0.4\\} $ (for Rainy and Sunny respectively)\n",
    "- Transition matrix:\n",
    "  $$\n",
    "  A = \\begin{bmatrix}\n",
    "  0.7 & 0.3 \\\\\n",
    "  0.4 & 0.6\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- Emission matrix:\n",
    "  $$\n",
    "  B = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 \\\\\n",
    "  0.6 & 0.3 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "If we observe the sequence $ O = \\{Walk, Shop, Clean\\} $:\n",
    "\n",
    "1. **Initialization**:\n",
    "    $$\n",
    "    \\delta_1(Rainy) = \\pi_1 \\cdot b_1(Walk) = 0.6 \\cdot 0.1 = 0.06\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_1(Sunny) = \\pi_2 \\cdot b_2(Walk) = 0.4 \\cdot 0.6 = 0.24\n",
    "    $$\n",
    "\n",
    "2. **Recursion for $ t = 2 $**:\n",
    "    $$\n",
    "    \\delta_2(Rainy) = \\max[ \\delta_1(Rainy) \\cdot a_{11}, \\delta_1(Sunny) \\cdot a_{21} ] \\cdot b_1(Shop)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Rainy) = \\max[ 0.06 \\cdot 0.7, 0.24 \\cdot 0.4 ] \\cdot 0.4 = 0.0384\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Sunny) = \\max[ \\delta_1(Rainy) \\cdot a_{12}, \\delta_1(Sunny) \\cdot a_{22} ] \\cdot b_2(Shop)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Sunny) = \\max[ 0.06 \\cdot 0.3, 0.24 \\cdot 0.6 ] \\cdot 0.3 = 0.0432\n",
    "    $$\n",
    "\n",
    "3. **Recursion for $ t = 3 $**:\n",
    "    $$\n",
    "    \\delta_3(Rainy) = \\max[ \\delta_2(Rainy) \\cdot a_{11}, \\delta_2(Sunny) \\cdot a_{21} ] \\cdot b_1(Clean)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Rainy) = \\max[ 0.0384 \\cdot 0.7, 0.0432 \\cdot 0.4 ] \\cdot 0.5 = 0.01344\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Sunny) = \\max[ \\delta_2(Rainy) \\cdot a_{12}, \\delta_2(Sunny) \\cdot a_{22} ] \\cdot b_2(Clean)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Sunny) = \\max[ 0.0384 \\cdot 0.3, 0.0432 \\cdot 0.6 ] \\cdot 0.1 = 0.002592\n",
    "    $$\n",
    "\n",
    "4. **Termination**:\n",
    "    $$\n",
    "    P^* = \\max[ \\delta_3(Rainy), \\delta_3(Sunny) ] = \\max[ 0.01344, 0.002592 ] = 0.01344\n",
    "    $$\n",
    "    $$\n",
    "    q_3^* = \\arg\\max[ \\delta_3(Rainy), \\delta_3(Sunny) ] = Rainy\n",
    "    $$\n",
    "\n",
    "5. **Path Backtracking**:\n",
    "    $$\n",
    "    q_2^* = \\psi_3(Rainy) = Sunny \\quad (since \\delta_2(Sunny) \\cdot a_{21} \\cdot b_1(Clean) was the max path)\n",
    "    $$\n",
    "    $$\n",
    "    q_1^* = \\psi_2(Sunny) = Sunny \\quad (since \\delta_1(Sunny) \\cdot a_{22} \\cdot b_2(Shop) was the max path)\n",
    "    $$\n",
    "\n",
    "Thus, the most probable hidden state sequence for the observations $ \\{Walk, Shop, Clean\\} $ is $ \\{Sunny, Sunny, Rainy\\} $.\n",
    "\n",
    "This explanation covers the essential steps and concepts involved in the Viterbi algorithm. If you have any specific questions or need further elaboration on any part, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7506db0e15f81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LAB: Assignment 2: POS, HMM, Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a8376f1c0b5d6",
   "metadata": {},
   "source": [
    "#### 0 - Data Sources\n",
    "This assignment will use two tagged data sets collected from the **Wall Street Journal (WSJ)**. \n",
    "\n",
    "[Here](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. \n",
    "- One data set (**WSJ-2_21.pos**) will be used for **training**.\n",
    "- The other (**WSJ-24.pos**) for **testing**. \n",
    "- The tagged training data has been preprocessed to form a vocabulary (**hmm_vocab.txt**). \n",
    "- The words in the vocabulary are words from the training set that were used two or more times. \n",
    "- The vocabulary is augmented with a set of 'unknown word tokens', described below. \n",
    "\n",
    "The training set will be used to create the emission, transition and tag counts. \n",
    "\n",
    "The test set (WSJ-24.pos) is read in to create `y`. \n",
    "- This contains both the test text and the true tag. \n",
    "- The test set has also been preprocessed to remove the tags to form **test_words.txt**. \n",
    "- This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in **utils_pos.py**. \n",
    "- This forms the list `prep`, the preprocessed text used to test our  POS taggers.\n",
    "\n",
    "A POS tagger will necessarily encounter words that are not in its datasets. \n",
    "- To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. \n",
    "- For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. \n",
    "- A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transition and tag data structures.\n",
    "\n",
    "\n",
    "<img src = \"pomocne_soubory/DataSources1.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49201e67025296ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:00:16.596055Z",
     "start_time": "2024-05-27T15:00:16.529078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"./pomocne_soubory/WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "94431e2b93f5923d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:00:30.722078Z",
     "start_time": "2024-05-27T15:00:30.717437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"./pomocne_soubory/hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb89762da30dc3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:00:43.207096Z",
     "start_time": "2024-05-27T15:00:43.199874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "':6\n",
      "'':7\n",
      "'40s:8\n",
      "'60s:9\n",
      "'70s:10\n",
      "'80s:11\n",
      "'86:12\n",
      "'90s:13\n",
      "'N:14\n",
      "'S:15\n",
      "'d:16\n",
      "'em:17\n",
      "'ll:18\n",
      "'m:19\n",
      "'n':20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {}\n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a54b7ae60823b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:01:04.845896Z",
     "start_time": "2024-05-27T15:01:04.833410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"./pomocne_soubory/WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "    \n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d564b49b6abbc23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:01:15.194459Z",
     "start_time": "2024-05-27T15:01:15.172015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of the test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"./pomocne_soubory/test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2ab1f1c93e2e",
   "metadata": {},
   "source": [
    "#### 1 - Parts-of-speech Tagging \n",
    "\n",
    "<a name='1.1'></a>\n",
    "##### 1.1 - Training\n",
    "You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. \n",
    "\n",
    "In this section, you will find the words that are not ambiguous. \n",
    "- For example, the word `is` is a verb and it is not ambiguous. \n",
    "- In the `WSJ` corpus, $86$% of the token are unambiguous (meaning they have only one tag) \n",
    "- About $14\\%$ are ambiguous (meaning that they have more than one tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7d67d854c2bc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860819295336c746",
   "metadata": {},
   "source": [
    "##### Transition counts\n",
    "- The first dictionary is the `transition_counts` dictionary which computes the number of times each tag happened next to another tag. \n",
    "\n",
    "This dictionary will be used to compute: \n",
    "$$P(t_i |t_{i-1}) \\tag{1}$$\n",
    "\n",
    "This is the probability of a tag at position $i$ given the tag at position $i-1$.\n",
    "\n",
    "In order for you to compute equation 1, you will create a `transition_counts` dictionary where \n",
    "- The keys are `(prev_tag, tag)`\n",
    "- The values are the number of times those two tags appeared in that order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618af5b2b4e0c72",
   "metadata": {},
   "source": [
    "##### Emission counts\n",
    "\n",
    "The second dictionary you will compute is the `emission_counts` dictionary. This dictionary will be used to compute:\n",
    "\n",
    "$$P(w_i|t_i)\\tag{2}$$\n",
    "\n",
    "In other words, you will use it to compute the probability of a word given its tag. \n",
    "\n",
    "In order for you to compute equation 2, you will create an `emission_counts` dictionary where \n",
    "- The keys are `(tag, word)` \n",
    "- The values are the number of times that pair showed up in your training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ea61481c03bcf",
   "metadata": {},
   "source": [
    "##### Tag counts\n",
    "\n",
    "The last dictionary you will compute is the `tag_counts` dictionary. \n",
    "- The key is the tag \n",
    "- The value is the number of times each tag appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb409675bb1fb8",
   "metadata": {},
   "source": [
    "#### Exercise 1 - create_dictionaries\n",
    "\n",
    "**Instructions:** Write a program that takes in the `training_corpus` and returns the three dictionaries mentioned above `transition_counts`, `emission_counts`, and `tag_counts`. \n",
    "- `emission_counts`: maps (tag, word) to the number of times it happened. \n",
    "- `transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. \n",
    "- `tag_counts`: maps (tag) to the number of times it has occured. \n",
    "\n",
    "Implementation note: This routine utilises *defaultdict*, which is a subclass of *dict*. \n",
    "- A standard Python dictionary throws a *KeyError* if you try to access an item with a key that is not currently in the dictionary. \n",
    "- In contrast, the *defaultdict* will create an item of the type of the argument, in this case an integer with the default value of 0. \n",
    "- See [defaultdict](https://docs.python.org/3.3/library/collections.html#defaultdict-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82729ca95f8975fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:05:15.050602Z",
     "start_time": "2024-05-27T15:05:15.045019Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab, verbose=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0 and verbose:\n",
    "            print(f\"word count = {i}\")\n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        # the function is defined as: get_word_tag(line, vocab)\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc1af3814bcda307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:05:25.675661Z",
     "start_time": "2024-05-27T15:05:24.912909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "568cdea5b01ad7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:05:34.301049Z",
     "start_time": "2024-05-27T15:05:34.298900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7dddaed1df4d0993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:05:46.583343Z",
     "start_time": "2024-05-27T15:05:46.467786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221148ca4a840a3",
   "metadata": {},
   "source": [
    "#### Exercise 2 - predict_pos\n",
    "\n",
    "**Instructions:** Implement `predict_pos` that computes the accuracy of your model. \n",
    "\n",
    "- This is a warm up exercise. \n",
    "- To assign a part of speech to a word, assign the most frequent POS for that word in the training set. \n",
    "- Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!\n",
    "- Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b32646c3678dcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:06:33.546382Z",
     "start_time": "2024-05-27T15:06:33.541822Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output:\n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Initialize total count to 0\n",
    "    total = 0\n",
    "    \n",
    "    for word, y_tup in zip(prep, y):\n",
    "        # Split the (word, POS) string into a list of two items\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        # Verify that y_tup contain both word and POS\n",
    "        if len(y_tup_l) == 2:\n",
    "            # Set the true POS label for this word\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word\n",
    "            continue\n",
    "        \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        # If the word is in the vocabulary...\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                # define the key as the tuple containing the POS and word\n",
    "                key = (pos, word)\n",
    "                \n",
    "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
    "                if key in emission_counts:\n",
    "                    # get the emission count of the (pos,word) tuple\n",
    "                    count = emission_counts[key]\n",
    "                    \n",
    "                    # keep track of the POS with the largest count\n",
    "                    if count > count_final:\n",
    "                        # update the final count (largest count)\n",
    "                        count_final = count\n",
    "                        # update the final POS\n",
    "                        pos_final = pos\n",
    "            \n",
    "            # If the final POS (with the largest count) matches the true POS:\n",
    "            if pos_final == true_label:\n",
    "                # Update the number of correct predictions\n",
    "                num_correct += 1\n",
    "        \n",
    "        # Keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "\n",
    "    accuracy = num_correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "475c9386abdce3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:06:40.248896Z",
     "start_time": "2024-05-27T15:06:40.144039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.9253\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f50806c903194",
   "metadata": {},
   "source": [
    "####  2.1 - Generating Matrices\n",
    "\n",
    "##### Creating the 'A' transition probabilities matrix\n",
    "Now that you have your `emission_counts`, `transition_counts`, and `tag_counts`, you will start implementing the Hidden Markov Model. \n",
    "\n",
    "This will allow you to quickly construct the \n",
    "- `A` transition probabilities matrix.\n",
    "- and the `B` emission probabilities matrix. \n",
    "\n",
    "You will also use some smoothing when computing these matrices. \n",
    "\n",
    "Here is an example of what the `A` transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n",
    "\n",
    "\n",
    "|**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...\n",
    "| --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----\n",
    "|**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...\n",
    "|**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...\n",
    "|**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...\n",
    "|**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...\n",
    "|**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...\n",
    "| ... |...| ...          | ...          | ...          | ...      | ...        | ...\n",
    "\n",
    "Note that the matrix above was computed with smoothing. \n",
    "\n",
    "Each cell gives you the probability to go from one part of speech to another. \n",
    "- In other words, there is a 4.47e-8 chance of going from parts-of-speech `TO` to `RP`. \n",
    "- The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n",
    "\n",
    "The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "944367cee71e48d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:07:26.433838Z",
     "start_time": "2024-05-27T15:07:26.430325Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "\n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "\n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "\n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i], all_tags[j])  # tuple of form (tag,tag)\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in trans_keys:  # Replace None in this line with the proper condition.\n",
    "\n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "\n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "\n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and total number of tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4818e7d9d95c060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:07:36.907083Z",
     "start_time": "2024-05-27T15:07:36.886810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "View a subset of transition matrix A\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a03a2754cd8b5",
   "metadata": {},
   "source": [
    "#### Create the 'B' emission probabilities matrix\n",
    "\n",
    "Now you will create the `B` emission matrix which computes the emission probability. \n",
    "\n",
    "You will use smoothing as defined below: \n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. \n",
    "\n",
    "The matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. \n",
    "\n",
    "Here is an example of the matrix, only a subset of tags and words are shown: \n",
    "<p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>\n",
    "\n",
    "|**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|\n",
    "|----|----|--------------|--------------|--------------|--------------|-------------|----|\n",
    "|**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|\n",
    "|**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|\n",
    "|**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|\n",
    "|**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|\n",
    "|**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|\n",
    "|**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|\n",
    "| ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1725c59213ed5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:08:08.713568Z",
     "start_time": "2024-05-27T15:08:08.710365Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
    "               within the function it'll be treated as a list\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for\n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # Get a set of all (POS, word) tuples \n",
    "    # from the keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "\n",
    "    # Go through each row (POS tags)\n",
    "    for i in range(num_tags): # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Go through each column (words)\n",
    "        for j in range(num_words): # Replace None in this line with the proper range.\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0 \n",
    "                    \n",
    "            # Define the (POS tag, word) tuple for this row and column\n",
    "            key = (all_tags[i], vocab[j])  # tuple of form (tag,word)\n",
    "            \n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emis_keys:  # Replace None in this line with the proper condition.\n",
    "                \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts[key]\n",
    "            \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5dfc5054e8f36a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:08:15.544707Z",
     "start_time": "2024-05-27T15:08:15.305207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "alpha = 0.001\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c84da154ab14",
   "metadata": {},
   "source": [
    "#### 3 - Viterbi Algorithm and Dynamic Programming\n",
    "\n",
    "In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. \n",
    "\n",
    "* **Initialization** - In this part you initialize the `best_paths` and `best_probabilities` matrices that you will be populating in `feed_forward`.\n",
    "* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n",
    "* **Feed backward**: This allows you to find the best path with the highest probabilities. \n",
    "\n",
    "<a name='3.1'></a>\n",
    "##### 3.1 - Initialization \n",
    "\n",
    "You will start by initializing two matrices of the same dimension. \n",
    "\n",
    "- best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
    "\n",
    "- best_paths: A matrix that helps you trace through the best possible path in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8c55ee5662933",
   "metadata": {},
   "source": [
    "#### Exercise 5 - initialize\n",
    "**Instructions**: \n",
    "Write a program below that initializes the `best_probs` and the `best_paths` matrix. \n",
    "\n",
    "Both matrices will be initialized to zero except for column zero of `best_probs`.  \n",
    "- Column zero of `best_probs` is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\"). \n",
    "- This allows you to reference the **A** matrix for the transition probability\n",
    "\n",
    "Here is how to initialize column 0 of `best_probs`:\n",
    "- The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\\textrm{best_probs}[s_{idx}, i]$.\n",
    "- This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\\mathbf{B}[i, vocab[corpus[0]]]$.\n",
    "- Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). \n",
    "- **vocab** is a dictionary that returns the unique integer that refers to that particular word.\n",
    "\n",
    "Conceptually, it looks like this:\n",
    "$\\textrm{best_probs}[i, 0] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, vocab[corpus[0]] ]$\n",
    "\n",
    "\n",
    "In order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:\n",
    "\n",
    "$best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n",
    "\n",
    "Please use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28950a5a9b090e12",
   "metadata": {},
   "source": [
    "The example below shows the initialization assuming the corpus starts with the phrase \"Loss tracks upward\".\n",
    "\n",
    "<img src = \"pomocne_soubory/Initialize4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "901ddfe931550958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:10:00.298351Z",
     "start_time": "2024-05-27T15:10:00.293430Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    # POS tags in the rows, number of words in the corpus as the columns\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    # POS tags in the rows, number of words in the corpus as columns\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "\n",
    "    # Go through each of the POS tags\n",
    "    for i in range(num_tags):  # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Initialize best_probs at POS tag 'i', column 0\n",
    "        # Check the formula in the instructions above\n",
    "        best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]])\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "573d18e877f24b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:10:06.627673Z",
     "start_time": "2024-05-27T15:10:06.625431Z"
    }
   },
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5290f5e5497ad916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:10:11.447513Z",
     "start_time": "2024-05-27T15:10:11.445156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\")\n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62474106c4912eba",
   "metadata": {},
   "source": [
    "####  3.2 - Viterbi Forward\n",
    "\n",
    "In this part of the assignment, you will implement the `viterbi_forward` segment. In other words, you will populate your `best_probs` and `best_paths` matrices.\n",
    "- Walk forward through the corpus.\n",
    "- For each word, compute a probability for each possible tag. \n",
    "- Unlike the previous algorithm `predict_pos` (the 'warm-up' exercise), this will include the path up to that (word,tag) combination. \n",
    "\n",
    "Here is an example with a three-word corpus \"Loss tracks upward\":\n",
    "- Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. \n",
    "- In the diagram below, the first word \"Loss\" is already initialized. \n",
    "- The algorithm will compute a probability for each of the potential tags in the second and future words. \n",
    "\n",
    "Compute the probability that the tag of the second word ('tracks') is a verb, 3rd person singular present (VBZ).  \n",
    "- In the `best_probs` matrix, go to the column of the second word ('tracks'), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\n",
    "- Examine each of the paths from the tags of the first word ('Loss') and choose the most likely path.  \n",
    "- An example of the calculation for **one** of those paths is the path from ('Loss', NN) to ('tracks', VBZ).\n",
    "- The log of the probability of the path up to and including the first word 'Loss' having POS tag NN is $-14.32$.  The `best_probs` matrix contains this value -14.32 in the column for 'Loss' and row for 'NN'.\n",
    "- Find the probability that NN transitions to VBZ.  To find this probability, go to the `A` transition matrix, and go to the row for 'NN' and the column for 'VBZ'.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. \n",
    "- Find the log of the probability that the tag VBS would 'emit' the word 'tracks'.  To find this, look at the 'B' emission matrix in row 'VBZ' and the column for the word 'tracks'.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.\n",
    "- The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the `best_probs` matrix at row 'VBZ' and column 'tracks' (as seen in the cell that is highlighted in light orange in the diagram).\n",
    "- All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column 'tracks' of matrix `best_probs`, and so the most likely path to 'VBZ' is from 'NN'.  'NN' is in row 20 of the `best_probs` matrix, so $20$ is the most likely path.\n",
    "- Store the most likely path $20$ in the `best_paths` table.  This is highlighted in light orange in the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f37d7e52e9b91d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cee0ec187ad6d808",
   "metadata": {},
   "source": [
    "The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:\n",
    "\n",
    "$\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})$\n",
    "\n",
    "where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.\n",
    "\n",
    "$\\mathrm{path} = k$\n",
    "\n",
    "where $k$ is the integer representing the previous POS tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9034c42a832c84",
   "metadata": {},
   "source": [
    "#### Exercise 6 - viterbi_forward\n",
    "\n",
    "Instructions: Implement the `viterbi_forward` algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n",
    "\n",
    "```\n",
    "for each word in the corpus\n",
    "\n",
    "    for each POS tag type that this word may be\n",
    "    \n",
    "        for POS tag type that the previous word could be\n",
    "        \n",
    "            compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n",
    "            \n",
    "            retain the highest probability computed for the current word\n",
    "            \n",
    "            set best_probs to this highest probability\n",
    "            \n",
    "            set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability \n",
    "```\n",
    "\n",
    "Please use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e83579a6bea2",
   "metadata": {},
   "source": [
    ".<img src = \"./pomocne_soubory/Forward4.PNG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7c1273872fe92b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:12:32.543467Z",
     "start_time": "2024-05-27T15:12:32.539206Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transition and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0 and verbose:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # for every pos tag\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags):\n",
    "            \n",
    "                # Calculate the probability\n",
    "                prob = best_probs[k, i-1] + \\\n",
    "                       math.log(A[k, j]) + \\\n",
    "                       math.log(B[j, vocab[test_corpus[i]]])\n",
    "\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i:\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j, i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j, i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eb60d2c58137e563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:15.950229Z",
     "start_time": "2024-05-27T15:12:42.732695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f330faadf04813be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:15.955498Z",
     "start_time": "2024-05-27T15:13:15.951749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45929f4943fed78",
   "metadata": {},
   "source": [
    "#### Exercise 7 - viterbi_backward\n",
    "Implement the `viterbi_backward` algorithm, which returns a list of predicted POS tags for each word in the corpus.\n",
    "\n",
    "- Note that the numbering of the index positions starts at 0 and not 1. \n",
    "- `m` is the number of words in the corpus.  \n",
    "    - So the indexing into the corpus goes from `0` to `m - 1`.\n",
    "    - Also, the columns in `best_probs` and `best_paths` are indexed from `0` to `m - 1`\n",
    "\n",
    "\n",
    "**In Step 1:**       \n",
    "Loop through all the rows (POS tags) in the last entry of `best_probs` and find the row (POS tag) with the maximum value.\n",
    "Convert the unique integer ID to a tag (a string representation) using the list `states`.  \n",
    "\n",
    "Referring to the three-word corpus described above:\n",
    "- `z[2] = 28`: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28.  Store 28 in `z` at position 2.\n",
    "- `states[28]` is 'RB': The POS tag ID 28 refers to the POS tag 'RB'.\n",
    "- `pred[2] = 'RB'`: In array `pred`, store the POS tag for the word 'upward'.\n",
    "\n",
    "**In Step 2:**  \n",
    "- Starting at the last column of best_paths, use `best_probs` to find the most likely POS tag for the last word in the corpus.\n",
    "- Then use `best_paths` to find the most likely POS tag for the previous word. \n",
    "- Update the POS tag for each word in `z` and in `preds`.\n",
    "\n",
    "Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.  \n",
    "`z[1] = best_paths[z[2],2]`  \n",
    "\n",
    "The small test following the routine prints the last few words of the corpus and their states to aid in debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71a472fc4bb5e0cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:26.884385Z",
     "start_time": "2024-05-27T15:13:26.881081Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    ## Step 1 ##\n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) with highest probability for the last word\n",
    "    for k in range(num_tags):\n",
    "        # If the probability of POS tag at row k is better than the previously best probability for the last word:\n",
    "        if best_probs[k, m-1] > best_prob_for_last_word:\n",
    "            # Store the new best probability for the last word\n",
    "            best_prob_for_last_word = best_probs[k, m-1]\n",
    "            # Store the unique integer ID of the POS tag which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag from its unique integer ID into the string representation\n",
    "    # using the 'states' list and store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(m - 1, 0, -1):\n",
    "        # Retrieve the unique integer ID of the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        # In best_paths, go to the row representing the POS tag of word i and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' list, where the key is the unique integer ID of the POS tag, and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "11bff3458f7d9e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:32.360102Z",
     "start_time": "2024-05-27T15:13:32.350230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e8ceb7471650bd29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:38.926733Z",
     "start_time": "2024-05-27T15:13:38.924673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a81883f25bc8aaa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:13:57.061330Z",
     "start_time": "2024-05-27T15:13:57.058532Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple) != 2:\n",
    "            continue\n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag:\n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea8c2f10c03ad71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T15:14:03.194655Z",
     "start_time": "2024-05-27T15:14:03.186091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211197e9e11464e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e6420cd2408e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7736c97a59b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823f90bfe7c4c872",
   "metadata": {},
   "source": "## Autocomplete and N-gram Language Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "N-gramovy jazykovy model je statisticky model, ktery predikuje nasledujici slovo v zavislosti na predchozich N \n",
    "slovech. Pouziva se v mnoha aplikacich, jako je napriklad automaticke dokoncovani textu, automaticke opravovani\n",
    "textu, automaticke generovani textu, a dalsi. N-gramovy model pocita pravdepodobnost vyskytu slova na zaklade\n",
    "predchozich N slov. Tj. pravdepodobnost slova $w_i$ na zaklade predchozich N slov $w_{i-N}, w_{i-N+1}, ..., w_{i-1}$\n",
    "je dana vzorcem:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1})$$\n",
    "N-gramovy model se vytvari pomoci trenovaciho korpusu textu. Trenovaci korpus je mnozina textovych dokumentu, ktere\n",
    "jsou pouzity k trenovani modelu. N-gramovy model se vytvari pomoci maximalni verohodnosti (maximum likelihood). Tj.\n",
    "naleznete takove parametry modelu, ktere maximalizuji pravdepodobnost vyskytu trenovacich dat. N-gramovy model se\n",
    "vytvori pomoci vzorce:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) = \\frac{C(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)}{C(w_{i-N}, w_{i-N+1}, ..., w_{i-1})}$$\n",
    "kde $C(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)$ je pocet vyskytu N-gramu $w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i$ v\n",
    "trenovacim korpusu a $C(w_{i-N}, w_{i-N+1}, ..., w_{i-1})$ je pocet vyskytu N-1 gramu $w_{i-N}, w_{i-N+1}, ..., w_{i-1}$\n",
    "v trenovacim korpusu.\n"
   ],
   "id": "98b88e648b6616b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "K vypocut N-gramoveho modelu pouzijeme Bayesuvu vetu:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) = \\frac{P(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)}{P(w_{i-N}, w_{i-N+1}, ..., w_{i-1})}$$\n",
    "Pouzijeme Bayesovo pravidlo: \n",
    "$$P(B|A) = \\frac{P(A,B)}{P(A)} \\rightarrow P(A,B) = P(B|A)P(A)$$\n",
    "$$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$"
   ],
   "id": "ed8fe6c795ae1b1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pak se veta \"the teacher drinks tea\" rozpadne na:\n",
    "$$P(the, teacher, drinks, tea) = P(the)P(teacher|the)P(drinks|the, teacher)P(tea|the, teacher, drinks)$$"
   ],
   "id": "7332b73e9565dfdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Protoze ovsem v korpusu se nemusi nachazat takhle dlouhy vety, tak se pouziva Markovuv predpoklad, ktery rika, ze\n",
    "pravdepodobnost vyskytu slova zavisi pouze na predchozich N slovech. Tj.:\n",
    "- Bigramovy model: $P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) \\approx P(w_i|w_{i-1})$ \n",
    "- Trigramovy model: $P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) \\approx P(w_i|w_{i-2}, w_{i-1})$\n",
    "\n",
    "A pak se cela veta modeluje jako:\n",
    "$$ P(w_i^n) \\approx \\prod_{i=1}^n P(w_i|w_{i-1}) \\quad ,$$\n",
    "kde $w_i^n$ je veta o delce n slov zacinajici slovem i.\n",
    "$$ P(w_i^n) \\approx P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})$$"
   ],
   "id": "a9181ab0787cb018"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vety zaciname a ukoncujeme nejakym specialnim znakem. Tokenem. Zpravidla se pouziva token \"\\<s\\>\" pro zacatek vety a \n",
    "token \"\\</s\\>\" pro konec vety. Tj. vety se modeluji jako:\n",
    "$$ P(w_i^n) \\approx P(w_1|<s>)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})P(</s>|w_n)$$"
   ],
   "id": "7b104a8ea66c5063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "V jinych modelech je to pak napriklad \"SOS\" jako start of sentence a \"EOS\" jako end of sentence.",
   "id": "eacc17120b7202af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LAB: Corpus Preprocessing",
   "id": "e7a029f3db8e83fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:00:09.775191Z",
     "start_time": "2024-05-27T17:00:04.691948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk               # NLP toolkit\n",
    "import re                 # Library for Regular expression operations\n",
    "\n",
    "nltk.download('punkt') "
   ],
   "id": "6a30ea56db8efa0a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmateju/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:01:17.067724Z",
     "start_time": "2024-05-27T17:01:17.063700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change the corpus to lowercase\n",
    "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# note that word \"learning\" will now be the same regardless of its position in the sentence\n",
    "print(corpus)"
   ],
   "id": "9a067b9e39ac87d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:01:28.761308Z",
     "start_time": "2024-05-27T17:01:28.758444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove special characters\n",
    "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
    "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
    "print(corpus)"
   ],
   "id": "1cd7c81904f44d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that this process gets rid of the happy face made with punctuations :). Remember that for sentiment analysis, this emoticon was very important. However, we will not consider it here.",
   "id": "11a73672d84edd53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:02:02.179967Z",
     "start_time": "2024-05-27T17:02:02.177313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split text by a delimiter to array\n",
    "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
    "\n",
    "# get the date parts in array\n",
    "date_parts = input_date.split(\" \")\n",
    "print(f\"date parts = {date_parts}\")\n",
    "\n",
    "#get the time parts in array\n",
    "time_parts = date_parts[4].split(\":\")\n",
    "print(f\"time parts = {time_parts}\")"
   ],
   "id": "ae92c81a0a71e63c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
      "time parts = ['07', '33', '35']\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once you have a list of sentences, the next step is to split each sentence into a list of words.\n",
    "\n",
    "This process could be done in several ways, even using the str.split method described above, but we will use the NLTK library [nltk](https://www.nltk.org/) to help us with that.\n",
    "\n",
    "In the code assignment, you will use the method [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize) to split your sentence into a list of words. Let us try the method in an example."
   ],
   "id": "fcd239200504ac78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:02:10.582731Z",
     "start_time": "2024-05-27T17:02:10.575971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenize the sentence into an array of words\n",
    "\n",
    "sentence = 'i am happy because i am learning.'\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(f'{sentence} -> {tokenized_sentence}')"
   ],
   "id": "68aed7095c919cfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:03:49.265696Z",
     "start_time": "2024-05-27T17:03:49.262469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# find length of each word in the tokenized sentence\n",
    "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "word_lengths = [(word, len(word)) for word in sentence] # Create a list with the word lengths using a list comprehension\n",
    "print(f' Lengths of the words: \\n{word_lengths}')"
   ],
   "id": "5d5ec7c7e6c001b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lengths of the words: \n",
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next step is to build n-grams from the tokenized sentences. \n",
    "\n",
    "A sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\n",
    "\n",
    "Here is an example method that prints all trigrams in the given sentence."
   ],
   "id": "de499cb897a62afa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:05:34.365422Z",
     "start_time": "2024-05-27T17:05:34.362396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentence_to_trigram(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    Prints all trigrams in the given tokenized sentence.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentence: The words list.\n",
    "    \n",
    "    Returns:\n",
    "        No output\n",
    "    \"\"\"\n",
    "    # note that the last position of i is 3rd to the end\n",
    "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tokenized_sentence[i : i + 3]\n",
    "        print(trigram)\n",
    "\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
    "sentence_to_trigram(tokenized_sentence)\n"
   ],
   "id": "39ac851fa300809d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "\n",
      "['i', 'am', 'happy']\n",
      "['am', 'happy', 'because']\n",
      "['happy', 'because', 'i']\n",
      "['because', 'i', 'am']\n",
      "['i', 'am', 'learning']\n",
      "['am', 'learning', '.']\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to \n",
    "calculate the probability of an n-gram.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n",
    "\\end{equation*}\n",
    "\n",
    "The following code shows how to get an (n-1)-gram  prefix from n-gram on an example of getting trigram from a 4-gram."
   ],
   "id": "d98cdfd4024db814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:07:21.385681Z",
     "start_time": "2024-05-27T17:07:21.383265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get trigram prefix from a 4-gram\n",
    "fourgram = ['i', 'am', 'happy','because']\n",
    "trigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\n",
    "print(trigram)"
   ],
   "id": "59b8a0b3a67e7480",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy']\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T17:07:32.638070Z",
     "start_time": "2024-05-27T17:07:32.635854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
    "n = 3\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "tokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"<e>\"]\n",
    "print(tokenized_sentence)"
   ],
   "id": "675a87ea0e7c25fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### N-gram Language Model",
   "id": "d0d475734a30c7dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro n-gramovy jazykovy model potrebujeme napocitat pravdepodobnosti prechodu. Tj. nejprve napoctame matice poctu \n",
    "n-gramu: pro kazdou vetu v korpusu napocitame vsechny n-gramy a z nich napocitame pravdepodobnosti prechodu."
   ],
   "id": "dfa1b1e4f95ec67a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image](./pomocne_soubory/bigram_matice_counts.png)",
   "id": "9edcb398864ba72e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro vypocet pravdepodobnosti prechodu pouzijeme vzorec:\n",
    "$$P(w_n|w_{n-N+1}^{n-1})=\\frac{C(w_{n-N+1}^n, w_n)}{C(w_{n-N+1}^{n-1})} \\quad ,$$\n",
    "kde $C(w_{n-N+1}^n, w_n)$ je pocet vyskytu n-gramu $w_{n-N+1}^n, w_n$ v korpusu a $C(w_{n-N+1}^{n-1})$ je pocet \n",
    "vyskytu n-1 gramu $w_{n-N+1}^{n-1}$ v korpusu.\n",
    "$$sum(row) = \\sum_{i=1}^{N} C(w_{n-N+1}^{n-1}, w_i) = C(w_{n-N+1}^{n-1})$$"
   ],
   "id": "9092994841d1575b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro bigramovy model pak pro vypocet pravdepodobnosti vety pouzijeme vzorec:\n",
    "$$P(w_1^n) = \\prod_{i=1}^n P(w_i|w_{i-1})$$\n",
    "Pro trigramovy model pak:\n",
    "$$P(w_1^n) = \\prod_{i=1}^n P(w_i|w_{i-2}, w_{i-1})$$"
   ],
   "id": "8a14340203a088b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pro lepsi numerickou stabilitu se casto pouziva logaritmus pravdepodobnosti:\n",
    "$$log(P(w_1^n)) = \\sum_{i=1}^n log(P(w_i|w_{i-1}))$$"
   ],
   "id": "122786a114099879"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Algoritmus modelu pak funguje nasledovne: \n",
    "1. zvoliime zacatek vety \\<s\\> a prvni slovo vety\n",
    "2. zvolime dalsi slovo na zaklade pravdepodobnosti prechodu\n",
    "3. pokracujeme dokud nedojdeme na konec vety \\</s\\>"
   ],
   "id": "422c64f14dd3bdd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LAB: N-gram Language Models",
   "id": "ab431ea192373a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:10:00.774122Z",
     "start_time": "2024-05-27T21:10:00.767550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# manipulate n_gram count dictionary\n",
    "\n",
    "n_gram_counts = {\n",
    "    ('i', 'am', 'happy'): 2,\n",
    "    ('am', 'happy', 'because'): 1}\n",
    "\n",
    "# get count for an n-gram tuple\n",
    "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
    "\n",
    "# check if n-gram is present in the dictionary\n",
    "if ('i', 'am', 'learning') in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
    "\n",
    "# update the count in the word count dictionary\n",
    "n_gram_counts[('i', 'am', 'learning')] = 1\n",
    "if ('i', 'am', 'learning') in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n"
   ],
   "id": "b9ce38174f027ccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of n-gram ('i', 'am', 'happy'): 2\n",
      "n-gram ('i', 'am', 'learning') missing\n",
      "n-gram ('i', 'am', 'learning') found\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:10:09.453917Z",
     "start_time": "2024-05-27T21:10:09.451734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# concatenate tuple for prefix and tuple with the last word to create the n_gram\n",
    "prefix = ('i', 'am', 'happy')\n",
    "word = 'because'\n",
    "\n",
    "# note here the syntax for creating a tuple for a single word\n",
    "n_gram = prefix + (word,)\n",
    "print(n_gram)"
   ],
   "id": "3777f8497f92ff93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'am', 'happy', 'because')\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:10:16.909752Z",
     "start_time": "2024-05-27T21:10:16.888598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "def single_pass_trigram_count_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: Pre-processed and tokenized corpus. \n",
    "    \n",
    "    Returns:\n",
    "        bigrams: list of all bigram prefixes, row index\n",
    "        vocabulary: list of all found words, the column index\n",
    "        count_matrix: pandas dataframe with bigram prefixes as rows, \n",
    "                      vocabulary words as columns \n",
    "                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    vocabulary = []\n",
    "    count_matrix_dict = defaultdict(dict)\n",
    "    \n",
    "    # go through the corpus once with a sliding window\n",
    "    for i in range(len(corpus) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tuple(corpus[i : i + 3])\n",
    "        \n",
    "        bigram = trigram[0 : -1]\n",
    "        if not bigram in bigrams:\n",
    "            bigrams.append(bigram)        \n",
    "        \n",
    "        last_word = trigram[-1]\n",
    "        if not last_word in vocabulary:\n",
    "            vocabulary.append(last_word)\n",
    "        \n",
    "        if (bigram,last_word) not in count_matrix_dict:\n",
    "            count_matrix_dict[bigram,last_word] = 0\n",
    "            \n",
    "        count_matrix_dict[bigram,last_word] += 1\n",
    "    \n",
    "    # convert the count_matrix to np.array to fill in the blanks\n",
    "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
    "    for trigram_key, trigam_count in count_matrix_dict.items():\n",
    "        count_matrix[bigrams.index(trigram_key[0]), \\\n",
    "                     vocabulary.index(trigram_key[1])]\\\n",
    "        = trigam_count\n",
    "    \n",
    "    # np.array to pandas dataframe conversion\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
    "    return bigrams, vocabulary, count_matrix\n",
    "\n",
    "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
    "\n",
    "print(count_matrix)\n"
   ],
   "id": "3dd8ad2864abd0af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:10:30.690216Z",
     "start_time": "2024-05-27T21:10:30.684088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the probability matrix from the count matrix\n",
    "row_sums = count_matrix.sum(axis=1)\n",
    "# divide each row by its sum\n",
    "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
    "\n",
    "print(prob_matrix)"
   ],
   "id": "e295d24f559395cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:10:58.560925Z",
     "start_time": "2024-05-27T21:10:58.558164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# find the probability of a trigram in the probability matrix\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the prefix bigram \n",
    "bigram = trigram[:-1]\n",
    "print(f'bigram: {bigram}')\n",
    "\n",
    "# find the last word of the trigram\n",
    "word = trigram[-1]\n",
    "print(f'word: {word}')\n",
    "\n",
    "# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\n",
    "trigram_probability = prob_matrix[word][bigram]\n",
    "print(f'trigram_probability: {trigram_probability}')"
   ],
   "id": "16c16ac5cbef29fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: ('i', 'am')\n",
      "word: happy\n",
      "trigram_probability: 0.5\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:11:11.367646Z",
     "start_time": "2024-05-27T21:11:11.365318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lists all words in vocabulary starting with a given prefix\n",
    "vocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\n",
    "starts_with = 'ha'\n",
    "\n",
    "print(f'words in vocabulary starting with prefix: {starts_with}\\n')\n",
    "for word in vocabulary:\n",
    "    if word.startswith(starts_with):\n",
    "        print(word)"
   ],
   "id": "8d644ffec1273fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in vocabulary starting with prefix: ha\n",
      "\n",
      "happy\n",
      "have\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:11:37.867941Z",
     "start_time": "2024-05-27T21:11:37.863783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we only need train and validation %, test is the remainder\n",
    "def train_validation_test_split(data, train_percent, validation_percent):\n",
    "    \"\"\"\n",
    "    Splits the input data to  train/validation/test according to the percentage provided\n",
    "    \n",
    "    Args:\n",
    "        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n",
    "        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n",
    "        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n",
    "        \n",
    "        Note: train_percent + validation_percent need to be <=100\n",
    "              the reminder to 100 is allocated for the test set\n",
    "    \n",
    "    Returns:\n",
    "        train_data: list of sentences, the training part of the corpus\n",
    "        validation_data: list of sentences, the validation part of the corpus\n",
    "        test_data: list of sentences, the test part of the corpus\n",
    "    \"\"\"\n",
    "    # fixed seed here for reproducibility\n",
    "    random.seed(87)\n",
    "    \n",
    "    # reshuffle all input sentences\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_percent / 100)\n",
    "    train_data = data[0:train_size]\n",
    "    \n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "    \n",
    "    test_data = data[train_size + validation_size:]\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "data = [x for x in range (0, 100)]\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\n",
    "print(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
    "      f\"test data:{test_data}\\n\")\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\n",
    "print(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
    "      f\"test data:{test_data}\\n\")"
   ],
   "id": "c2dfcc9520d6708e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 80/10/10:\n",
      " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
      " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
      " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
      "\n",
      "split 98/1/1:\n",
      " train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n",
      " validation data:[35]\n",
      " test data:[75]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Perplexity\n",
    "\n",
    "In order to implement the perplexity formula, you'll need to know how to implement m-th order root of a variable.\n",
    "\n",
    "\\begin{equation*}\n",
    "PP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Remember from calculus:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here is a code that will help you with the formula."
   ],
   "id": "3c2b8a0fb81c504"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:12:05.406979Z",
     "start_time": "2024-05-27T21:12:05.404311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to calculate the exponent, use the following syntax\n",
    "p = 10 ** (-250)\n",
    "M = 100\n",
    "perplexity = p ** (-1 / M)\n",
    "print(perplexity)"
   ],
   "id": "2d427e2a50fa177a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316.22776601683796\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Evaluation",
   "id": "4c15760d15c3df3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data je nutno rozdelit jako vzdy na train/val/test set. Na velikosti corpusu zalezi, jak velky bude train a val set.\n",
    "Na malem corpusu jsou pomery zhruba 80/10/10, na velkem 98/1/1. Dale je mozne vybirat bud casti delsiho textu do \n",
    "ruznych mnozin, nebo mame-li kratsi useky, tak ty davat do mnozin primo."
   ],
   "id": "a196d8b97eb7d0f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A jak vyhodnotit, zda je model dobre natrenovany? To se da udelat pomoci perplexity. Perplexity je metrika, ktera\n",
    "meri, jak dobre model predikuje data. Cim nizsi perplexity, tim lepsi model. Perplexity se pocita jako:\n",
    "$$PP(W) = P(s_1, s_2, ..., s_m)^{-\\frac{1}{m}}$$\n",
    "kde $s_1, s_2, ..., s_N$ je veta o delce N slov. Tj. perplexity je inverzni hodnota geometrickeho prumeru\n",
    "pravdepodobnosti vyskytu slov ve vete. \n",
    "$$ PP(W) = \\sqrt[m]{\\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)}|w_{j-1}^Ã­(i))}}$$"
   ],
   "id": "76193c347cd61e92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Kdyz spojime vsechny vety v test mnozine do jedne dlouhe vety, pak perplexity je \n",
    "$$PP(W) = \\sqrt[m]{\\prod_{i=1}^{m} \\frac{1}{P(w_i|w_{i-1})}}$$\n",
    "A pro lepsi numerickou stabilitu opet pouzijeme logaritmus:\n",
    "$$log(PP(W)) = \\frac{1}{m} \\sum_{i=1}^{m} log(\\frac{1}{P(w_i|w_{i-1})})$$"
   ],
   "id": "f3d170fd43c7ae4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Out of Vocabulary Words\n",
    "\n",
    "Ulohy muzeme rozdelit na ty s \"uzavrenym slovnikem\" a na ty s \"otevrenym slovnikem\". Uzavreny slovnik znamena, ze \n",
    "nemuzeme potkat slova, ktera nejsou ve slovniku. A otevreny slovnik logicky naopak. Uzavreny slovnik je napriklad\n",
    "automaticke opravovani textu, kde se kontroluje, zda slovo je ve slovniku. Otevreny slovnik je napriklad automaticke\n",
    "dokoncovani textu, kde se predikuje slovo, ktere neni ve slovniku.\n",
    "\n",
    "Proto v pripade, kdy pracujeme s otevrenym slovnikem postupujeme nasledovne: \n",
    "- vytvorime slovnik V, ktery obsahuje vsechna slova v trenovacim korpusu\n",
    "- nahradime vsechna slova, ktera jsou v korpusu, ale nejsou ve slovniku V, specialnim tokenem \\<UNK\\>\n",
    "- vypocitame pravdepodobnosti prechodu jako obvykle s novym tokenem \\<UNK\\> "
   ],
   "id": "db105c15ec0a276f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aby slovnik nebyl tak velky, tak se slova s nizkou frekvenci nahrazuji specialnim tokenem \\<UNK\\>. Napriklad, s \n",
    "$\\text{min_freqency} = 2$ se nahrazi slova, ktera se vyskytuji mene nez dvakrat. Bezna kriteria pro vyber slovniku jsou\n",
    "- min_freqency: minimalni frekvence slova\n",
    "- max_vocab_size: maximalni velikost slovniku\n",
    "- unknown_token: token pro nezname slovo by melo byt pouzivano vyjimecne. Jinak by to vedlo k tomu, ze ten token bude\n",
    " pouzivan casto \"jako zolik\" a model bude mit nizkou perplexitu, i kdyz vlastne bude prd predikovat\n",
    "- perplexity metriku je treba pouzivat jen na LM modelech se stejnym slovnikem"
   ],
   "id": "92a598a243261f85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Smoothing\n",
    "V pripade, ze pro nekterou kombinaci slov neexistuje bigram ve slovniku, tak by byla pravdepodobnost nulova. Tri \n",
    "bezne metody pro reseni tohoto problemu jsou\n",
    "- smoothing\n",
    "- backoff\n",
    "- interpolation\n",
    "\n",
    "#### Smoothing\n",
    "Smoothing je metoda, ktera prida k vsem vyskytum slov v korpusu nejakou konstantu. Napriklad, Laplace smoothing prida k\n",
    "vsem vyskytum slov konstantu 1. Tj. vzorec pro pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |V|}$$\n",
    "pro obecne $k$ vzorec vypada:\n",
    "$$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + k|V|}$$"
   ],
   "id": "291b30ea032ade8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Backoff\n",
    "Backoff je metoda, ktera se snazi najit nejlepsi model pro predikci. Pokud neexistuje n-gram, tak se pouzije (n-1)\n",
    "-gram.  Pokud neexistuje ani (n-1)-gram, tak se pouzije (n-2)-gram, atd. Nicmene, zkracovani n-gramu vede k diztorzi \n",
    "rozdeleni pravdepodobnosti (zvlaste u mensich korpusu) a proto je treba nizsi rady n-gramu diskontovat.  Tj. vzorec \n",
    "pro pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}^{n-1}) = \\begin{cases} P(w_i|w_{i-1}^{n-1}) & \\text{if } C(w_{i-1}^n) > 0 \\\\ \\alpha(w_{i-1}^{n-1})P(w_i|w_{i-1}^{n-2}) & \\text{if } C(w_{i-1}^n) = 0 \\end{cases}$$\n",
    "kde $\\alpha(w_{i-1}^{n-1})$ je normalizacni konstanta, ktera zajisti, ze pravdepodobnosti budou soucet jedna. Viz \n",
    "Katzova metoda: https://en.wikipedia.org/wiki/Katz's_back-off_model\n",
    "\n",
    "\"Stupid backoff\" je jednodussi verze backoff, ktera pouziva konstantu $\\alpha = 0.4$. Tj. vzorec pro pravdepodobnost\n",
    "prechodu je:\n",
    "$$P(w_i|w_{i-1}) = \\begin{cases} P(w_i|w_{i-1}) & \\text{if } C(w_{i-1}, w_i) > 0 \\\\ 0.4P(w_i) & \\text{if } C(w_{i-1}, w_i) = 0 \\end{cases}$$"
   ],
   "id": "c7822abf3f40b4bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpolation\n",
    "Interpolace je metoda, ktera kombinuje pravdepodobnosti vsech n-gramu nizsich radu az po n-ty rad. Tj. vzorec pro \n",
    "pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}^{n-1}) = \\lambda_1P(w_i|w_{i-1}^{n-1}) + \\lambda_2P(w_i|w_{i-1}^{n-2}) + ... + \\lambda_nP(w_i)$$\n",
    "kde $\\lambda_i$ je vahovaci faktor, ktery zajisti, ze pravdepodobnosti budou soucet jedna. \n",
    "$$ \\sum_{i=1}^{n} \\lambda_i = 1$$"
   ],
   "id": "e0b979966b5c8b5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "V pripade, ze chybi nejaky n-gram, tak se pouzije nizsi rada n-gramu. Nestane se tak, ze by pravdepodobnost byla \n",
    "nulova, prestoze nejvyssi rad neexistuje, ale nizsi rady n-gramu prispivaji k odhadu pravdepodobnosti."
   ],
   "id": "58bbfc1defd2ed31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LAB: Smoothing",
   "id": "f5cb4389acc6566f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:11:58.563418Z",
     "start_time": "2024-05-27T22:11:58.558032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the vocabulary from M most frequent words\n",
    "# use Counter object from the collections library to find M most common words\n",
    "from collections import Counter\n",
    "\n",
    "# the target size of the vocabulary\n",
    "M = 3\n",
    "\n",
    "# pre-calculated word counts\n",
    "# Counter could be used to build this dictionary from the source corpus\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
    "\n",
    "vocabulary = Counter(word_counts).most_common(M)\n",
    "\n",
    "# remove the frequencies and leave just the words\n",
    "vocabulary = [w[0] for w in vocabulary]\n",
    "\n",
    "print(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n",
    "    "
   ],
   "id": "6e784081eeba6c4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:12:04.367505Z",
     "start_time": "2024-05-27T22:12:04.364841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\n",
    "sentence = ['am', 'i', 'learning']\n",
    "output_sentence = []\n",
    "print(f\"input sentence: {sentence}\")\n",
    "\n",
    "for w in sentence:\n",
    "    # test if word w is in vocabulary\n",
    "    if w in vocabulary:\n",
    "        output_sentence.append(w)\n",
    "    else:\n",
    "        output_sentence.append('<UNK>')\n",
    "        \n",
    "print(f\"output sentence: {output_sentence}\")"
   ],
   "id": "92df365a9588b240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: ['am', 'i', 'learning']\n",
      "output sentence: ['<UNK>', '<UNK>', 'learning']\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:12:14.311822Z",
     "start_time": "2024-05-27T22:12:14.309470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# iterate through all word counts and print words with given frequency f\n",
    "f = 3\n",
    "\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
    "\n",
    "for word, freq in word_counts.items():\n",
    "    if freq == f:\n",
    "        print(word)"
   ],
   "id": "2a5931fc8d8e91fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because\n",
      "learning\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:12:26.741016Z",
     "start_time": "2024-05-27T22:12:26.736763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# many <unk> low perplexity \n",
    "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
    "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
    "\n",
    "test_set = ['i', 'am', 'learning']\n",
    "test_set_unk = ['i', 'am', '<UNK>']\n",
    "\n",
    "M = len(test_set)\n",
    "probability = 1\n",
    "probability_unk = 1\n",
    "\n",
    "# pre-calculated probabilities\n",
    "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
    "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
    "\n",
    "# got through the test set and calculate its bigram probability\n",
    "for i in range(len(test_set) - 2 + 1):\n",
    "    bigram = tuple(test_set[i: i + 2])\n",
    "    probability = probability * bigram_probabilities[bigram]\n",
    "        \n",
    "    bigram_unk = tuple(test_set_unk[i: i + 2])\n",
    "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
    "\n",
    "# calculate perplexity for both original test set and test set with <UNK>\n",
    "perplexity = probability ** (-1 / M)\n",
    "perplexity_unk = probability_unk ** (-1 / M)\n",
    "\n",
    "print(f\"perplexity for the training set: {perplexity}\")\n",
    "print(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")\n"
   ],
   "id": "841ea64a5223ed49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity for the training set: 1.2599210498948732\n",
      "perplexity for the training set with <UNK>: 1.0\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:13:36.132894Z",
     "start_time": "2024-05-27T22:13:36.130024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    numerator = n_gram_count + k\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    return numerator / denominator\n",
    "\n",
    "trigram_probabilities = {('i', 'am', 'happy') : 2}\n",
    "bigram_probabilities = {( 'i', 'am') : 10}\n",
    "vocabulary_size = 5\n",
    "k = 1\n",
    "\n",
    "probability_known_trigram = add_k_smoothing_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n",
    "                           bigram_probabilities[( 'i', 'am')])\n",
    "\n",
    "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
    "\n",
    "print(f\"probability_known_trigram: {probability_known_trigram}\")\n",
    "print(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n"
   ],
   "id": "374280e4cb10b42f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability_known_trigram: 0.2\n",
      "probability_unknown_trigram: 0.2\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:13:41.362624Z",
     "start_time": "2024-05-27T22:13:41.359413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('are', 'you', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# search for first non-zero probability starting with trigram\n",
    "# to generalize this for any order of n-gram hierarchy, \n",
    "# you could loop through the probability dictionaries instead of if/else cascade\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
    "    print(f\"probability for trigram {trigram} not found\")\n",
    "    \n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
    "        print(f\"probability for bigram {bigram} not found\")\n",
    "        \n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"probability for unigram {unigram} found\\n\")\n",
    "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n"
   ],
   "id": "e83a1a37c6612935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') not found\n",
      "probability for bigram ('you', 'happy') not found\n",
      "probability for unigram happy found\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T22:13:47.208716Z",
     "start_time": "2024-05-27T22:13:47.205311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# the weights come from optimization on a validation set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n",
    "+ lambda_2 * bigram_probabilities[bigram]\n",
    "+ lambda_3 * unigram_probabilities[unigram]\n",
    "\n",
    "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n"
   ],
   "id": "a3c318c9e7fbd244",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
      "\n",
      "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LAB: Assignment - N-gram model",
   "id": "51a4321e47b78d4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a20f0dd83023282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9ed9fea9b9fd41c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e956af3db857f1fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "679509eeb965047b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "592aceb7fa4eefed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf63b28e837cfade"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e9cb9ab3509ae18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1de1ba75c8fc5caf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e2d70d19c22d841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fca5251cb1a1cf9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b54cdbedb95ba487"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bda31bcd5df9ffd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83bdeea28fdddb9d"
  },
  {
   "cell_type": "markdown",
   "id": "e974d95c2062fc65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "## Word embeddings with neural networks"
  },
  {
   "cell_type": "markdown",
   "id": "41bcc47f92327582",
   "metadata": {},
   "source": [
    "Slova je mozno kodovat nekolika ruznymi zpusoby. Asi nejzakladnejsi, je je representovat jako cisla. Tj. mame-li slovnik\n",
    "s N slovy, pak kazde slovo je reprezentovano cislem od 1 do N. Tento zpusob je ne uplne stastny, protoze neni\n",
    "zrejme, jaky vztah maji jednotlive cisla k sobe. Proc by napriklad zebra mela byt na konci a kun uprostred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e071b283c21ef61",
   "metadata": {},
   "source": [
    "Dalsi zpusob je reprezentovat slova jako vektory. Takovym zpusobem je one-hot encoding, coz je pristup jako ke \n",
    "kategorickemu kodovani. Mame-li slovnik s N slovy, pak kazde slovo je reprezentovano vektorem o delce N, kde je jedna\n",
    "jednicka a vsechny ostatni hodnoty jsou nulove. Tento zpusob take neni uplne stastny, protoze vytvari velmi velke\n",
    "vektory. Navic je prace s takovymi vektory vypocetne narocna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96006106488c981",
   "metadata": {},
   "source": [
    "![One Hot Encoding](./pomocne_soubory/one_hot_enc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866674ac8f626a3",
   "metadata": {},
   "source": [
    "Dalsi zpusob je pouziti word embeddings, coz je opet vektorova\n",
    "reprezentace slov, ale o nizsi dimenzionalite. V podsate PCA pro slova, kde jednotlive dimenze nejsou maximalni \n",
    "variance, ale spise vztahy mezi slovy. Word embeddings se pouzivaji v mnoha NLP aplikacich, jako je napriklad sentiment analysis, machine \n",
    "translation, named entity recognition, part-of-speech tagging, automatic summarization, question answering, text \n",
    "classification, document clustering, a jine. Word embeddings se vytvari pomoci metod strojoveho uceni, jako je napriklad Word2Vec, GloVe, FastText, a jine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecd3d292ab4c34",
   "metadata": {},
   "source": [
    "![One Hot Encoding](./pomocne_soubory/word_embedding.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e72e97997d85e",
   "metadata": {},
   "source": [
    "K vytvoreni word embeddings, potrebujeme corpus textu, ktery pouzijeme k trenovani modelu a dale si musime zvolit \n",
    "nejakou embedovaci metodu. Word embedding vznika v podstate jako by-produkt pri reseni nejakeho ukolu strojoveho \n",
    "uceni. Tou ulohou je prave ta embedovaci medota. Napriklad, uloha muze byt predikce nasledujiciho slova v zavislosti na\n",
    "predchozich slovech. Nebo na slovech okolnich. Takovy ukol se nazyva \"self-supervised learning\". Znamena to, ze sice \n",
    "jako input mame neolablovany text, nicmene sama uloha je supervizovana a labely si vytvrari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543b8b5bc43d811",
   "metadata": {},
   "source": [
    "![One Hot Encoding](./pomocne_soubory/word_embedding_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8b9cf49af0374",
   "metadata": {},
   "source": [
    "Embeddingovych metod je nekolik:\n",
    "- Klasicke:\n",
    "    - Word2Vec\n",
    "    - Continous Bag Of Words (CBOW): the model learns to predict the center word given some context words.\n",
    "    - Continuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a\n",
    "     given input word.\n",
    "    - GloVe: factorizes the logarithm of the corpus's word co-occurrence matrix, similar to the count matrix youâ€™ve used \n",
    "    before.\n",
    "    - FastText: based on the skip-gram model and takes into account the structure of words by representing words as \n",
    "    an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n",
    "- Deep Learning:\n",
    "    - ELMo\n",
    "    - BERT\n",
    "    - GPT 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfe3c27b460cc7",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b93e476d5ced",
   "metadata": {},
   "source": [
    "### Continuous Bag Of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383e1eccdf4766b",
   "metadata": {},
   "source": [
    "Cilem CBOW modelu je predikovat slovo uprosted nejakeho okna v zavislosti na okolnich slovech. Tj. mame-li vetu \"I am \n",
    "happy because I am learning\", pak pro slovo \"happy\" by mohlo mit okolni slova \"I\", \"am\", \"because\", \"I\". CBOW \n",
    "model tak predikuje \"happy\" na zaklade techto slov. To, jak velke okno pouzijeme pro okolni kontextova slova, je \n",
    "hyperparametr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd7034d451e29e",
   "metadata": {},
   "source": [
    "![One Hot Encoding](./pomocne_soubory/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fad078e9ffaa9f",
   "metadata": {},
   "source": [
    "#### Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00959740b5825",
   "metadata": {},
   "source": [
    "- letter case : lower case\n",
    "- punctuation : remove / replace with space / replace with dot\n",
    "- numbers : remove / replace with special token <NUM>\n",
    "- special characters : remove / replace with space\n",
    "- special words : remove / replace with special token <URL>, <EMAIL>, <PHONE>, <CURRENCY>, <DATE>, <TIME>\n",
    "\n",
    "Pro nektery pripady je vhodne specialni slova ponechat, napriklad pro sentiment analysis, kde je dulezite, zda je v \n",
    "textu smilik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7b3da2f74603b",
   "metadata": {},
   "source": [
    "Pro cisteni textu a tokenizaci se pouzivaji knihovny jako je napriklad NLTK, Spacy, Gensim, a dalsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f2716711f0b027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T07:36:42.245744Z",
     "start_time": "2024-05-24T07:36:42.134725Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[,!?;-]+\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mcorpus\u001B[49m)\n\u001B[1;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mword_tokenize(data)\n\u001B[1;32m      3\u001B[0m data \u001B[38;5;241m=\u001B[39m [ ch\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m ch \u001B[38;5;129;01min\u001B[39;00m data \u001B[38;5;28;01mif\u001B[39;00m ch\u001B[38;5;241m.\u001B[39misalpha() \u001B[38;5;129;01mor\u001B[39;00m ch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;129;01mor\u001B[39;00m emoji\u001B[38;5;241m.\u001B[39mget_emoji_regexp()\u001B[38;5;241m.\u001B[39msearch(ch) ]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.'  or emoji.get_emoji_regexp().search(ch) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab976e990caa230a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Transforming Text to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e7250aff67e44",
   "metadata": {},
   "source": [
    "Jedna z moznosti je vzit vsechny slova, zakodovat je jako one-hot encoding a udelat jejich prumer. Tento prumer je\n",
    "potom reprezentaci contextovych slov. A to slouzi jako vstup do shallow neural network, ktera predikuje slovo \n",
    "uprosted. Velikost embeddingu je velkost vnitrni vrstvy teto site. A je to hyperparametr.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfbcb9f6753d60",
   "metadata": {},
   "source": [
    "![One Hot Encoding](./pomocne_soubory/cbow_model_architect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb51b1f10ec8a29",
   "metadata": {},
   "source": [
    "Duvod proc to delame je by-product tenhle neuronky, tj. matice vah W_1 a W_2. To je to, co je onen embedding. Matice \n",
    "W_1 dobre popisuje kontextova slova a vytvari embed_dim representaci kontextu, matice W_2 zase dobre popisuje slovo \n",
    "uprostred. Bud se bere matice W_1 nebo W_2 jako embedding, nebo se bere prumer obou. Kazde ma sve pro a proti. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2299c7f5a67423e",
   "metadata": {},
   "source": [
    "Jak nasledne validovat, zda embedding funguje? Jsou dve skupiny evaluace: vnitrni a vnejsi. \n",
    "- Vnitrni evaluace se diva na analogie, clustering, visualizace, a dalsi. \n",
    "- Vnejsi evaluace se diva na konkretni ukoly, jako je naprikld sentiment analysis. Vyhoda je, ze rovnou testuje \n",
    "vyslednou aplikaci. Nevyhoda je, ze to trva dlouho a hure se to trouble-shootuje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f1eb4a68443bf",
   "metadata": {},
   "source": [
    "### Continuous skip-gram / Skip-gram with negative sampling (SGNS) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bdd7bf8e8a996",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e08e6c9a985208",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca687d9740568d2a",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8500f265ccc67c4",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a4d69bbd34e67",
   "metadata": {},
   "source": [
    "### GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95674d88fc92ea5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7f3a6ca9420c017",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Natural Language Processing with Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845c98c1b8d25cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9b6f700e318b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0b81086711031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb26e78b6fb0996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c65634b3923fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca6c55facf84b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6012ad518c091a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Natural Language Processing with Attention Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab961a71eaf1fc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff21b6a77f026973",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9638b5a425f633cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
