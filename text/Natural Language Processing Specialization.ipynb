{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Natural Language Processing with Classification and Vector Spaces",
   "id": "e5412230414e7119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis with Logistic Regression",
   "id": "1d09a7243269df5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis with Naïve Bayes",
   "id": "e48de073088b7a1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector Space Models",
   "id": "4330dfc0cdd9dd93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Machine Translation and Document Search",
   "id": "2e505452a3535c89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Natural Language Processing with Probabilistic Models",
   "id": "9d5b9fa3dfffc32d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Autocorrect",
   "id": "6194824742d9d58a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part of Speech Tagging and Hidden Markov Models",
   "id": "14b0497c45c1c269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Autocomplete and Language Models",
   "id": "823f90bfe7c4c872"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Word embeddings with neural networks",
   "id": "e974d95c2062fc65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Slova je mozno kodovat nekolika ruznymi zpusoby. Asi nejzakladnejsi, je je representovat jako cisla. Tj. mame-li slovnik\n",
    "s N slovy, pak kazde slovo je reprezentovano cislem od 1 do N. Tento zpusob je ne uplne stastny, protoze neni\n",
    "zrejme, jaky vztah maji jednotlive cisla k sobe. Proc by napriklad zebra mela byt na konci a kun uprostred. "
   ],
   "id": "41bcc47f92327582"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dalsi zpusob je reprezentovat slova jako vektory. Takovym zpusobem je one-hot encoding, coz je pristup jako ke \n",
    "kategorickemu kodovani. Mame-li slovnik s N slovy, pak kazde slovo je reprezentovano vektorem o delce N, kde je jedna\n",
    "jednicka a vsechny ostatni hodnoty jsou nulove. Tento zpusob take neni uplne stastny, protoze vytvari velmi velke\n",
    "vektory. Navic je prace s takovymi vektory vypocetne narocna."
   ],
   "id": "2e071b283c21ef61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![One Hot Encoding](./pomocne_soubory/one_hot_enc.png)",
   "id": "e96006106488c981"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dalsi zpusob je pouziti word embeddings, coz je opet vektorova\n",
    "reprezentace slov, ale o nizsi dimenzionalite. V podsate PCA pro slova, kde jednotlive dimenze nejsou maximalni \n",
    "variance, ale spise vztahy mezi slovy. Word embeddings se pouzivaji v mnoha NLP aplikacich, jako je napriklad sentiment analysis, machine \n",
    "translation, named entity recognition, part-of-speech tagging, automatic summarization, question answering, text \n",
    "classification, document clustering, a jine. Word embeddings se vytvari pomoci metod strojoveho uceni, jako je napriklad Word2Vec, GloVe, FastText, a jine."
   ],
   "id": "7866674ac8f626a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![One Hot Encoding](./pomocne_soubory/word_embedding.png) ",
   "id": "b9ecd3d292ab4c34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "K vytvoreni word embeddings, potrebujeme corpus textu, ktery pouzijeme k trenovani modelu a dale si musime zvolit \n",
    "nejakou embedovaci metodu. Word embedding vznika v podstate jako by-produkt pri reseni nejakeho ukolu strojoveho \n",
    "uceni. Tou ulohou je prave ta embedovaci medota. Napriklad, uloha muze byt predikce nasledujiciho slova v zavislosti na\n",
    "predchozich slovech. Nebo na slovech okolnich. Takovy ukol se nazyva \"self-supervised learning\". Znamena to, ze sice \n",
    "jako input mame neolablovany text, nicmene sama uloha je supervizovana a labely si vytvrari."
   ],
   "id": "2b3e72e97997d85e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![One Hot Encoding](./pomocne_soubory/word_embedding_2.png)",
   "id": "6543b8b5bc43d811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Embeddingovych metod je nekolik:\n",
    "- Klasicke:\n",
    "    - Word2Vec\n",
    "    - Continous Bag Of Words (CBOW): the model learns to predict the center word given some context words.\n",
    "    - Continuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a\n",
    "     given input word.\n",
    "    - GloVe: factorizes the logarithm of the corpus's word co-occurrence matrix, similar to the count matrix you’ve used \n",
    "    before.\n",
    "    - FastText: based on the skip-gram model and takes into account the structure of words by representing words as \n",
    "    an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n",
    "- Deep Learning:\n",
    "    - ELMo\n",
    "    - BERT\n",
    "    - GPT 2 "
   ],
   "id": "31a8b9cf49af0374"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Word2Vec",
   "id": "15dfe3c27b460cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Continuous Bag Of Words (CBOW) model",
   "id": "9288b93e476d5ced"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Cilem CBOW modelu je predikovat slovo uprosted nejakeho okna v zavislosti na okolnich slovech. Tj. mame-li vetu \"I am \n",
    "happy because I am learning\", pak pro slovo \"happy\" by mohlo mit okolni slova \"I\", \"am\", \"because\", \"I\". CBOW \n",
    "model tak predikuje \"happy\" na zaklade techto slov. To, jak velke okno pouzijeme pro okolni kontextova slova, je \n",
    "hyperparametr."
   ],
   "id": "2383e1eccdf4766b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![One Hot Encoding](./pomocne_soubory/cbow.png)",
   "id": "12dd7034d451e29e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Cleaning and Tokenization",
   "id": "97fad078e9ffaa9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- letter case : lower case\n",
    "- punctuation : remove / replace with space / replace with dot\n",
    "- numbers : remove / replace with special token <NUM>\n",
    "- special characters : remove / replace with space\n",
    "- special words : remove / replace with special token <URL>, <EMAIL>, <PHONE>, <CURRENCY>, <DATE>, <TIME>\n",
    "\n",
    "Pro nektery pripady je vhodne specialni slova ponechat, napriklad pro sentiment analysis, kde je dulezite, zda je v \n",
    "textu smilik."
   ],
   "id": "5b00959740b5825"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pro cisteni textu a tokenizaci se pouzivaji knihovny jako je napriklad NLTK, Spacy, Gensim, a dalsi.",
   "id": "cfc7b3da2f74603b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.'  or emoji.get_emoji_regexp().search(ch) ]"
   ],
   "id": "13f2716711f0b027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transforming Text to Numbers",
   "id": "ab976e990caa230a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Jedna z moznosti je vzit vsechny slova, zakodovat je jako one-hot encoding a udelat jejich prumer. Tento prumer je\n",
    "potom reprezentaci contextovych slov. A to slouzi jako vstup do shallow neural network, ktera predikuje slovo \n",
    "uprosted. Velikost embeddingu je velkost vnitrni vrstvy teto site. A je to hyperparametr.  "
   ],
   "id": "bd6e7250aff67e44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![One Hot Encoding](./pomocne_soubory/cbow_model_architect.png)",
   "id": "fecfbcb9f6753d60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Duvod proc to delame je by-product tenhle neuronky, tj. matice vah W_1 a W_2. To je to, co je onen embedding. Matice \n",
    "W_1 dobre popisuje kontextova slova a vytvari embed_dim representaci kontextu, matice W_2 zase dobre popisuje slovo \n",
    "uprostred. Bud se bere matice W_1 nebo W_2 jako embedding, nebo se bere prumer obou. Kazde ma sve pro a proti. "
   ],
   "id": "bcb51b1f10ec8a29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Jak nasledne validovat, zda embedding funguje? Jsou dve skupiny evaluace: vnitrni a vnejsi. \n",
    "- Vnitrni evaluace se diva na analogie, clustering, visualizace, a dalsi. \n",
    "- Vnejsi evaluace se diva na konkretni ukoly, jako je naprikld sentiment analysis. Vyhoda je, ze rovnou testuje \n",
    "vyslednou aplikaci. Nevyhoda je, ze to trva dlouho a hure se to trouble-shootuje."
   ],
   "id": "f2299c7f5a67423e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Continuous skip-gram / Skip-gram with negative sampling (SGNS) model",
   "id": "bc8f1eb4a68443bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GloVe",
   "id": "a95bdd7bf8e8a996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FastText",
   "id": "e8e08e6c9a985208"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ELMo",
   "id": "ca687d9740568d2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BERT",
   "id": "b8500f265ccc67c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT 2",
   "id": "294a4d69bbd34e67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "95674d88fc92ea5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Natural Language Processing with Sequence Models",
   "id": "f7f3a6ca9420c017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c845c98c1b8d25cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9bc9b6f700e318b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10e0b81086711031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5cb26e78b6fb0996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9c65634b3923fa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53ca6c55facf84b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Natural Language Processing with Attention Models",
   "id": "ba6012ad518c091a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fab961a71eaf1fc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ff21b6a77f026973"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9638b5a425f633cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
