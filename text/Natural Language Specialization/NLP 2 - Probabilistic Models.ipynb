{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d892b1f4-9b8a-408b-8a96-8142b99f0040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T13:50:59.580050Z",
     "start_time": "2024-07-12T13:50:58.070711Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import re # regular expression library; for tokenization of words\n",
    "from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
    "from pomocne_soubory.utils_pos import get_word_tag, preprocess\n",
    "from pomocne_soubory.utils2 import get_dict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "\n",
    "# TODO: Dodelat dalsi klasifikatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b9fa3dfffc32d",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing with Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194824742d9d58a",
   "metadata": {},
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ae3d-9882-4c37-a625-8803edcd8947",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e7833-d86b-43b9-b3a3-1b88a44ec02b",
   "metadata": {},
   "source": [
    "1. Zkontroluj slovo ve slovniku. Pokud tam neni, je to asi typo.\n",
    "2. Najdi vsechny stringy vzdaleny \"n edits\". (Edit je operace insert (add a letter), delete (delete letter), switch (prohod dve pismena), replace (nahrad jedno pismeno)\n",
    "3. Profiltruj stringy, ktery jsou ve slovnku\n",
    "4. Napocitej pravdepodobnosti slov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8df5c1-a18d-4fc5-8408-2ca165351899",
   "metadata": {},
   "source": [
    "#### LAB: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ecbeefe-3185-4aa6-8dc9-28f318362ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow ORANGE BLUE BLUE PINK\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# the tiny corpus of text ! \n",
    "text = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # ðŸŒˆ\n",
    "print(text)\n",
    "print('string length : ',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e68bd03-a564-4b98-ba2d-df941f6ea2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow orange blue blue pink\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# Preprocessnig\n",
    "# convert all letters to lower case\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n",
    "print('string length : ',len(text_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6efced-82af-4e54-ab88-5646f73418f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\n",
      "count :  10\n"
     ]
    }
   ],
   "source": [
    "# some regex to tokenize the string to words and return them in a list\n",
    "words = re.findall(r'\\w+', text_lowercase)\n",
    "print(words)\n",
    "print('count : ',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea78c8bb-7a47-48d8-99c6-093f5d431150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue', 'yellow', 'orange', 'pink', 'red'}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 1\n",
    "# create vocab\n",
    "vocab = set(words)\n",
    "print(vocab)\n",
    "print('count : ',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d73246-52b1-4fa4-8607-9d9ba7a9782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 2\n",
    "# create vocab including word count\n",
    "counts_a = dict()\n",
    "for w in words:\n",
    "    counts_a[w] = counts_a.get(w,0)+1\n",
    "print(counts_a)\n",
    "print('count : ',len(counts_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9027e924-8b7b-482a-a3d1-27938f03559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "#Â Create Vocabulary\n",
    "# Option 3\n",
    "# create vocab including word count using collections.Counter\n",
    "counts_b = dict()\n",
    "counts_b = Counter(words)\n",
    "print(counts_b)\n",
    "print('count : ',len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff81c0d-e9a9-4830-9766-ea318938b703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barchart of sorted word counts\n",
    "d = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\n",
    "plt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n",
    "_ = plt.xticks(range(len(d)), list(d.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4794de4-1dcc-48e2-ad0c-26ba30197009",
   "metadata": {},
   "source": [
    "### Napocitej pravdepodobnosti slov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26125c45-7be6-42a8-b78e-d8a461cd25db",
   "metadata": {},
   "source": [
    "Model je zalozeny na predikci nejpravdepodobnejsiho slova, takze je treba napocitat pravdepodobnosti slov:\n",
    "$$\n",
    "    P(w) = \\frac{C(w)}{V}\n",
    "$$\n",
    "kde $ P(w) $ je pravdepodobnost slova, $V$ je velikost corpusu, $C(w)$ je pocet slov v nem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ba1b2-ec18-460a-aa72-978efdece329",
   "metadata": {},
   "source": [
    "O neco sofistikovanejsi je moznost pocitat dve slova po sobe jdouci a pouzit pak prvni slovo jak zjistit vyssi pravdepodbnost comba. Napriklad: jestli je \"there friends\" nebo \"their friends\" pravdepodobnejsi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61dadd-65e7-4c57-9d51-a6841690ebd5",
   "metadata": {},
   "source": [
    "#### LAB: String Edits Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceba8e9-3ca0-4f66-a9e0-b6e9904ad65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "word = 'dearz' # ðŸ¦Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76648087-1dc3-41ff-8d79-bce707801fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'dearz']\n",
      "['d', 'earz']\n",
      "['de', 'arz']\n",
      "['dea', 'rz']\n",
      "['dear', 'z']\n",
      "['dearz', '']\n"
     ]
    }
   ],
   "source": [
    "# splits with a loop\n",
    "splits_a = []\n",
    "for i in range(len(word)+1):\n",
    "    splits_a.append([word[:i],word[i:]])\n",
    "\n",
    "for i in splits_a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cf44d5-aca9-4260-863a-5d5ddbf75d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'dearz')\n",
      "('d', 'earz')\n",
      "('de', 'arz')\n",
      "('dea', 'rz')\n",
      "('dear', 'z')\n",
      "('dearz', '')\n"
     ]
    }
   ],
   "source": [
    "# same splits, done using a list comprehension\n",
    "splits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "for i in splits_b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e26461-395c-4245-826f-5462b14273f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "earz  <-- delete  d\n",
      "darz  <-- delete  e\n",
      "derz  <-- delete  a\n",
      "deaz  <-- delete  r\n",
      "dear  <-- delete  z\n"
     ]
    }
   ],
   "source": [
    "# Delete Edit\n",
    "# deletes with a loop\n",
    "splits = splits_a\n",
    "deletes = []\n",
    "\n",
    "print('word : ', word)\n",
    "for L,R in splits:\n",
    "    if R:\n",
    "        print(L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f286ea8-d08b-4a11-94ed-df17a877a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "first item from the splits list :  ['', 'dearz']\n",
      "L :  \n",
      "R :  dearz\n",
      "*** now implicit delete by excluding the leading letter ***\n",
      "L + R[1:] :  earz  <-- delete  d\n"
     ]
    }
   ],
   "source": [
    "# breaking it down\n",
    "print('word : ', word)\n",
    "one_split = splits[0]\n",
    "print('first item from the splits list : ', one_split)\n",
    "L = one_split[0]\n",
    "R = one_split[1]\n",
    "print('L : ', L)\n",
    "print('R : ', R)\n",
    "print('*** now implicit delete by excluding the leading letter ***')\n",
    "print('L + R[1:] : ',L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6387d3df-ba80-4169-968e-e0abaada3993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "*** which is the same as ***\n",
      "earz\n",
      "darz\n",
      "derz\n",
      "deaz\n",
      "dear\n"
     ]
    }
   ],
   "source": [
    "# deletes with a list comprehension\n",
    "splits = splits_a\n",
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "\n",
    "print(deletes)\n",
    "print('*** which is the same as ***')\n",
    "for i in deletes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27aa2b90-84f8-4af6-9f24-c2510821d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\n",
      "edits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "candidate words :  {'dear'}\n"
     ]
    }
   ],
   "source": [
    "vocab = ['dean','deer','dear','fries','and','coke']\n",
    "edits = list(deletes)\n",
    "\n",
    "print('vocab : ', vocab)\n",
    "print('edits : ', edits)\n",
    "\n",
    "candidates=[]\n",
    "\n",
    "### START CODE HERE ###\n",
    "candidates = set.intersection(set(vocab), set(edits))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print('candidate words : ', candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fccaa-2fde-4592-afba-aead4d8483e5",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b1fd1-866e-4f16-95e2-5e67486f9880",
   "metadata": {},
   "source": [
    "Je mira, ktera popisuje podobnost dvou stringu. Jedna se o aplikaci dynamickeho programovani. Do tabulky si zapisujeme zdrojove slovo a cilove slovo a ukladame do ni cenu ruznych uprav."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f667acc-46e7-4451-b61a-23fa88249209",
   "metadata": {},
   "source": [
    "![image](../pomocne_soubory/dyn_prog.png)\n",
    "\n",
    "Cena z # do # je 0.\n",
    "Z P do # je 1, protoze je potreba smazat P.\n",
    "Z # do P je 1, protoze je potreba pridat P.\n",
    "Z P do P je 0, protoze neni potreba nic delat.\n",
    "Z P do S je 2, protoze je potreba nahradit P za S (minimum cost z P do S)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827c9a6-94d5-4ab3-83d0-b7ba7060bc03",
   "metadata": {},
   "source": [
    "Obecny vzorec na vypocet ceny je: \n",
    "$$\n",
    "    D[i,j] = min \\begin{cases}\n",
    "        D[i-1,j] + del\\_cost \\\\\n",
    "        D[i,j-1] + ins\\_cost \\\\\n",
    "        D[i-1,j-1] + c\n",
    "    \\end{cases}       \n",
    "$$\n",
    "kde $D[i,j]$ je cena zmeny z i-teho znaku zdrojoveho slova na j-ty znak ciloveho slova, $del\\_cost$ je cena smazani, \n",
    "$ins\\_cost$ je cena pridani, a $c$: \n",
    "\n",
    "$$ \n",
    "    c = \\begin{cases}\n",
    "        0 & \\text{ if } source[i] = target[j] \\\\\n",
    "        rep\\_cost & \\text{ if } source[i] \\neq target[j]\n",
    "    \\end{cases}\n",
    "$$\n",
    "a $rep\\_cost$ je cena nahrazeni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d231ff06a4b51",
   "metadata": {},
   "source": [
    "- $D[i,j] = D[i-1,j] + del\\_cost$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i-1,j) a pricte se cena smazani.\n",
    "- $D[i,j] = D[i,j-1] + ins\\_cost$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i,j-1) a pricte se cena pridani.\n",
    "- $D[i,j] = D[i-1,j-1] + c$ oznacuje, ze po vypocet ceny na (i,j) v tabulce se pouzije cena z (i-1,j-1) a pricte se $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb47f8a32696377",
   "metadata": {},
   "source": [
    "![image](../pomocne_soubory/dyn_prog_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cffb7d752f9dc0",
   "metadata": {},
   "source": [
    "Z obrazku je patrne, ze kdyz mame string \"P\" a chceme prazdny string, cena za smazeni je 1. Kdyz mame string \"PL\" a \n",
    "chceme prazdny string, cena za smazeni je 2. Kdyz mame string \"PL\" a chceme string \"P\", cena za smazeni je 1 a za \n",
    "dalsi smazani je taky 1. Analogicky pro pridavani stringu. Pro vyplneni bunky (1,1) je treba si spocitat cenu z\n",
    "bunky (0,0) a pricist cenu za nahrazeni. Nebo si spocitat cenu z bunky (0,1), coz je string \"P\" a smazani \"P\" a k \n",
    "tomu pridat cenu za pridani \"S\". Nebo analogicky pro bunku (1,0). Vezmeme minumum z techto cisel, tj. 2. A takto\n",
    "postupujeme pro vsechny bunky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480e8698004a18",
   "metadata": {},
   "source": [
    "#### LAB: Assignment 1: Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fae2ff1fd1e59",
   "metadata": {},
   "source": [
    "##### Exercise 1 - process_data\n",
    "Implement the function `process_data` which \n",
    "\n",
    "1) Reads in a corpus (text file)\n",
    "\n",
    "2) Changes everything to lowercase\n",
    "\n",
    "3) Returns a list of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a145d8b4e422ab",
   "metadata": {},
   "source": [
    "##### Options and Hints\n",
    "- If you would like more of a real-life practice, don't open the 'Hints' below (yet) and try searching the web to derive your answer.\n",
    "- If you want a little help, click on the green \"General Hints\" section by clicking on it with your mouse.\n",
    "- If you get stuck or are not getting the expected results, click on the green 'Detailed Hints' section to get hints for each step that you'll take to complete this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06bc691f80d482",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>General Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "General Hints to get started\n",
    "<ul>\n",
    "    <li>Python <a href=\"https://docs.python.org/3/tutorial/inputoutput.html\">input and output<a></li>\n",
    "    <li>Python <a href=\"https://docs.python.org/3/library/re.html\" >'re' documentation </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba559ca5891b21",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Detailed Hints</b></font>\n",
    "</summary>\n",
    "<p>     \n",
    "Detailed hints if you're stuck\n",
    "<ul>\n",
    "    <li>Use 'with' syntax to read a file</li>\n",
    "    <li>Decide whether to use 'read()' or 'readline().  What's the difference?</li>\n",
    "    <li>You can use str.lower() to convert to lowercase.</li>\n",
    "    <li>Use re.findall(pattern, string)</li>\n",
    "    <li>Look for the \"Raw String Notation\" section in the Python 're' documentation to understand the difference between r'\\W', r'\\W' and '\\\\W'. </li>\n",
    "    <li>For the pattern, decide between using '\\s', '\\w', '\\s+' or '\\w+'.  What do you think are the differences?</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f544e1a0fd58b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        text = file.read()\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Split text into words based on whitespace\n",
    "        words = re.findall(r'\\w+', text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d637233433437daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
      "There are 6116 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data('./pomocne_soubory/shakespeare.txt')\n",
    "vocab = set(word_l)  # this will be your new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587930b575af45",
   "metadata": {},
   "source": [
    "##### Exercise 2 - get_count\n",
    "\n",
    "Implement a `get_count` function that returns a dictionary\n",
    "- The dictionary's keys are words\n",
    "- The value for each word is the number of times that word appears in the corpus. \n",
    "\n",
    "For example, given the following sentence: **\"I am happy because I am learning\"**, your dictionary should return the following: \n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b>Key </b>  </td>\n",
    "    <td> <b>Value </b> </td> \n",
    "\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> I  </td>\n",
    "    <td> 2</td> \n",
    " \n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td>am</td>\n",
    "    <td>2</td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>happy</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Instructions**: \n",
    "Implement a `get_count` which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba591c2191e9b7",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Try implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions</li>\n",
    "    <li>You can also use defaultdict instead of a regular dictionary, along with the for loop</li>\n",
    "    <li>Otherwise, to skip using a `for` loop, you can use Python's <a href=\"https://docs.python.org/3.7/library/collections.html#collections.Counter\" > Counter class</a> </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d8f0359fba8a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # This will hold the word counts\n",
    "\n",
    "    for word in word_l:\n",
    "        if word in word_count_dict:\n",
    "            word_count_dict[word] += 1  # Increment the count of the word\n",
    "        else:\n",
    "            word_count_dict[word] = 1   # Add the word with a count of 1 if it's not already in the dictionary\n",
    "\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdee0fb0ad45d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6116 key values pairs\n",
      "The count for the word 'thee' is 240\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef087e2c898ab5b9",
   "metadata": {},
   "source": [
    "##### Exercise 3 - get_probs\n",
    "Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2}$$\n",
    "where \n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus.\n",
    "\n",
    "For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'** is:\n",
    "\n",
    "$$P(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.$$\n",
    "\n",
    "**Instructions:** Implement `get_probs` function which gives you the probability \n",
    "that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b926e16a04ada",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "General advice\n",
    "<ul>\n",
    "    <li> Use dictionary.values() </li>\n",
    "    <li> Use sum() </li>\n",
    "    <li> The cardinality (number of words in the corpus should be equal to len(word_l).  You will calculate this same number, but using the word count dictionary.</li>\n",
    "</ul>\n",
    "    \n",
    "If you're using a for loop:\n",
    "<ul>\n",
    "    <li> Use dictionary.keys() </li>\n",
    "</ul>\n",
    "    \n",
    "If you're using a dictionary comprehension:\n",
    "<ul>\n",
    "    <li>Use dictionary.items() </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0b71a9756796cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    M = sum(word_count_dict.values())\n",
    "\n",
    "    # Calculate the probability for each word\n",
    "    for word, count in word_count_dict.items():\n",
    "        probs[word] = count / M\n",
    "    ### END CODE HERE ###\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cfed09373edb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 6116\n",
      "P('thee') is 0.0045\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b056d522c7a294",
   "metadata": {},
   "source": [
    "##### 2 - String Manipulations\n",
    "\n",
    "Now that you have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions: \n",
    "\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d61b52a05d092",
   "metadata": {},
   "source": [
    "##### Exercise 4 - delete_letter\n",
    "\n",
    "**Instructions for delete_letter():** Implement a `delete_letter()` function that, given a word, returns a list of strings with one character deleted. \n",
    "\n",
    "For example, given the word **nice**, it would return the set: {'ice', 'nce', 'nic', 'nie'}. \n",
    "\n",
    "**Step 1:** Create a list of 'splits'. This is all the ways you can split a word into Left and Right: For example,   \n",
    "'nice is split into : `[('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')]`\n",
    "This is common to all four functions (delete, replace, switch, insert).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29779ffd00aecb08",
   "metadata": {},
   "source": [
    "**Step 2:** This is specific to `delete_letter`. Here, we are generating all words that result from deleting one character.  \n",
    "This can be done in a single line with a list comprehension. You can make use of this type of syntax:  \n",
    "`[f(a,b) for a, b in splits if condition]`  \n",
    "\n",
    "For our 'nice' example you get: \n",
    "['ice', 'nce', 'nie', 'nic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd3f80b46e96fc",
   "metadata": {},
   "source": [
    "##### Levels of assistance\n",
    "\n",
    "Try this exercise with these levels of assistance.  \n",
    "- We hope that this will make it both a meaningful experience but also not a frustrating experience. \n",
    "- Start with level 1, then move onto level 2, and 3 as needed.\n",
    "\n",
    "    - Level 1. Try to think this through and implement this yourself.\n",
    "    - Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n",
    "    - Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n",
    "    \n",
    "- If you are still stuck, look at the images in the \"list comprehensions\" section above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ea04029933ecf",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Level 3 Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>\n",
    "    <li>Do this in a loop or list comprehension, so that you have a list of tuples.\n",
    "    <li> For example, \"cake\" can get split into \"ca\" and \"ke\". They're stored in a tuple (\"ca\",\"ke\"), and the tuple is appended to a list.  We'll refer to these as L and R, so the tuple is (L,R)</li>\n",
    "    <li>When choosing the range for your loop, if you input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) </li>\n",
    "    <li>deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings</li>\n",
    "    <li>When combining the tuples, make sure that you leave out a middle character.</li>\n",
    "    <li>Use array slicing to leave out the first character of the right substring.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fdd0c9a5ebb52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i+1:]) for i in range(len(word))]\n",
    "    delete_l = [y[0] + y[1] for y in split_l]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "    return  delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bd65541d6a3cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word cans, \n",
      "split_l = [('', 'ans'), ('c', 'ns'), ('ca', 's'), ('can', '')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6653da22992a8b",
   "metadata": {},
   "source": [
    "##### Note 1\n",
    "- Notice how it has the extra tuple `('cans', '')`.\n",
    "- This will be fine as long as you have checked the size of the right-side substring in tuple (L,R).\n",
    "- Can you explain why this will give you the same result for the list of deletion strings (delete_l)?\n",
    "\n",
    "```CPP\n",
    "input word cans, \n",
    "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \n",
    "delete_l = ['ans', 'cns', 'cas', 'can']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecc3d0aff2b6c2",
   "metadata": {},
   "source": [
    "##### Note 2\n",
    "If you end up getting the same word as your input word, like this:\n",
    "\n",
    "```Python\n",
    "input word cans, \n",
    "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \n",
    "delete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n",
    "```\n",
    "\n",
    "- Check how you set the `range`.\n",
    "- See if you check the length of the string on the right-side of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa4dcc17ead9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs of delete_letter('at') is 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3bfebbaabe3d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5b80222404a07f",
   "metadata": {},
   "source": [
    "##### Exercise 5 - switch_letter\n",
    "\n",
    "**Instructions for switch_letter()**: Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters **that are adjacent to each other**. \n",
    "- For example, given the word 'eta', it returns {'eat', 'tea'}, but does not return 'ate'.\n",
    "\n",
    "**Step 1:** is the same as in delete_letter()  \n",
    "**Step 2:** A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:  \n",
    "`[f(L,R) for L, R in splits if condition]`  where 'condition' will test the length of R in a given iteration. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739f39b3074125e",
   "metadata": {},
   "source": [
    "##### Levels of difficulty\n",
    "\n",
    "Try this exercise with these levels of difficulty.  \n",
    "- Level 1. Try to think this through and implement this yourself.\n",
    "- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n",
    "- Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440297fab5d1e782",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b16e38f4c7549c",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Level 3 Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.</li>\n",
    "    <li>Splitting is the same as for delete_letter</li>\n",
    "    <li>To perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings</li>\n",
    "    <li>The four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.</li>\n",
    "    <li>Unlike delete_letter, you will want to check that your right substring is at least a minimum length.  To see why, review the previous hint bullet point (directly before this one).</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10b10e602be93cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(1, len(word))]\n",
    "    switch_l = [L[:-1] + R[0] + L[-1] + R[1:] for L,R in split_l]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "    \n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68c5e1cdf1bd3f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41ced86a5cb78a",
   "metadata": {},
   "source": [
    "##### Note 1\n",
    "\n",
    "You may get this:\n",
    "```Python\n",
    "Input word = eta \n",
    "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \n",
    "switch_l = ['tea', 'eat']\n",
    "```\n",
    "- Notice how it has the extra tuple `('eta', '')`.\n",
    "- This is also correct.\n",
    "- Can you think of why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705ff51a3ebecf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547960415c1a52f0",
   "metadata": {},
   "source": [
    "##### Exercise 6 - replace_letter\n",
    "**Instructions for replace_letter()**: Now implement a function that takes in a word and returns a list of strings with one **replaced letter** from the original word. \n",
    "\n",
    "**Step 1:** is the same as in `delete_letter()`\n",
    "\n",
    "**Step 2:** A list comprehension or for loop which form strings by replacing letters.  This can be of the form:  \n",
    "`[f(a,b,c) for a, b in splits if condition for c in string]`   Note the use of the second for loop.  \n",
    "It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of 'ear' with 'e' will return 'ear'.\n",
    "\n",
    "**Step 3:** Remove the original input letter from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004c4ab77b3e4b",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>To remove a word from a list, first store its contents inside a set()</li>\n",
    "    <li>Use set.discard('the_word') to remove a word in a set.  Using set.remove('the_word') throws a KeyError if the word does not exist in the set. </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b1731ab9812cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "\n",
    "    replace_set = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    # Step 1: Create splits\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # Step 2: Replace letters\n",
    "    for a, b in split_l:\n",
    "        if b:  # Ensure there is something to replace\n",
    "            for c in letters:\n",
    "                if b[0] != c:  # Condition to ensure it's actually a replacement\n",
    "                    replace_set.add(a + c + b[1:])\n",
    "\n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47e037caefa104bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0d72a18c678cc",
   "metadata": {},
   "source": [
    "##### Exercise 7 - insert_letter\n",
    "\n",
    "**Instructions for insert_letter()**: Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\n",
    "\n",
    "**Step 1:** is the same as in `delete_letter()`\n",
    "\n",
    "**Step 2:** This can be a list comprehension of the form:  \n",
    "`[f(a,b,c) for a, b in splits if condition for c in string]`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71baa0b1e7652a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Insert each letter of the alphabet into every split position\n",
    "    insert_l = [a + c + b for a, b in split_l for c in letters]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5e69d4ed6b1cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6f06dd284f8b",
   "metadata": {},
   "source": [
    "##### 3 - Combining the Edits\n",
    "\n",
    "Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be `edit_one_letter()` and `edit_two_letters()`.\n",
    "\n",
    "##### 3.1 - Edit One Letter\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "##### Exercise 8 - edit_one_letter\n",
    "\n",
    "**Instructions**: Implement the `edit_one_letter` function to get all the possible edits that are one edit away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The 'switch' function  is a less common edit function, so its use will be selected by an \"allow_switches\" input argument.\n",
    "\n",
    "Note that those functions return *lists* while this function should return a *python set*. Utilizing a set eliminates any duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a11d8534fe4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "\n",
    "    # Generate all possible words with one letter replaced\n",
    "    replace_set = set(replace_letter(word))\n",
    "    edit_one_set.update(replace_set)\n",
    "    \n",
    "    # Generate all possible words with one letter inserted\n",
    "    insert_set = set(insert_letter(word))\n",
    "    edit_one_set.update(insert_set)\n",
    "    \n",
    "    # Generate all possible words with one letter deleted\n",
    "    delete_set = set(delete_letter(word))\n",
    "    edit_one_set.update(delete_set)\n",
    "    \n",
    "    # Optionally generate all possible words with two adjacent letters switched\n",
    "    if allow_switches:\n",
    "        switch_set = set(switch_letter(word))\n",
    "        edit_one_set.update(switch_set)\n",
    "    \n",
    "    # return this as a set and not a list\n",
    "    return set(edit_one_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e3516422cbcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf05627a0be8af3",
   "metadata": {},
   "source": [
    "##### 3.2 - Edit Two Letters\n",
    "\n",
    "<a name='ex-9'></a>\n",
    "##### Exercise 9 - edit_two_letters\n",
    "\n",
    "Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again. \n",
    "\n",
    "**Instructions**: Implement the `edit_two_letters` function that returns a set of words that are two edits away. Note that creating additional edits based on the `edit_one_letter` function may 'restore' some one_edits to zero or one edits. That is allowed here. This is accounted for in get_corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75134bf19b649de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "\n",
    "    # Generate all possible words with one edit\n",
    "    edit_one_set = edit_one_letter(word, allow_switches)\n",
    "    \n",
    "    # For each word in the edit_one_set, generate all possible words with another edit\n",
    "    for word_one_edit in edit_one_set:\n",
    "        edit_two_set.update(edit_one_letter(word_one_edit, allow_switches))  \n",
    "    \n",
    "    # return this as a set instead of a list\n",
    "    return set(edit_two_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76e1837b2acd0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997649757cc9168",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84e2b2dcd78678c",
   "metadata": {},
   "source": [
    "##### 3.3 - Suggest Spelling Suggestions\n",
    "\n",
    "Now you will use your `edit_two_letters` function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type a.k.a your typing suggestion.\n",
    "\n",
    "##### Exercise 10 - get_corrections\n",
    "**Instructions**: Implement `get_corrections`, which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word). \n",
    "\n",
    "**Step 1:** Generate suggestions for a supplied word: You'll use the edit functions you have developed. The 'suggestion algorithm' should follow this logic: \n",
    "* If the word is in the vocabulary, suggest the word. \n",
    "* Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary, use those. \n",
    "* Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary, use those. \n",
    "* Otherwise, suggest the input word.*  \n",
    "* The idea is that words generated from fewer edits are more likely than words with more edits.\n",
    "\n",
    "\n",
    "Note: \n",
    "- Edits of two letters may 'restore' strings to either zero or one edit. This algorithm accounts for this by preferentially selecting lower distance edits first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb63386e557b84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef5b6e85bc481a0f",
   "metadata": {},
   "source": [
    "##### Short circuit\n",
    "In Python, logical operations such as `and` and `or` have two useful properties. They can operate on lists and they have ['short-circuit' behavior](https://docs.python.org/3/library/stdtypes.html). Try these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4fb6d319dcec88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['a', 'b']\n",
      "['Most', 'Likely']\n",
      "['least', 'of', 'all']\n"
     ]
    }
   ],
   "source": [
    "# example of logical operation on lists or sets\n",
    "print( [] and [\"a\",\"b\"] )\n",
    "print( [] or [\"a\",\"b\"] )\n",
    "#example of Short circuit behavior\n",
    "val1 =  [\"Most\",\"Likely\"] or [\"Less\",\"so\"] or [\"least\",\"of\",\"all\"]  # selects first, does not evalute remainder\n",
    "print(val1)\n",
    "val2 =  [] or [] or [\"least\",\"of\",\"all\"] # continues evaluation until there is a non-empty list\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a20114b13f93f7",
   "metadata": {},
   "source": [
    "The logical `or` could be used to implement the suggestion algorithm very compactly. Alternately, if/elif/else constructs could be used.\n",
    " \n",
    "**Step 2**: Create a 'best_words' dictionary where the 'key' is a suggestion and the 'value' is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.\n",
    "\n",
    "**Step 3**: Select the n best suggestions. There may be fewer than n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b1d859af86d95",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>edit_one_letter and edit_two_letters return *python sets*. </li>\n",
    "    <li> Sets have a handy <a href=\"https://docs.python.org/2/library/sets.html\" > set.intersection </a> feature</li>\n",
    "    <li>To find the keys that have the highest values in a dictionary, you can use the Counter dictionary to create a Counter object from a regular dictionary.  Then you can use Counter.most_common(n) to get the n most common keys.\n",
    "    </li>\n",
    "    <li>To find the intersection of two sets, you can use set.intersection or the & operator.</li>\n",
    "    <li>If you are not as familiar with short circuit syntax (as shown above), feel free to use if else statements instead.</li>\n",
    "    <li>To use an if statement to check of a set is empty, use 'if not x:' syntax </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc2d30da4320ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    # If the word is in the vocabulary, suggest the word\n",
    "    if word in vocab:\n",
    "        suggestions = [(word, probs[word])]\n",
    "    \n",
    "    # Otherwise, if there are suggestions from edit_one_letter that are in the vocabulary, use those\n",
    "    else:\n",
    "        edit_one_set = edit_one_letter(word)\n",
    "        edit_one_suggestions = [(w, probs[w]) for w in edit_one_set if w in vocab]\n",
    "        suggestions = edit_one_suggestions or suggestions\n",
    "    \n",
    "    # Otherwise, if there are suggestions from edit_two_letters that are in the vocabulary, use those\n",
    "    if not suggestions:\n",
    "        edit_two_set = edit_two_letters(word)\n",
    "        edit_two_suggestions = [(w, probs[w]) for w in edit_two_set if w in vocab]\n",
    "        suggestions = edit_two_suggestions or suggestions\n",
    "    \n",
    "    # Otherwise, suggest the input word\n",
    "    if not suggestions:\n",
    "        suggestions = [(word, 0)]\n",
    "    \n",
    "    # Get the n best suggestions based on probability\n",
    "    n_best = sorted(suggestions, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f585439972de8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  dys \n",
      "suggestions =  [('dye', 1.865184466743761e-05), ('days', 0.0004103405826836274)]\n",
      "word 0: days, probability 0.000410\n",
      "word 1: dye, probability 0.000019\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38dd42016fff79",
   "metadata": {},
   "source": [
    "##### 4 - Minimum Edit Distance\n",
    "\n",
    "Now that you have implemented your auto-correct, how do you evaluate the similarity between two strings? For example: 'waht' and 'what'\n",
    "\n",
    "Also how do you efficiently find the shortest path to go from the word, 'waht' to the word 'what'?\n",
    "\n",
    "You will implement a dynamic programming system that will tell you the minimum number of edits required to convert a string into another string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7708d7be24263680",
   "metadata": {},
   "source": [
    "#### 4.1 - Dynamic Programming\n",
    "\n",
    "Dynamic Programming breaks a problem down into subproblems which can be combined to form the final solution. Here, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.\n",
    "\n",
    "You have to create a matrix and update each element in the matrix as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72162f883e9ecc8a",
   "metadata": {},
   "source": [
    "The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string \"EER\" to \"\". The first row is initialized to represent the cumulative cost of inserting the target characters to convert from \"\" to \"NEAR\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae0394d7829207",
   "metadata": {},
   "source": [
    "![image](../pomocne_soubory/EditDistInit4.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9c023cdc607cb",
   "metadata": {},
   "source": [
    "![image](../pomocne_soubory/EditDistExample1.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b96fe4948d2b1",
   "metadata": {},
   "source": [
    "#### Exercise 11 - min_edit_distance\n",
    "\n",
    "Again, the word \"substitution\" appears in the figure, but think of this as \"replacement\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f61a2c1f56d92",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The range(start, stop, step) function excludes 'stop' from its output</li>\n",
    "    <li><a href=\"\" > words </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c0859e8ec28d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        D[row, 0] = row * del_cost  # cost of deleting characters from source\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1, n+1):\n",
    "        D[0, col] = col * ins_cost  # cost of inserting characters to match target\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1, m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1, n+1):\n",
    "            \n",
    "            # Initialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column\n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation to calculate for D[row, col] (the minimum of three calculated costs)\n",
    "            D[row, col] = min(D[row-1, col] + del_cost,   # Deletion\n",
    "                              D[row, col-1] + ins_cost,   # Insertion\n",
    "                              D[row-1, col-1] + r_cost)  # Replacement or no change\n",
    "            \n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m, n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4187c64a87b13d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "# testing your implementation \n",
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a97191c9fb8e20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  3 \n",
      "\n",
      "   #  n  e  a  r\n",
      "#  0  1  2  3  4\n",
      "e  1  2  1  2  3\n",
      "e  2  3  2  3  4\n",
      "r  3  4  3  4  3\n"
     ]
    }
   ],
   "source": [
    "# testing your implementation \n",
    "source =  'eer'\n",
    "target = 'near'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36db5ae789c3af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a634f8b794215b",
   "metadata": {},
   "source": [
    "Expected Results\n",
    "(empty)\n",
    "The 'replace()' routine utilizes all letters a-z one of which returns the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ca00dc608f2466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer eer 0\n"
     ]
    }
   ],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 2 and min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5038d41cf09de04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c23e1f933cdc5e2",
   "metadata": {},
   "source": [
    "##### 5 - Backtrace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd4e3fb9932013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrace(matrix, source, target, i, j):\n",
    "    if i == 0 and j == 0:\n",
    "        return []\n",
    "    elif i == 0:\n",
    "        return [f\"INSERT {target[j-1]}\"] + backtrace(matrix, source, target, i, j-1)\n",
    "    elif j == 0:\n",
    "        return [f\"DELETE {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j)\n",
    "    else:\n",
    "        if matrix[i][j] == matrix[i-1][j-1] and source[i-1] == target[j-1]:\n",
    "            return [f\"KEEP {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j-1)\n",
    "        elif matrix[i][j] == matrix[i-1][j-1] + 1:\n",
    "            return [f\"REPLACE {source[i-1]} with {target[j-1]}\"] + backtrace(matrix, source, target, i-1, j-1)\n",
    "        elif matrix[i][j] == matrix[i-1][j] + 1:\n",
    "            return [f\"DELETE {source[i-1]}\"] + backtrace(matrix, source, target, i-1, j)\n",
    "        else:\n",
    "            return [f\"INSERT {target[j-1]}\"] + backtrace(matrix, source, target, i, j-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0497c45c1c269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcda28-0296-4d08-9632-57b292f1c888",
   "metadata": {},
   "source": [
    "Part of Speech Tagging (POS) je proces, kdy kazdemu slovu priradime nejaky \"part of speech\", jako je podstatne jmeno, sloveso, atp. Jeden z moznych modelu jsou markovske processy, hidden markov chains a Viterbi Algoritmus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ba29d-bac5-4b5d-8b99-8f7895dd3529",
   "metadata": {},
   "source": [
    "POS se pouziva k taggovani \n",
    "- jmen,\n",
    "- speech recognition,\n",
    "- coreference resolution (na co ukazuji zajmena ve vete). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5fb07-40b1-4dba-b25e-eac30095e6d7",
   "metadata": {},
   "source": [
    "### LAB: POS, Prace se textem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cb41b1-af3b-458f-9804-a15ce962cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lines from 'WSJ_02-21.pos' file and save them into the 'lines' variable\n",
    "with open(\"../pomocne_soubory/WSJ_02-21.pos\", 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fe47e1-4d26-4ed6-807b-483ede6550a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tWord \tTag\n",
      "\n",
      "line number 1: In\tIN\n",
      "\n",
      "line number 2: an\tDT\n",
      "\n",
      "line number 3: Oct.\tNNP\n",
      "\n",
      "line number 4: 19\tCD\n",
      "\n",
      "line number 5: review\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print columns for reference\n",
    "print(\"\\t\\tWord\", \"\\tTag\\n\")\n",
    "\n",
    "# Print first five lines of the dataset\n",
    "for i in range(5):\n",
    "    print(f'line number {i+1}: {lines[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47933-1502-4c63-86d7-30e8b8a766d6",
   "metadata": {},
   "source": [
    "Vyznam zkratek pro Tag je mozne najit [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ed89df-35ba-4138-9bfb-584c018f407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In\\tIN\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first line (unformatted)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247d44a-4216-45c0-a0c1-943db9f4fc02",
   "metadata": {},
   "source": [
    "Indeed there is a tab between the word and the tag and a newline at the end of each line.\n",
    "\n",
    "#### Creating a vocabulary\n",
    "\n",
    "Now that you understand how the dataset is structured, you will create a vocabulary out of it. A vocabulary is made up of every word that appeared at least 2 times in the dataset. \n",
    "For this, follow these steps:\n",
    "- Get only the words from the dataset\n",
    "- Use a defaultdict to count the number of times each word appears\n",
    "- Filter the dict to only include words that appeared at least 2 times\n",
    "- Create a list out of the filtered dict\n",
    "- Sort the list\n",
    "\n",
    "For step 1 you can use the fact that every word and tag are separated by a tab and that words always come first. Using list comprehension the words list can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927b0de2-1d4f-4d22-822d-b31b8b988aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words from each line in the dataset\n",
    "words = [line.split('\\t')[0] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df4c89-6d51-4e9d-bb60-bbab21c1d0a8",
   "metadata": {},
   "source": [
    "Step 2 can be done easily by leveraging `defaultdict`. In case you aren't familiar with defaultdicts they are a special kind of dictionaries that **return the \"zero\" value of a type if you try to access a key that does not exist**. Since you want the frequencies of words, you should define the defaultdict with a type of `int`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415351ee-5b83-4751-b0f1-91bd8a9622d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define defaultdict of type 'int'\n",
    "freq = defaultdict(int)\n",
    "\n",
    "# Count frequency of ocurrence for each word in the dataset\n",
    "for word in words:\n",
    "    freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4739fb44-28e1-4d3a-8492-502365abf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary by filtering the 'freq' dictionary\n",
    "vocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3740cc89-759d-4479-aa95-6eee5e1f4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early\n",
      "Earnings\n",
      "Earth\n",
      "Earthquake\n",
      "East\n"
     ]
    }
   ],
   "source": [
    "# Sort the vocabulary\n",
    "vocab.sort()\n",
    "\n",
    "# Print some random values of the vocabulary\n",
    "for i in range(4000, 4005):\n",
    "    print(vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80272f84-7182-450e-bdde-d2c03682a313",
   "metadata": {},
   "source": [
    "#### Processing new text sources\n",
    "##### Dealing with unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f4451-7d59-46ad-b3b1-578dad915612",
   "metadata": {},
   "source": [
    "Now that you have a vocabulary, you will use it when processing new text sources. **A new text will have words that do not appear in the current vocabulary**. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding `unknown token`. \n",
    "\n",
    "This function will do the following checks and return an appropriate token:\n",
    "\n",
    "   - Check if the unknown word contains any character that is a digit \n",
    "       - return `--unk_digit--`\n",
    "   - Check if the unknown word contains any punctuation character \n",
    "       - return `--unk_punct--`\n",
    "   - Check if the unknown word contains any upper-case character \n",
    "       - return `--unk_upper--`\n",
    "   - Check if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb \n",
    "        - return `--unk_noun--`, `--unk_verb--`, `--unk_adj--`, `--unk_adv--` respectively\n",
    "\n",
    "If a word fails to fall under any condition then its token will be a plain `--unk--`. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns. \n",
    "\n",
    "This function is implemented next. Notice that the `any()` function is being heavily used. It returns `True` if at least one of the cases it evaluates is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99c70d5-8790-49a0-889f-6b0b917ad5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "    \"\"\"\n",
    "    Assign tokens to unknown words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Punctuation characters\n",
    "    # Try printing them out in a new cell!\n",
    "    punct = set(string.punctuation)\n",
    "    \n",
    "    # Suffixes\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    # Loop the characters in the word, check if any is a digit\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is a punctuation character\n",
    "    elif any(char in punct for char in word):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Loop the characters in the word, check if any is an upper case character\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Check if word ends with any noun suffix\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Check if word ends with any verb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Check if word ends with any adjective suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Check if word ends with any adverb suffix\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    \n",
    "    # If none of the previous criteria is met, return plain unknown\n",
    "    return \"--unk--\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680919d3-1142-4bb0-a929-5acd47397f2b",
   "metadata": {},
   "source": [
    "##### Getting the correct tag for a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84855568-c660-4f71-9541-cdbb980d9c46",
   "metadata": {},
   "source": [
    "All that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\n",
    "\n",
    "This function should check if a line is empty and if so, it should return a placeholder word and tag, `--n--` and `--s--` respectively. \n",
    "\n",
    "If not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function `assign_unk()` should be used.\n",
    "\n",
    "The function is implemented next. Notice that the `split()` method can be used without specifying the delimiter, in which case it will default to any whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3ece5b-f93c-4740-8fa8-959a7519fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # If line is empty return placeholders for word and tag\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "    else:\n",
    "        # Split line to separate word and tag\n",
    "        word, tag = line.split()\n",
    "        # Check if word is not in vocabulary\n",
    "        if word not in vocab: \n",
    "            # Handle unknown word\n",
    "            tag = assign_unk(word)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64a8660-e097-4b63-bdf2-a58903829dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--n--', '--s--')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b0b375-bc40-4276-9bc9-0dfbc23ef7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In', 'IN')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('In\\tIN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86d72e4-8a42-4b99-b042-e9be087cea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tardigrade', '--unk--')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('tardigrade\\tNN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e88f67-e775-4438-b461-d62ed228f87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scrutinize', '--unk_verb--')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('scrutinize\\tVB\\n', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ff3e1-a193-4a4b-8f89-1a14a13427da",
   "metadata": {},
   "source": [
    "### Markovsky Retezce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70623ef4-1f87-4037-a63d-e1feb98ca133",
   "metadata": {},
   "source": [
    "Tady asi neni poradne o cem. Proste klasicky markovsky retezce. Stavovy prostor, prechazi se mezi stavy pomoci matice prechodu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823527b343c91382",
   "metadata": {},
   "source": [
    "!<img src=\"../pomocne_soubory/markov_chains.png\"  width=\"920\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21e6a7eb3d92c7",
   "metadata": {},
   "source": [
    "Skryte maarkovske procesy se lisi tim, ze maji jeste jednu vrstvu, kterou pozorujeme, tzv. emission probabilities. \n",
    "Transition probabilities nevidime a nemuzeme je pozorovat. Nejcasteji se parametry modelu odhaduji pomoci maximalni \n",
    "verohodnosti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d1c749ec9fd1b",
   "metadata": {},
   "source": [
    "!<img src=\"../pomocne_soubory/hidden_mc_1.png\"  width=\"920\">\n",
    "!<img src=\"../pomocne_soubory/hidden_mc_2.png\"  width=\"720\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a21ab6d4514de",
   "metadata": {},
   "source": [
    "Vzhledem k tomu, ze Markovsky retezec je zalozeny na predpokladu, ze soucasny stav $t_i$ zavisi jen na predcozim \n",
    "stavu v case $t_{i-1}$, tak potrebujeme napocitat pravdepodobnostni distribuci pro vsechny stavy $t_i$ v zavislosti na\n",
    "vsech predchozich stavech $t_{i-1}$. Tuto distribuci muzeme zapsat jako:\n",
    "$$P(t_i|t_{i-1}, t_{i-2}, ..., t_1) = P(t_i|t_{i-1})$$\n",
    "Tuto distribuci muzeme zapsat jako matici prechodu $A$, kde $A_{ij} = P(t_j|t_i)$.\n",
    "Pravdepodobnosti prechodu $A$ muzeme odhadnout pomoci maximalni verohodnosti. V nasem pripade pak \n",
    "$$ P(t_i|t_{i-1}) = \n",
    "\\frac{C(t_{i-1}, t_i)} {\\sum_{j=1}^N C(t_{i-1}, t_j))}\n",
    "$$\n",
    "kde $C(t_{i-1}, t_i)$ je pocet prechodu kolikrat se tag $t_{i-1}$ objevil pred tagem $t_i$ a $\\sum_{j=1}^N C(t_{i-1},\n",
    " t_j) = C(t_{i-1})$ je pocet vyskytu tagu $t_{i-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5abd0a426d326f",
   "metadata": {},
   "source": [
    "Abychom se vyhli problemum, kdy pro nejakou pripad $t_{i-1, i}$ nebyl v trenovacim datasetu, tak se pouziva Laplace \n",
    "smoothing, kdy se ke kazdemu $C(t_{i-1}, t_i)$ pricita male $\\epsilon$. Vysledny vzorec pak vypada:\n",
    "$$ P(t_i|t_{i-1}) =\n",
    "\\frac{C(t_{i-1}, t_i) + \\epsilon} {\\sum_{j=1}^N C(t_{i-1}, t_j) + \\epsilon \\cdot N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa922b74f2a837a",
   "metadata": {},
   "source": [
    "Analogicky se pak vypocitava matice emisi $B$, kde $B_{ij} = P(w_j|t_i)$, kde $w_j$ je slovo a $t_i$ je tag. Vysledny vzorec\n",
    "pak vypada:\n",
    "$$ P(w_j|t_i) =\n",
    "\\frac{C(t_i, w_j) + \\epsilon} {\\sum_{j=1}^N C(t_i, w_j) + \\epsilon \\cdot N} = \n",
    "\\frac{C(t_i, w_j) + \\epsilon} {C(t_i) + \\epsilon \\cdot V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed44531a498d17",
   "metadata": {},
   "source": [
    "### LAB: Parts-of-Speech Tagging with Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc09ee1b9fb6a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tags for Adverb, Noun and To (the preposition) , respectively\n",
    "tags = ['RB', 'NN', 'TO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f947f7c0c8857ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'transition_counts' dictionary\n",
    "# Note: values are the same as the ones in the assignment\n",
    "transition_counts = {\n",
    "    ('NN', 'NN'): 16241,\n",
    "    ('RB', 'RB'): 2263,\n",
    "    ('TO', 'TO'): 2,\n",
    "    ('NN', 'TO'): 5256,\n",
    "    ('RB', 'TO'): 855,\n",
    "    ('TO', 'NN'): 734,\n",
    "    ('NN', 'RB'): 2431,\n",
    "    ('RB', 'NN'): 358,\n",
    "    ('TO', 'RB'): 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b86ab8275d26b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the number of tags in the 'num_tags' variable\n",
    "num_tags = len(tags)\n",
    "\n",
    "# Initialize a 3X3 numpy array with zeros\n",
    "transition_matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8128d246e19947fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of the matrix\n",
    "transition_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc24029dab788ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'RB', 'TO']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sorted version of the tag's list\n",
    "sorted_tags = sorted(tags)\n",
    "\n",
    "# Print sorted list\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8de51ee7991e385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n",
       "       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n",
       "       [7.3400e+02, 2.0000e+02, 2.0000e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop rows\n",
    "for i in range(num_tags):\n",
    "    # Loop columns\n",
    "    for j in range(num_tags):\n",
    "        # Define tag pair\n",
    "        tag_tuple = (sorted_tags[i], sorted_tags[j])\n",
    "        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n",
    "        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n",
    "\n",
    "# Print matrix\n",
    "transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41968b61e4f71964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'print_matrix' function\n",
    "def print_matrix(matrix):\n",
    "    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc2d23bc21102dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NN      RB      TO\n",
      "NN  16241.0  2431.0  5256.0\n",
      "RB    358.0  2263.0   855.0\n",
      "TO    734.0   200.0     2.0\n"
     ]
    }
   ],
   "source": [
    "# Print the 'transition_matrix' by calling the 'print_matrix' function\n",
    "print_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc261134e50eaaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23928.],\n",
       "       [ 3476.],\n",
       "       [  936.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sum of row for each row\n",
    "rows_sum = transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Print sum of rows\n",
    "rows_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d894ec5ae6e4e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NN        RB        TO\n",
      "NN  0.678745  0.101596  0.219659\n",
      "RB  0.102992  0.651036  0.245972\n",
      "TO  0.784188  0.213675  0.002137\n"
     ]
    }
   ],
   "source": [
    "# Normalize transition matrix\n",
    "transition_matrix = transition_matrix / rows_sum\n",
    "\n",
    "# Print normalized matrix\n",
    "print_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b04ee99a1e024ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "942dde20f2e0bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy transition matrix for for-loop example\n",
    "t_matrix_for = np.copy(transition_matrix)\n",
    "\n",
    "# Copy transition matrix for numpy functions example\n",
    "t_matrix_np = np.copy(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a83774e15fe9d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NN        RB        TO\n",
      "NN  10.761549  0.101596  0.219659\n",
      "RB   0.102992  8.804673  0.245972\n",
      "TO   0.784188  0.213675  6.843752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_76077/158205974.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n"
     ]
    }
   ],
   "source": [
    "# Loop values in the diagonal\n",
    "for i in range(num_tags):\n",
    "    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n",
    "\n",
    "# Print matrix\n",
    "print_matrix(t_matrix_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bc5d0a7bedd811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save diagonal in a numpy array\n",
    "d = np.diag(t_matrix_np)\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1be31c2b43653ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape diagonal numpy array\n",
    "d = np.reshape(d, (3,1))\n",
    "\n",
    "# Print shape of diagonal\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1dc794582c133445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NN        RB        TO\n",
      "NN  10.761549  0.101596  0.219659\n",
      "RB   0.102992  8.804673  0.245972\n",
      "TO   0.784188  0.213675  6.843752\n"
     ]
    }
   ],
   "source": [
    "# Perform the vectorized operation\n",
    "d = d + np.vectorize(math.log)(rows_sum)\n",
    "\n",
    "# Use numpy's 'fill_diagonal' function to update the diagonal\n",
    "np.fill_diagonal(t_matrix_np, d)\n",
    "\n",
    "# Print the matrix\n",
    "print_matrix(t_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f015842d509725c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for equality\n",
    "t_matrix_for == t_matrix_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d22cbb7f36c0d",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b163ccae62b455",
   "metadata": {},
   "source": [
    "Viterbiho algoritmus je dynamicky programovaci algoritmus, ktery se pouziva k nalezeni nejpravdepodobnejsi \n",
    "posloupnosti skrytych stavu (take zvana Viterbiho cesta), ktera vede k posloupnosti pozorovanych udalosti, zejmena v\n",
    " kontextu skrytych Markovovych modelu (HMM). Tento algoritmus je siroce pouzivan v ruznych aplikacich, jako je \n",
    " rozpoznavani reci, bioinformatika a dekodovani konvolucnich kodu. \n",
    "V nasem pripade budeme pouzivat Viterbiho algoritmus k tagovani slov v textu. \n",
    "Zjednodusene: napocitavame pravdepoodbnosti prechodu pro kazde pozorovane slovo a ukladame si indexy tagu z kterych \n",
    "jdeme. A zaroven si ukladame indexy tagu z kterych jdeme. Jakmile mame plnou matici vezmeme nejpravdepodonejsi cestu \n",
    "a vracime se pres indexy v D matici, abychom nasli tagy vsech predchozich slov. \n",
    "Ilustrativne zde: https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/9BigR/viterbi-forward-pass a \n",
    "tady: https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/4OAHP/viterbi-backward-pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ca8fbfc874dd8",
   "metadata": {},
   "source": [
    "The Viterbi algorithm is a dynamic programming algorithm used to find the most probable sequence of hidden states \n",
    "(also known as the Viterbi path) that results in a sequence of observed events, especially in the context of Hidden Markov Models (HMMs). This algorithm is widely used in various applications such as speech recognition, bioinformatics, and decoding of convolutional codes.\n",
    "\n",
    "#### Key Components of the Viterbi Algorithm\n",
    "\n",
    "1. **States**: $ S = \\{s_1, s_2, \\ldots, s_N\\} $ - Set of possible states in the HMM.\n",
    "2. **Observations**: $ O = \\{o_1, o_2, \\ldots, o_T\\} $ - Sequence of observed events.\n",
    "3. **Initial Probabilities**: $ \\pi = \\{\\pi_i\\} $ where $ \\pi_i $ is the probability of starting in state $ s_i $.\n",
    "4. **Transition Probabilities**: $ A = \\{a_{ij}\\} $ where $ a_{ij} $ is the probability of transitioning from state $ s_i $ to state $ s_j $.\n",
    "5. **Emission Probabilities**: $ B = \\{b_j(o_t)\\} $ where $ b_j(o_t) $ is the probability of observing $ o_t $ from state $ s_j $.\n",
    "\n",
    "#### Viterbi Algorithm Steps\n",
    "\n",
    "1. **Initialization**:\n",
    "    For each state $ s_i $:\n",
    "    $$\n",
    "    \\delta_1(i) = \\pi_i b_i(o_1)\n",
    "    $$\n",
    "    $$\n",
    "    \\psi_1(i) = 0\n",
    "    $$\n",
    "Where $ \\delta_1(i) $ is the highest probability of any path that reaches state $ s_i $ at time $ t = 1 $, and $ \\psi_1(i) $ is the backpointer to reconstruct the path.\n",
    "\n",
    "2. **Recursion**:\n",
    "    For each time step $ t = 2, 3, \\ldots, T $:\n",
    "    For each state $ s_j $:\n",
    "    $$\n",
    "    \\delta_t(j) = \\max_{i} [\\delta_{t-1}(i) a_{ij}] b_j(o_t)\n",
    "    $$\n",
    "    $$\n",
    "    \\psi_t(j) = \\arg\\max_{i} [\\delta_{t-1}(i) a_{ij}]\n",
    "    $$\n",
    "Where $ \\delta_t(j) $ is the highest probability of any path that reaches state $ s_j $ at time $ t $, and $ \\psi_t(j) $ is the backpointer.\n",
    "\n",
    "3. **Termination**:\n",
    "    $$\n",
    "    P^* = \\max_{i} [\\delta_T(i)]\n",
    "    $$\n",
    "    $$\n",
    "    q_T^* = \\arg\\max_{i} [\\delta_T(i)]\n",
    "    $$\n",
    "Where $ P^* $ is the highest probability of the most probable path, and $ q_T^* $ is the final state of this path.\n",
    "\n",
    "4. **Path Backtracking**:\n",
    "    For $ t = T-1, T-2, \\ldots, 1 $:\n",
    "    $$\n",
    "    q_t^* = \\psi_{t+1}(q_{t+1}^*)\n",
    "    $$\n",
    "This step reconstructs the most probable path by backtracking from the final state.\n",
    "\n",
    "#### Example Illustration\n",
    "\n",
    "Suppose we have an HMM with 2 states (Rainy and Sunny) and 3 possible observations (Walk, Shop, Clean). Let:\n",
    "\n",
    "- Initial probabilities: $ \\pi = \\{0.6, 0.4\\} $ (for Rainy and Sunny respectively)\n",
    "- Transition matrix:\n",
    "  $$\n",
    "  A = \\begin{bmatrix}\n",
    "  0.7 & 0.3 \\\\\n",
    "  0.4 & 0.6\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- Emission matrix:\n",
    "  $$\n",
    "  B = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 \\\\\n",
    "  0.6 & 0.3 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "If we observe the sequence $ O = \\{Walk, Shop, Clean\\} $:\n",
    "\n",
    "1. **Initialization**:\n",
    "    $$\n",
    "    \\delta_1(Rainy) = \\pi_1 \\cdot b_1(Walk) = 0.6 \\cdot 0.1 = 0.06\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_1(Sunny) = \\pi_2 \\cdot b_2(Walk) = 0.4 \\cdot 0.6 = 0.24\n",
    "    $$\n",
    "\n",
    "2. **Recursion for $ t = 2 $**:\n",
    "    $$\n",
    "    \\delta_2(Rainy) = \\max[ \\delta_1(Rainy) \\cdot a_{11}, \\delta_1(Sunny) \\cdot a_{21} ] \\cdot b_1(Shop)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Rainy) = \\max[ 0.06 \\cdot 0.7, 0.24 \\cdot 0.4 ] \\cdot 0.4 = 0.0384\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Sunny) = \\max[ \\delta_1(Rainy) \\cdot a_{12}, \\delta_1(Sunny) \\cdot a_{22} ] \\cdot b_2(Shop)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_2(Sunny) = \\max[ 0.06 \\cdot 0.3, 0.24 \\cdot 0.6 ] \\cdot 0.3 = 0.0432\n",
    "    $$\n",
    "\n",
    "3. **Recursion for $ t = 3 $**:\n",
    "    $$\n",
    "    \\delta_3(Rainy) = \\max[ \\delta_2(Rainy) \\cdot a_{11}, \\delta_2(Sunny) \\cdot a_{21} ] \\cdot b_1(Clean)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Rainy) = \\max[ 0.0384 \\cdot 0.7, 0.0432 \\cdot 0.4 ] \\cdot 0.5 = 0.01344\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Sunny) = \\max[ \\delta_2(Rainy) \\cdot a_{12}, \\delta_2(Sunny) \\cdot a_{22} ] \\cdot b_2(Clean)\n",
    "    $$\n",
    "    $$\n",
    "    \\delta_3(Sunny) = \\max[ 0.0384 \\cdot 0.3, 0.0432 \\cdot 0.6 ] \\cdot 0.1 = 0.002592\n",
    "    $$\n",
    "\n",
    "4. **Termination**:\n",
    "    $$\n",
    "    P^* = \\max[ \\delta_3(Rainy), \\delta_3(Sunny) ] = \\max[ 0.01344, 0.002592 ] = 0.01344\n",
    "    $$\n",
    "    $$\n",
    "    q_3^* = \\arg\\max[ \\delta_3(Rainy), \\delta_3(Sunny) ] = Rainy\n",
    "    $$\n",
    "\n",
    "5. **Path Backtracking**:\n",
    "    $$\n",
    "    q_2^* = \\psi_3(Rainy) = Sunny \\quad (since \\delta_2(Sunny) \\cdot a_{21} \\cdot b_1(Clean) was the max path)\n",
    "    $$\n",
    "    $$\n",
    "    q_1^* = \\psi_2(Sunny) = Sunny \\quad (since \\delta_1(Sunny) \\cdot a_{22} \\cdot b_2(Shop) was the max path)\n",
    "    $$\n",
    "\n",
    "Thus, the most probable hidden state sequence for the observations $ \\{Walk, Shop, Clean\\} $ is $ \\{Sunny, Sunny, Rainy\\} $.\n",
    "\n",
    "This explanation covers the essential steps and concepts involved in the Viterbi algorithm. If you have any specific questions or need further elaboration on any part, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7506db0e15f81",
   "metadata": {},
   "source": [
    "### LAB: Assignment 2: POS, HMM, Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a8376f1c0b5d6",
   "metadata": {},
   "source": [
    "#### 0 - Data Sources\n",
    "This assignment will use two tagged data sets collected from the **Wall Street Journal (WSJ)**. \n",
    "\n",
    "[Here](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. \n",
    "- One data set (**WSJ-2_21.pos**) will be used for **training**.\n",
    "- The other (**WSJ-24.pos**) for **testing**. \n",
    "- The tagged training data has been preprocessed to form a vocabulary (**hmm_vocab.txt**). \n",
    "- The words in the vocabulary are words from the training set that were used two or more times. \n",
    "- The vocabulary is augmented with a set of 'unknown word tokens', described below. \n",
    "\n",
    "The training set will be used to create the emission, transition and tag counts. \n",
    "\n",
    "The test set (WSJ-24.pos) is read in to create `y`. \n",
    "- This contains both the test text and the true tag. \n",
    "- The test set has also been preprocessed to remove the tags to form **test_words.txt**. \n",
    "- This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in **utils_pos.py**. \n",
    "- This forms the list `prep`, the preprocessed text used to test our  POS taggers.\n",
    "\n",
    "A POS tagger will necessarily encounter words that are not in its datasets. \n",
    "- To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. \n",
    "- For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. \n",
    "- A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transition and tag data structures.\n",
    "\n",
    "\n",
    "<img src = \"pomocne_soubory/DataSources1.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49201e67025296ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"../pomocne_soubory/WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "94431e2b93f5923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data, split by each line of text, and save the list\n",
    "with open(\"../pomocne_soubory/hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb89762da30dc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "':6\n",
      "'':7\n",
      "'40s:8\n",
      "'60s:9\n",
      "'70s:10\n",
      "'80s:11\n",
      "'86:12\n",
      "'90s:13\n",
      "'N:14\n",
      "'S:15\n",
      "'d:16\n",
      "'em:17\n",
      "'ll:18\n",
      "'m:19\n",
      "'n':20\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {}\n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a54b7ae60823b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"../pomocne_soubory/WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "    \n",
    "print(\"A sample of the test corpus\")\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d564b49b6abbc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of the test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"../pomocne_soubory/test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2ab1f1c93e2e",
   "metadata": {},
   "source": [
    "#### 1 - Parts-of-speech Tagging \n",
    "\n",
    "<a name='1.1'></a>\n",
    "##### 1.1 - Training\n",
    "You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. \n",
    "\n",
    "In this section, you will find the words that are not ambiguous. \n",
    "- For example, the word `is` is a verb and it is not ambiguous. \n",
    "- In the `WSJ` corpus, $86$% of the token are unambiguous (meaning they have only one tag) \n",
    "- About $14\\%$ are ambiguous (meaning that they have more than one tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7d67d854c2bc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860819295336c746",
   "metadata": {},
   "source": [
    "##### Transition counts\n",
    "- The first dictionary is the `transition_counts` dictionary which computes the number of times each tag happened next to another tag. \n",
    "\n",
    "This dictionary will be used to compute: \n",
    "$$P(t_i |t_{i-1}) \\tag{1}$$\n",
    "\n",
    "This is the probability of a tag at position $i$ given the tag at position $i-1$.\n",
    "\n",
    "In order for you to compute equation 1, you will create a `transition_counts` dictionary where \n",
    "- The keys are `(prev_tag, tag)`\n",
    "- The values are the number of times those two tags appeared in that order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618af5b2b4e0c72",
   "metadata": {},
   "source": [
    "##### Emission counts\n",
    "\n",
    "The second dictionary you will compute is the `emission_counts` dictionary. This dictionary will be used to compute:\n",
    "\n",
    "$$P(w_i|t_i)\\tag{2}$$\n",
    "\n",
    "In other words, you will use it to compute the probability of a word given its tag. \n",
    "\n",
    "In order for you to compute equation 2, you will create an `emission_counts` dictionary where \n",
    "- The keys are `(tag, word)` \n",
    "- The values are the number of times that pair showed up in your training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ea61481c03bcf",
   "metadata": {},
   "source": [
    "##### Tag counts\n",
    "\n",
    "The last dictionary you will compute is the `tag_counts` dictionary. \n",
    "- The key is the tag \n",
    "- The value is the number of times each tag appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb409675bb1fb8",
   "metadata": {},
   "source": [
    "#### Exercise 1 - create_dictionaries\n",
    "\n",
    "**Instructions:** Write a program that takes in the `training_corpus` and returns the three dictionaries mentioned above `transition_counts`, `emission_counts`, and `tag_counts`. \n",
    "- `emission_counts`: maps (tag, word) to the number of times it happened. \n",
    "- `transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. \n",
    "- `tag_counts`: maps (tag) to the number of times it has occured. \n",
    "\n",
    "Implementation note: This routine utilises *defaultdict*, which is a subclass of *dict*. \n",
    "- A standard Python dictionary throws a *KeyError* if you try to access an item with a key that is not currently in the dictionary. \n",
    "- In contrast, the *defaultdict* will create an item of the type of the argument, in this case an integer with the default value of 0. \n",
    "- See [defaultdict](https://docs.python.org/3.3/library/collections.html#defaultdict-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "82729ca95f8975fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab, verbose=True):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0 and verbose:\n",
    "            print(f\"word count = {i}\")\n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        # the function is defined as: get_word_tag(line, vocab)\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc1af3814bcda307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count = 50000\n",
      "word count = 100000\n",
      "word count = 150000\n",
      "word count = 200000\n",
      "word count = 250000\n",
      "word count = 300000\n",
      "word count = 350000\n",
      "word count = 400000\n",
      "word count = 450000\n",
      "word count = 500000\n",
      "word count = 550000\n",
      "word count = 600000\n",
      "word count = 650000\n",
      "word count = 700000\n",
      "word count = 750000\n",
      "word count = 800000\n",
      "word count = 850000\n",
      "word count = 900000\n",
      "word count = 950000\n"
     ]
    }
   ],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "568cdea5b01ad7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7dddaed1df4d0993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221148ca4a840a3",
   "metadata": {},
   "source": [
    "#### Exercise 2 - predict_pos\n",
    "\n",
    "**Instructions:** Implement `predict_pos` that computes the accuracy of your model. \n",
    "\n",
    "- This is a warm up exercise. \n",
    "- To assign a part of speech to a word, assign the most frequent POS for that word in the training set. \n",
    "- Then evaluate how well this approach works.  Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same.  If so, the prediction was correct!\n",
    "- Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b32646c3678dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output:\n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Initialize total count to 0\n",
    "    total = 0\n",
    "    \n",
    "    for word, y_tup in zip(prep, y):\n",
    "        # Split the (word, POS) string into a list of two items\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        # Verify that y_tup contain both word and POS\n",
    "        if len(y_tup_l) == 2:\n",
    "            # Set the true POS label for this word\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word\n",
    "            continue\n",
    "        \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        # If the word is in the vocabulary...\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                # define the key as the tuple containing the POS and word\n",
    "                key = (pos, word)\n",
    "                \n",
    "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
    "                if key in emission_counts:\n",
    "                    # get the emission count of the (pos,word) tuple\n",
    "                    count = emission_counts[key]\n",
    "                    \n",
    "                    # keep track of the POS with the largest count\n",
    "                    if count > count_final:\n",
    "                        # update the final count (largest count)\n",
    "                        count_final = count\n",
    "                        # update the final POS\n",
    "                        pos_final = pos\n",
    "            \n",
    "            # If the final POS (with the largest count) matches the true POS:\n",
    "            if pos_final == true_label:\n",
    "                # Update the number of correct predictions\n",
    "                num_correct += 1\n",
    "        \n",
    "        # Keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "\n",
    "    accuracy = num_correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "475c9386abdce3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.9253\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f50806c903194",
   "metadata": {},
   "source": [
    "####  2.1 - Generating Matrices\n",
    "\n",
    "##### Creating the 'A' transition probabilities matrix\n",
    "Now that you have your `emission_counts`, `transition_counts`, and `tag_counts`, you will start implementing the Hidden Markov Model. \n",
    "\n",
    "This will allow you to quickly construct the \n",
    "- `A` transition probabilities matrix.\n",
    "- and the `B` emission probabilities matrix. \n",
    "\n",
    "You will also use some smoothing when computing these matrices. \n",
    "\n",
    "Here is an example of what the `A` transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n",
    "\n",
    "\n",
    "|**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...\n",
    "| --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----\n",
    "|**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...\n",
    "|**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...\n",
    "|**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...\n",
    "|**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...\n",
    "|**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...\n",
    "| ... |...| ...          | ...          | ...          | ...      | ...        | ...\n",
    "\n",
    "Note that the matrix above was computed with smoothing. \n",
    "\n",
    "Each cell gives you the probability to go from one part of speech to another. \n",
    "- In other words, there is a 4.47e-8 chance of going from parts-of-speech `TO` to `RP`. \n",
    "- The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n",
    "\n",
    "The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "944367cee71e48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "\n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "\n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "\n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i], all_tags[j])  # tuple of form (tag,tag)\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in trans_keys:  # Replace None in this line with the proper condition.\n",
    "\n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "\n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "\n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and total number of tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4818e7d9d95c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "View a subset of transition matrix A\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a03a2754cd8b5",
   "metadata": {},
   "source": [
    "#### Create the 'B' emission probabilities matrix\n",
    "\n",
    "Now you will create the `B` emission matrix which computes the emission probability. \n",
    "\n",
    "You will use smoothing as defined below: \n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. \n",
    "\n",
    "The matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. \n",
    "\n",
    "Here is an example of the matrix, only a subset of tags and words are shown: \n",
    "<p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>\n",
    "\n",
    "|**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|\n",
    "|----|----|--------------|--------------|--------------|--------------|-------------|----|\n",
    "|**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|\n",
    "|**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|\n",
    "|**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|\n",
    "|**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|\n",
    "|**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|\n",
    "|**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|\n",
    "| ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1725c59213ed5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
    "               within the function it'll be treated as a list\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for\n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # Get a set of all (POS, word) tuples \n",
    "    # from the keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "\n",
    "    # Go through each row (POS tags)\n",
    "    for i in range(num_tags): # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Go through each column (words)\n",
    "        for j in range(num_words): # Replace None in this line with the proper range.\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0 \n",
    "                    \n",
    "            # Define the (POS tag, word) tuple for this row and column\n",
    "            key = (all_tags[i], vocab[j])  # tuple of form (tag,word)\n",
    "            \n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emis_keys:  # Replace None in this line with the proper condition.\n",
    "                \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts[key]\n",
    "            \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5dfc5054e8f36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "alpha = 0.001\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1c84da154ab14",
   "metadata": {},
   "source": [
    "#### 3 - Viterbi Algorithm and Dynamic Programming\n",
    "\n",
    "In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps for you. \n",
    "\n",
    "* **Initialization** - In this part you initialize the `best_paths` and `best_probabilities` matrices that you will be populating in `feed_forward`.\n",
    "* **Feed forward** - At each step, you calculate the probability of each path happening and the best paths up to that point. \n",
    "* **Feed backward**: This allows you to find the best path with the highest probabilities. \n",
    "\n",
    "<a name='3.1'></a>\n",
    "##### 3.1 - Initialization \n",
    "\n",
    "You will start by initializing two matrices of the same dimension. \n",
    "\n",
    "- best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
    "\n",
    "- best_paths: A matrix that helps you trace through the best possible path in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8c55ee5662933",
   "metadata": {},
   "source": [
    "#### Exercise 5 - initialize\n",
    "**Instructions**: \n",
    "Write a program below that initializes the `best_probs` and the `best_paths` matrix. \n",
    "\n",
    "Both matrices will be initialized to zero except for column zero of `best_probs`.  \n",
    "- Column zero of `best_probs` is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\"). \n",
    "- This allows you to reference the **A** matrix for the transition probability\n",
    "\n",
    "Here is how to initialize column 0 of `best_probs`:\n",
    "- The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\\textrm{best_probs}[s_{idx}, i]$.\n",
    "- This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\\mathbf{B}[i, vocab[corpus[0]]]$.\n",
    "- Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). \n",
    "- **vocab** is a dictionary that returns the unique integer that refers to that particular word.\n",
    "\n",
    "Conceptually, it looks like this:\n",
    "$\\textrm{best_probs}[i, 0] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, vocab[corpus[0]] ]$\n",
    "\n",
    "\n",
    "In order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:\n",
    "\n",
    "$best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n",
    "\n",
    "Please use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28950a5a9b090e12",
   "metadata": {},
   "source": [
    "The example below shows the initialization assuming the corpus starts with the phrase \"Loss tracks upward\".\n",
    "\n",
    "<img src = \"pomocne_soubory/Initialize4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "901ddfe931550958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    # POS tags in the rows, number of words in the corpus as the columns\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    # POS tags in the rows, number of words in the corpus as columns\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "\n",
    "    # Go through each of the POS tags\n",
    "    for i in range(num_tags):  # Replace None in this line with the proper range.\n",
    "        \n",
    "        # Initialize best_probs at POS tag 'i', column 0\n",
    "        # Check the formula in the instructions above\n",
    "        best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]])\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "573d18e877f24b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5290f5e5497ad916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\")\n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62474106c4912eba",
   "metadata": {},
   "source": [
    "####  3.2 - Viterbi Forward\n",
    "\n",
    "In this part of the assignment, you will implement the `viterbi_forward` segment. In other words, you will populate your `best_probs` and `best_paths` matrices.\n",
    "- Walk forward through the corpus.\n",
    "- For each word, compute a probability for each possible tag. \n",
    "- Unlike the previous algorithm `predict_pos` (the 'warm-up' exercise), this will include the path up to that (word,tag) combination. \n",
    "\n",
    "Here is an example with a three-word corpus \"Loss tracks upward\":\n",
    "- Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. \n",
    "- In the diagram below, the first word \"Loss\" is already initialized. \n",
    "- The algorithm will compute a probability for each of the potential tags in the second and future words. \n",
    "\n",
    "Compute the probability that the tag of the second word ('tracks') is a verb, 3rd person singular present (VBZ).  \n",
    "- In the `best_probs` matrix, go to the column of the second word ('tracks'), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\n",
    "- Examine each of the paths from the tags of the first word ('Loss') and choose the most likely path.  \n",
    "- An example of the calculation for **one** of those paths is the path from ('Loss', NN) to ('tracks', VBZ).\n",
    "- The log of the probability of the path up to and including the first word 'Loss' having POS tag NN is $-14.32$.  The `best_probs` matrix contains this value -14.32 in the column for 'Loss' and row for 'NN'.\n",
    "- Find the probability that NN transitions to VBZ.  To find this probability, go to the `A` transition matrix, and go to the row for 'NN' and the column for 'VBZ'.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. \n",
    "- Find the log of the probability that the tag VBS would 'emit' the word 'tracks'.  To find this, look at the 'B' emission matrix in row 'VBZ' and the column for the word 'tracks'.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.\n",
    "- The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the `best_probs` matrix at row 'VBZ' and column 'tracks' (as seen in the cell that is highlighted in light orange in the diagram).\n",
    "- All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column 'tracks' of matrix `best_probs`, and so the most likely path to 'VBZ' is from 'NN'.  'NN' is in row 20 of the `best_probs` matrix, so $20$ is the most likely path.\n",
    "- Store the most likely path $20$ in the `best_paths` table.  This is highlighted in light orange in the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f37d7e52e9b91d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cee0ec187ad6d808",
   "metadata": {},
   "source": [
    "The formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:\n",
    "\n",
    "$\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})$\n",
    "\n",
    "where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.\n",
    "\n",
    "$\\mathrm{path} = k$\n",
    "\n",
    "where $k$ is the integer representing the previous POS tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9034c42a832c84",
   "metadata": {},
   "source": [
    "#### Exercise 6 - viterbi_forward\n",
    "\n",
    "Instructions: Implement the `viterbi_forward` algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags` using the pseudo code below.\n",
    "\n",
    "```\n",
    "for each word in the corpus\n",
    "\n",
    "    for each POS tag type that this word may be\n",
    "    \n",
    "        for POS tag type that the previous word could be\n",
    "        \n",
    "            compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n",
    "            \n",
    "            retain the highest probability computed for the current word\n",
    "            \n",
    "            set best_probs to this highest probability\n",
    "            \n",
    "            set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability \n",
    "```\n",
    "\n",
    "Please use [math.log](https://docs.python.org/3/library/math.html) to compute the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e83579a6bea2",
   "metadata": {},
   "source": [
    ".<img src = \"../pomocne_soubory/Forward4.PNG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7c1273872fe92b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transition and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0 and verbose:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # for every pos tag\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags):\n",
    "            \n",
    "                # Calculate the probability\n",
    "                prob = best_probs[k, i-1] + \\\n",
    "                       math.log(A[k, j]) + \\\n",
    "                       math.log(B[j, vocab[test_corpus[i]]])\n",
    "\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i:\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j, i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j, i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eb60d2c58137e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f330faadf04813be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45929f4943fed78",
   "metadata": {},
   "source": [
    "#### Exercise 7 - viterbi_backward\n",
    "Implement the `viterbi_backward` algorithm, which returns a list of predicted POS tags for each word in the corpus.\n",
    "\n",
    "- Note that the numbering of the index positions starts at 0 and not 1. \n",
    "- `m` is the number of words in the corpus.  \n",
    "    - So the indexing into the corpus goes from `0` to `m - 1`.\n",
    "    - Also, the columns in `best_probs` and `best_paths` are indexed from `0` to `m - 1`\n",
    "\n",
    "\n",
    "**In Step 1:**       \n",
    "Loop through all the rows (POS tags) in the last entry of `best_probs` and find the row (POS tag) with the maximum value.\n",
    "Convert the unique integer ID to a tag (a string representation) using the list `states`.  \n",
    "\n",
    "Referring to the three-word corpus described above:\n",
    "- `z[2] = 28`: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28.  Store 28 in `z` at position 2.\n",
    "- `states[28]` is 'RB': The POS tag ID 28 refers to the POS tag 'RB'.\n",
    "- `pred[2] = 'RB'`: In array `pred`, store the POS tag for the word 'upward'.\n",
    "\n",
    "**In Step 2:**  \n",
    "- Starting at the last column of best_paths, use `best_probs` to find the most likely POS tag for the last word in the corpus.\n",
    "- Then use `best_paths` to find the most likely POS tag for the previous word. \n",
    "- Update the POS tag for each word in `z` and in `preds`.\n",
    "\n",
    "Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.  \n",
    "`z[1] = best_paths[z[2],2]`  \n",
    "\n",
    "The small test following the routine prints the last few words of the corpus and their states to aid in debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71a472fc4bb5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m # DO NOT replace the \"None\"\n",
    "    \n",
    "    ## Step 1 ##\n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) with highest probability for the last word\n",
    "    for k in range(num_tags):\n",
    "        # If the probability of POS tag at row k is better than the previously best probability for the last word:\n",
    "        if best_probs[k, m-1] > best_prob_for_last_word:\n",
    "            # Store the new best probability for the last word\n",
    "            best_prob_for_last_word = best_probs[k, m-1]\n",
    "            # Store the unique integer ID of the POS tag which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag from its unique integer ID into the string representation\n",
    "    # using the 'states' list and store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(m - 1, 0, -1):\n",
    "        # Retrieve the unique integer ID of the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        # In best_paths, go to the row representing the POS tag of word i and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' list, where the key is the unique integer ID of the POS tag, and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "11bff3458f7d9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e8ceb7471650bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a81883f25bc8aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple) != 2:\n",
    "            continue\n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag:\n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea8c2f10c03ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211197e9e11464e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e6420cd2408e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7736c97a59b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823f90bfe7c4c872",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Autocomplete and N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b88e648b6616b4",
   "metadata": {},
   "source": [
    "N-gramovy jazykovy model je statisticky model, ktery predikuje nasledujici slovo v zavislosti na predchozich N \n",
    "slovech. Pouziva se v mnoha aplikacich, jako je napriklad automaticke dokoncovani textu, automaticke opravovani\n",
    "textu, automaticke generovani textu, a dalsi. N-gramovy model pocita pravdepodobnost vyskytu slova na zaklade\n",
    "predchozich N slov. Tj. pravdepodobnost slova $w_i$ na zaklade predchozich N slov $w_{i-N}, w_{i-N+1}, ..., w_{i-1}$\n",
    "je dana vzorcem:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1})$$\n",
    "N-gramovy model se vytvari pomoci trenovaciho korpusu textu. Trenovaci korpus je mnozina textovych dokumentu, ktere\n",
    "jsou pouzity k trenovani modelu. N-gramovy model se vytvari pomoci maximalni verohodnosti (maximum likelihood). Tj.\n",
    "naleznete takove parametry modelu, ktere maximalizuji pravdepodobnost vyskytu trenovacich dat. N-gramovy model se\n",
    "vytvori pomoci vzorce:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) = \\frac{C(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)}{C(w_{i-N}, w_{i-N+1}, ..., w_{i-1})}$$\n",
    "kde $C(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)$ je pocet vyskytu N-gramu $w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i$ v\n",
    "trenovacim korpusu a $C(w_{i-N}, w_{i-N+1}, ..., w_{i-1})$ je pocet vyskytu N-1 gramu $w_{i-N}, w_{i-N+1}, ..., w_{i-1}$\n",
    "v trenovacim korpusu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fe6c795ae1b1b",
   "metadata": {},
   "source": [
    "K vypocut N-gramoveho modelu pouzijeme Bayesuvu vetu:\n",
    "$$P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) = \\frac{P(w_{i-N}, w_{i-N+1}, ..., w_{i-1}, w_i)}{P(w_{i-N}, w_{i-N+1}, ..., w_{i-1})}$$\n",
    "Pouzijeme Bayesovo pravidlo: \n",
    "$$P(B|A) = \\frac{P(A,B)}{P(A)} \\rightarrow P(A,B) = P(B|A)P(A)$$\n",
    "$$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332b73e9565dfdd",
   "metadata": {},
   "source": [
    "Pak se veta \"the teacher drinks tea\" rozpadne na:\n",
    "$$P(the, teacher, drinks, tea) = P(the)P(teacher|the)P(drinks|the, teacher)P(tea|the, teacher, drinks)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9181ab0787cb018",
   "metadata": {},
   "source": [
    "Protoze ovsem v korpusu se nemusi nachazat takhle dlouhy vety, tak se pouziva Markovuv predpoklad, ktery rika, ze\n",
    "pravdepodobnost vyskytu slova zavisi pouze na predchozich N slovech. Tj.:\n",
    "- Bigramovy model: $P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) \\approx P(w_i|w_{i-1})$ \n",
    "- Trigramovy model: $P(w_i | w_{i-N}, w_{i-N+1}, ..., w_{i-1}) \\approx P(w_i|w_{i-2}, w_{i-1})$\n",
    "\n",
    "A pak se cela veta modeluje jako:\n",
    "$$ P(w_i^n) \\approx \\prod_{i=1}^n P(w_i|w_{i-1}) \\quad ,$$\n",
    "kde $w_i^n$ je veta o delce n slov zacinajici slovem i.\n",
    "$$ P(w_i^n) \\approx P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b104a8ea66c5063",
   "metadata": {},
   "source": [
    "Vety zaciname a ukoncujeme nejakym specialnim znakem. Tokenem. Zpravidla se pouziva token \"\\<s\\>\" pro zacatek vety a \n",
    "token \"\\</s\\>\" pro konec vety. Tj. vety se modeluji jako:\n",
    "$$ P(w_i^n) \\approx P(w_1|<s>)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})P(</s>|w_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc17120b7202af",
   "metadata": {},
   "source": [
    "V jinych modelech je to pak napriklad \"SOS\" jako start of sentence a \"EOS\" jako end of sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a029f3db8e83fa",
   "metadata": {},
   "source": [
    "### LAB: Corpus Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6a30ea56db8efa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmateju/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk               # NLP toolkit\n",
    "import re                 # Library for Regular expression operations\n",
    "\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9a067b9e39ac87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
     ]
    }
   ],
   "source": [
    "# change the corpus to lowercase\n",
    "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# note that word \"learning\" will now be the same regardless of its position in the sentence\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cd7c81904f44d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "source": [
    "# remove special characters\n",
    "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
    "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a73672d84edd53",
   "metadata": {},
   "source": [
    "Note that this process gets rid of the happy face made with punctuations :). Remember that for sentiment analysis, this emoticon was very important. However, we will not consider it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae92c81a0a71e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
      "time parts = ['07', '33', '35']\n"
     ]
    }
   ],
   "source": [
    "# split text by a delimiter to array\n",
    "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
    "\n",
    "# get the date parts in array\n",
    "date_parts = input_date.split(\" \")\n",
    "print(f\"date parts = {date_parts}\")\n",
    "\n",
    "#get the time parts in array\n",
    "time_parts = date_parts[4].split(\":\")\n",
    "print(f\"time parts = {time_parts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd239200504ac78",
   "metadata": {},
   "source": [
    "Once you have a list of sentences, the next step is to split each sentence into a list of words.\n",
    "\n",
    "This process could be done in several ways, even using the str.split method described above, but we will use the NLTK library [nltk](https://www.nltk.org/) to help us with that.\n",
    "\n",
    "In the code assignment, you will use the method [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize) to split your sentence into a list of words. Let us try the method in an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68aed7095c919cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sentence into an array of words\n",
    "\n",
    "sentence = 'i am happy because i am learning.'\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(f'{sentence} -> {tokenized_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d5ec7c7e6c001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lengths of the words: \n",
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# find length of each word in the tokenized sentence\n",
    "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "word_lengths = [(word, len(word)) for word in sentence] # Create a list with the word lengths using a list comprehension\n",
    "print(f' Lengths of the words: \\n{word_lengths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de499cb897a62afa",
   "metadata": {},
   "source": [
    "The next step is to build n-grams from the tokenized sentences. \n",
    "\n",
    "A sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\n",
    "\n",
    "Here is an example method that prints all trigrams in the given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39ac851fa300809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
      "\n",
      "['i', 'am', 'happy']\n",
      "['am', 'happy', 'because']\n",
      "['happy', 'because', 'i']\n",
      "['because', 'i', 'am']\n",
      "['i', 'am', 'learning']\n",
      "['am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_trigram(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    Prints all trigrams in the given tokenized sentence.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentence: The words list.\n",
    "    \n",
    "    Returns:\n",
    "        No output\n",
    "    \"\"\"\n",
    "    # note that the last position of i is 3rd to the end\n",
    "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tokenized_sentence[i : i + 3]\n",
    "        print(trigram)\n",
    "\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
    "sentence_to_trigram(tokenized_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cdfd4024db814",
   "metadata": {},
   "source": [
    "The n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to \n",
    "calculate the probability of an n-gram.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n",
    "\\end{equation*}\n",
    "\n",
    "The following code shows how to get an (n-1)-gram  prefix from n-gram on an example of getting trigram from a 4-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "59b8a0b3a67e7480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# get trigram prefix from a 4-gram\n",
    "fourgram = ['i', 'am', 'happy','because']\n",
    "trigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "675a87ea0e7c25fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
     ]
    }
   ],
   "source": [
    "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
    "n = 3\n",
    "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "tokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"<e>\"]\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d475734a30c7dd",
   "metadata": {},
   "source": [
    "### N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1b1e4f95ec67a",
   "metadata": {},
   "source": [
    "Pro n-gramovy jazykovy model potrebujeme napocitat pravdepodobnosti prechodu. Tj. nejprve napoctame matice poctu \n",
    "n-gramu: pro kazdou vetu v korpusu napocitame vsechny n-gramy a z nich napocitame pravdepodobnosti prechodu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcb398864ba72e",
   "metadata": {},
   "source": [
    "![image](../pomocne_soubory/bigram_matice_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092994841d1575b",
   "metadata": {},
   "source": [
    "Pro vypocet pravdepodobnosti prechodu pouzijeme vzorec:\n",
    "$$P(w_n|w_{n-N+1}^{n-1})=\\frac{C(w_{n-N+1}^n, w_n)}{C(w_{n-N+1}^{n-1})} \\quad ,$$\n",
    "kde $C(w_{n-N+1}^n, w_n)$ je pocet vyskytu n-gramu $w_{n-N+1}^n, w_n$ v korpusu a $C(w_{n-N+1}^{n-1})$ je pocet \n",
    "vyskytu n-1 gramu $w_{n-N+1}^{n-1}$ v korpusu.\n",
    "$$sum(row) = \\sum_{i=1}^{N} C(w_{n-N+1}^{n-1}, w_i) = C(w_{n-N+1}^{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14340203a088b5",
   "metadata": {},
   "source": [
    "Pro bigramovy model pak pro vypocet pravdepodobnosti vety pouzijeme vzorec:\n",
    "$$P(w_1^n) = \\prod_{i=1}^n P(w_i|w_{i-1})$$\n",
    "Pro trigramovy model pak:\n",
    "$$P(w_1^n) = \\prod_{i=1}^n P(w_i|w_{i-2}, w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122786a114099879",
   "metadata": {},
   "source": [
    "Pro lepsi numerickou stabilitu se casto pouziva logaritmus pravdepodobnosti:\n",
    "$$log(P(w_1^n)) = \\sum_{i=1}^n log(P(w_i|w_{i-1}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c64f14dd3bdd9",
   "metadata": {},
   "source": [
    "Algoritmus modelu pak funguje nasledovne: \n",
    "1. zvoliime zacatek vety \\<s\\> a prvni slovo vety\n",
    "2. zvolime dalsi slovo na zaklade pravdepodobnosti prechodu\n",
    "3. pokracujeme dokud nedojdeme na konec vety \\</s\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab431ea192373a4b",
   "metadata": {},
   "source": [
    "### LAB: N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9ce38174f027ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of n-gram ('i', 'am', 'happy'): 2\n",
      "n-gram ('i', 'am', 'learning') missing\n",
      "n-gram ('i', 'am', 'learning') found\n"
     ]
    }
   ],
   "source": [
    "# manipulate n_gram count dictionary\n",
    "\n",
    "n_gram_counts = {\n",
    "    ('i', 'am', 'happy'): 2,\n",
    "    ('am', 'happy', 'because'): 1}\n",
    "\n",
    "# get count for an n-gram tuple\n",
    "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
    "\n",
    "# check if n-gram is present in the dictionary\n",
    "if ('i', 'am', 'learning') in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
    "\n",
    "# update the count in the word count dictionary\n",
    "n_gram_counts[('i', 'am', 'learning')] = 1\n",
    "if ('i', 'am', 'learning') in n_gram_counts:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
    "else:\n",
    "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3777f8497f92ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'am', 'happy', 'because')\n"
     ]
    }
   ],
   "source": [
    "# concatenate tuple for prefix and tuple with the last word to create the n_gram\n",
    "prefix = ('i', 'am', 'happy')\n",
    "word = 'because'\n",
    "\n",
    "# note here the syntax for creating a tuple for a single word\n",
    "n_gram = prefix + (word,)\n",
    "print(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3dd8ad2864abd0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "def single_pass_trigram_count_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: Pre-processed and tokenized corpus. \n",
    "    \n",
    "    Returns:\n",
    "        bigrams: list of all bigram prefixes, row index\n",
    "        vocabulary: list of all found words, the column index\n",
    "        count_matrix: pandas dataframe with bigram prefixes as rows, \n",
    "                      vocabulary words as columns \n",
    "                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    vocabulary = []\n",
    "    count_matrix_dict = defaultdict(dict)\n",
    "    \n",
    "    # go through the corpus once with a sliding window\n",
    "    for i in range(len(corpus) - 3 + 1):\n",
    "        # the sliding window starts at position i and contains 3 words\n",
    "        trigram = tuple(corpus[i : i + 3])\n",
    "        \n",
    "        bigram = trigram[0 : -1]\n",
    "        if not bigram in bigrams:\n",
    "            bigrams.append(bigram)        \n",
    "        \n",
    "        last_word = trigram[-1]\n",
    "        if not last_word in vocabulary:\n",
    "            vocabulary.append(last_word)\n",
    "        \n",
    "        if (bigram,last_word) not in count_matrix_dict:\n",
    "            count_matrix_dict[bigram,last_word] = 0\n",
    "            \n",
    "        count_matrix_dict[bigram,last_word] += 1\n",
    "    \n",
    "    # convert the count_matrix to np.array to fill in the blanks\n",
    "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
    "    for trigram_key, trigam_count in count_matrix_dict.items():\n",
    "        count_matrix[bigrams.index(trigram_key[0]), \\\n",
    "                     vocabulary.index(trigram_key[1])]\\\n",
    "        = trigam_count\n",
    "    \n",
    "    # np.array to pandas dataframe conversion\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
    "    return bigrams, vocabulary, count_matrix\n",
    "\n",
    "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
    "\n",
    "print(count_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e295d24f559395cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  happy  because    i   am  learning    .\n",
      "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
      "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
      "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
      "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
      "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# create the probability matrix from the count matrix\n",
    "row_sums = count_matrix.sum(axis=1)\n",
    "# divide each row by its sum\n",
    "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
    "\n",
    "print(prob_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "16c16ac5cbef29fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: ('i', 'am')\n",
      "word: happy\n",
      "trigram_probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "# find the probability of a trigram in the probability matrix\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the prefix bigram \n",
    "bigram = trigram[:-1]\n",
    "print(f'bigram: {bigram}')\n",
    "\n",
    "# find the last word of the trigram\n",
    "word = trigram[-1]\n",
    "print(f'word: {word}')\n",
    "\n",
    "# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\n",
    "trigram_probability = prob_matrix[word][bigram]\n",
    "print(f'trigram_probability: {trigram_probability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8d644ffec1273fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in vocabulary starting with prefix: ha\n",
      "\n",
      "happy\n",
      "have\n"
     ]
    }
   ],
   "source": [
    "# lists all words in vocabulary starting with a given prefix\n",
    "vocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\n",
    "starts_with = 'ha'\n",
    "\n",
    "print(f'words in vocabulary starting with prefix: {starts_with}\\n')\n",
    "for word in vocabulary:\n",
    "    if word.startswith(starts_with):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c2dfcc9520d6708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 80/10/10:\n",
      " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
      " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
      " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
      "\n",
      "split 98/1/1:\n",
      " train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n",
      " validation data:[35]\n",
      " test data:[75]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we only need train and validation %, test is the remainder\n",
    "def train_validation_test_split(data, train_percent, validation_percent):\n",
    "    \"\"\"\n",
    "    Splits the input data to  train/validation/test according to the percentage provided\n",
    "    \n",
    "    Args:\n",
    "        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n",
    "        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n",
    "        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n",
    "        \n",
    "        Note: train_percent + validation_percent need to be <=100\n",
    "              the reminder to 100 is allocated for the test set\n",
    "    \n",
    "    Returns:\n",
    "        train_data: list of sentences, the training part of the corpus\n",
    "        validation_data: list of sentences, the validation part of the corpus\n",
    "        test_data: list of sentences, the test part of the corpus\n",
    "    \"\"\"\n",
    "    # fixed seed here for reproducibility\n",
    "    random.seed(87)\n",
    "    \n",
    "    # reshuffle all input sentences\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_percent / 100)\n",
    "    train_data = data[0:train_size]\n",
    "    \n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "    \n",
    "    test_data = data[train_size + validation_size:]\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "data = [x for x in range (0, 100)]\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\n",
    "print(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
    "      f\"test data:{test_data}\\n\")\n",
    "\n",
    "train_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\n",
    "print(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
    "      f\"test data:{test_data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b8a0fb81c504",
   "metadata": {},
   "source": [
    "#### Perplexity\n",
    "\n",
    "In order to implement the perplexity formula, you'll need to know how to implement m-th order root of a variable.\n",
    "\n",
    "\\begin{equation*}\n",
    "PP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
    "\\end{equation*}\n",
    "\n",
    "Remember from calculus:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here is a code that will help you with the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2d427e2a50fa177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316.22776601683796\n"
     ]
    }
   ],
   "source": [
    "# to calculate the exponent, use the following syntax\n",
    "p = 10 ** (-250)\n",
    "M = 100\n",
    "perplexity = p ** (-1 / M)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15760d15c3df3e",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196d8b97eb7d0f8",
   "metadata": {},
   "source": [
    "Data je nutno rozdelit jako vzdy na train/val/test set. Na velikosti corpusu zalezi, jak velky bude train a val set.\n",
    "Na malem corpusu jsou pomery zhruba 80/10/10, na velkem 98/1/1. Dale je mozne vybirat bud casti delsiho textu do \n",
    "ruznych mnozin, nebo mame-li kratsi useky, tak ty davat do mnozin primo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76193c347cd61e92",
   "metadata": {},
   "source": [
    "A jak vyhodnotit, zda je model dobre natrenovany? To se da udelat pomoci perplexity. Perplexity je metrika, ktera\n",
    "meri, jak dobre model predikuje data. Cim nizsi perplexity, tim lepsi model. Perplexity se pocita jako:\n",
    "$$PP(W) = P(s_1, s_2, ..., s_m)^{-\\frac{1}{m}}$$\n",
    "kde $s_1, s_2, ..., s_N$ je veta o delce N slov. Tj. perplexity je inverzni hodnota geometrickeho prumeru\n",
    "pravdepodobnosti vyskytu slov ve vete. \n",
    "$$ PP(W) = \\sqrt[m]{\\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)}|w_{j-1}^Ã­(i))}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d170fd43c7ae4d",
   "metadata": {},
   "source": [
    "Kdyz spojime vsechny vety v test mnozine do jedne dlouhe vety, pak perplexity je \n",
    "$$PP(W) = \\sqrt[m]{\\prod_{i=1}^{m} \\frac{1}{P(w_i|w_{i-1})}}$$\n",
    "A pro lepsi numerickou stabilitu opet pouzijeme logaritmus:\n",
    "$$log(PP(W)) = \\frac{1}{m} \\sum_{i=1}^{m} log(\\frac{1}{P(w_i|w_{i-1})})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db105c15ec0a276f",
   "metadata": {},
   "source": [
    "### Out of Vocabulary Words\n",
    "\n",
    "Ulohy muzeme rozdelit na ty s \"uzavrenym slovnikem\" a na ty s \"otevrenym slovnikem\". Uzavreny slovnik znamena, ze \n",
    "nemuzeme potkat slova, ktera nejsou ve slovniku. A otevreny slovnik logicky naopak. Uzavreny slovnik je napriklad\n",
    "automaticke opravovani textu, kde se kontroluje, zda slovo je ve slovniku. Otevreny slovnik je napriklad automaticke\n",
    "dokoncovani textu, kde se predikuje slovo, ktere neni ve slovniku.\n",
    "\n",
    "Proto v pripade, kdy pracujeme s otevrenym slovnikem postupujeme nasledovne: \n",
    "- vytvorime slovnik V, ktery obsahuje vsechna slova v trenovacim korpusu\n",
    "- nahradime vsechna slova, ktera jsou v korpusu, ale nejsou ve slovniku V, specialnim tokenem \\<UNK\\>\n",
    "- vypocitame pravdepodobnosti prechodu jako obvykle s novym tokenem \\<UNK\\> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a598a243261f85",
   "metadata": {},
   "source": [
    "Aby slovnik nebyl tak velky, tak se slova s nizkou frekvenci nahrazuji specialnim tokenem \\<UNK\\>. Napriklad, s \n",
    "min_freqency = 2 se nahrazi slova, ktera se vyskytuji mene nez dvakrat. Bezna kriteria pro vyber slovniku jsou\n",
    "- min_freqency: minimalni frekvence slova\n",
    "- max_vocab_size: maximalni velikost slovniku\n",
    "- unknown_token: token pro nezname slovo by melo byt pouzivano vyjimecne. Jinak by to vedlo k tomu, ze ten token bude\n",
    " pouzivan casto \"jako zolik\" a model bude mit nizkou perplexitu, i kdyz vlastne bude prd predikovat\n",
    "- perplexity metriku je treba pouzivat jen na LM modelech se stejnym slovnikem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b30ea032ade8d",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "V pripade, ze pro nekterou kombinaci slov neexistuje bigram ve slovniku, tak by byla pravdepodobnost nulova. Tri \n",
    "bezne metody pro reseni tohoto problemu jsou\n",
    "- smoothing\n",
    "- backoff\n",
    "- interpolation\n",
    "\n",
    "#### Smoothing\n",
    "Smoothing je metoda, ktera prida k vsem vyskytum slov v korpusu nejakou konstantu. Napriklad, Laplace smoothing prida k\n",
    "vsem vyskytum slov konstantu 1. Tj. vzorec pro pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |V|}$$\n",
    "pro obecne $k$ vzorec vypada:\n",
    "$$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + k|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7822abf3f40b4bd",
   "metadata": {},
   "source": [
    "#### Backoff\n",
    "Backoff je metoda, ktera se snazi najit nejlepsi model pro predikci. Pokud neexistuje n-gram, tak se pouzije (n-1)\n",
    "-gram.  Pokud neexistuje ani (n-1)-gram, tak se pouzije (n-2)-gram, atd. Nicmene, zkracovani n-gramu vede k diztorzi \n",
    "rozdeleni pravdepodobnosti (zvlaste u mensich korpusu) a proto je treba nizsi rady n-gramu diskontovat.  Tj. vzorec \n",
    "pro pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}^{n-1}) = \\begin{cases} P(w_i|w_{i-1}^{n-1}) & \\text{if } C(w_{i-1}^n) > 0 \\\\ \\alpha(w_{i-1}^{n-1})P(w_i|w_{i-1}^{n-2}) & \\text{if } C(w_{i-1}^n) = 0 \\end{cases}$$\n",
    "kde $\\alpha(w_{i-1}^{n-1})$ je normalizacni konstanta, ktera zajisti, ze pravdepodobnosti budou soucet jedna. Viz \n",
    "Katzova metoda: https://en.wikipedia.org/wiki/Katz's_back-off_model\n",
    "\n",
    "\"Stupid backoff\" je jednodussi verze backoff, ktera pouziva konstantu $\\alpha = 0.4$. Tj. vzorec pro pravdepodobnost\n",
    "prechodu je:\n",
    "$$P(w_i|w_{i-1}) = \\begin{cases} P(w_i|w_{i-1}) & \\text{if } C(w_{i-1}, w_i) > 0 \\\\ 0.4P(w_i) & \\text{if } C(w_{i-1}, w_i) = 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b979966b5c8b5e",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "Interpolace je metoda, ktera kombinuje pravdepodobnosti vsech n-gramu nizsich radu az po n-ty rad. Tj. vzorec pro \n",
    "pravdepodobnost prechodu je:\n",
    "$$P(w_i|w_{i-1}^{n-1}) = \\lambda_1P(w_i|w_{i-1}^{n-1}) + \\lambda_2P(w_i|w_{i-1}^{n-2}) + ... + \\lambda_nP(w_i)$$\n",
    "kde $\\lambda_i$ je vahovaci faktor, ktery zajisti, ze pravdepodobnosti budou soucet jedna. \n",
    "$$ \\sum_{i=1}^{n} \\lambda_i = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbfc1defd2ed31",
   "metadata": {},
   "source": [
    "V pripade, ze chybi nejaky n-gram, tak se pouzije nizsi rada n-gramu. Nestane se tak, ze by pravdepodobnost byla \n",
    "nulova, prestoze nejvyssi rad neexistuje, ale nizsi rady n-gramu prispivaji k odhadu pravdepodobnosti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb4389acc6566f",
   "metadata": {},
   "source": [
    "### LAB: Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6e784081eeba6c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary from M most frequent words\n",
    "# use Counter object from the collections library to find M most common words\n",
    "from collections import Counter\n",
    "\n",
    "# the target size of the vocabulary\n",
    "M = 3\n",
    "\n",
    "# pre-calculated word counts\n",
    "# Counter could be used to build this dictionary from the source corpus\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
    "\n",
    "vocabulary = Counter(word_counts).most_common(M)\n",
    "\n",
    "# remove the frequencies and leave just the words\n",
    "vocabulary = [w[0] for w in vocabulary]\n",
    "\n",
    "print(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "92df365a9588b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: ['am', 'i', 'learning']\n",
      "output sentence: ['<UNK>', '<UNK>', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\n",
    "sentence = ['am', 'i', 'learning']\n",
    "output_sentence = []\n",
    "print(f\"input sentence: {sentence}\")\n",
    "\n",
    "for w in sentence:\n",
    "    # test if word w is in vocabulary\n",
    "    if w in vocabulary:\n",
    "        output_sentence.append(w)\n",
    "    else:\n",
    "        output_sentence.append('<UNK>')\n",
    "        \n",
    "print(f\"output sentence: {output_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a5931fc8d8e91fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# iterate through all word counts and print words with given frequency f\n",
    "f = 3\n",
    "\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
    "\n",
    "for word, freq in word_counts.items():\n",
    "    if freq == f:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "841ea64a5223ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity for the training set: 1.2599210498948732\n",
      "perplexity for the training set with <UNK>: 1.0\n"
     ]
    }
   ],
   "source": [
    "# many <unk> low perplexity \n",
    "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
    "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
    "\n",
    "test_set = ['i', 'am', 'learning']\n",
    "test_set_unk = ['i', 'am', '<UNK>']\n",
    "\n",
    "M = len(test_set)\n",
    "probability = 1\n",
    "probability_unk = 1\n",
    "\n",
    "# pre-calculated probabilities\n",
    "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
    "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
    "\n",
    "# got through the test set and calculate its bigram probability\n",
    "for i in range(len(test_set) - 2 + 1):\n",
    "    bigram = tuple(test_set[i: i + 2])\n",
    "    probability = probability * bigram_probabilities[bigram]\n",
    "        \n",
    "    bigram_unk = tuple(test_set_unk[i: i + 2])\n",
    "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
    "\n",
    "# calculate perplexity for both original test set and test set with <UNK>\n",
    "perplexity = probability ** (-1 / M)\n",
    "perplexity_unk = probability_unk ** (-1 / M)\n",
    "\n",
    "print(f\"perplexity for the training set: {perplexity}\")\n",
    "print(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "374280e4cb10b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability_known_trigram: 0.2\n",
      "probability_unknown_trigram: 0.2\n"
     ]
    }
   ],
   "source": [
    "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    numerator = n_gram_count + k\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    return numerator / denominator\n",
    "\n",
    "trigram_probabilities = {('i', 'am', 'happy') : 2}\n",
    "bigram_probabilities = {( 'i', 'am') : 10}\n",
    "vocabulary_size = 5\n",
    "k = 1\n",
    "\n",
    "probability_known_trigram = add_k_smoothing_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n",
    "                           bigram_probabilities[( 'i', 'am')])\n",
    "\n",
    "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
    "\n",
    "print(f\"probability_known_trigram: {probability_known_trigram}\")\n",
    "print(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e83a1a37c6612935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') not found\n",
      "probability for bigram ('you', 'happy') not found\n",
      "probability for unigram happy found\n",
      "\n",
      "probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('are', 'you', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# search for first non-zero probability starting with trigram\n",
    "# to generalize this for any order of n-gram hierarchy, \n",
    "# you could loop through the probability dictionaries instead of if/else cascade\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
    "    print(f\"probability for trigram {trigram} not found\")\n",
    "    \n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
    "        print(f\"probability for bigram {bigram} not found\")\n",
    "        \n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"probability for unigram {unigram} found\\n\")\n",
    "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a3c318c9e7fbd244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
      "\n",
      "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# the weights come from optimization on a validation set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n",
    "+ lambda_2 * bigram_probabilities[bigram]\n",
    "+ lambda_3 * unigram_probabilities[unigram]\n",
    "\n",
    "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4321e47b78d4b",
   "metadata": {},
   "source": [
    "### LAB: Assignment - N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20f0dd83023282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmateju/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfbcb3086266cb",
   "metadata": {},
   "source": [
    "#### 1 - Load and Preprocess Data\n",
    "\n",
    "<a name='1.1'></a>\n",
    "#### 1.1 - Load the Data\n",
    "You will use twitter data.\n",
    "Load the data and view the first few sentences by running the next cell.\n",
    "\n",
    "Notice that data is a long string that contains many many tweets.\n",
    "Observe that there is a line break \"\\n\" between tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ed9fea9b9fd41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...â€œ: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Familyâ€\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"../pomocne_soubory/en_US.twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6a9b8b7faca76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06c96d22b1535e8",
   "metadata": {},
   "source": [
    "1.2 - Pre-process the Data\n",
    "\n",
    "Preprocess this data with the following steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter.\n",
    "1. Split each sentence into tokens. Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
    "1. Assign sentences into train or test sets.\n",
    "1. Find tokens that appear at least N times in the training data.\n",
    "1. Replace tokens that appear less than N times by `<unk>`\n",
    "\n",
    "\n",
    "Note: we omit validation data in this exercise.\n",
    "- In real applications, we should hold a part of data as a validation set and use it to tune our training.\n",
    "- We skip this process for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679509eeb965047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split(\"\\n\")\n",
    "\n",
    "    # Additional clearning \n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592aceb7fa4eefed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have a pen.\n",
      "I have an apple. \n",
      "Ah\n",
      "Apple pen.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "x = \"\"\"\n",
    "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
    "\"\"\"\n",
    "print(x)\n",
    "\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6711a3b10f8298b",
   "metadata": {},
   "source": [
    "Exercise 2 - tokenize_sentences\n",
    "\n",
    "The next step is to tokenize sentences (split a sentence into a list of words). \n",
    "- Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.\n",
    "- Append each tokenized list of words into a list of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9cb9ab3509ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of lists of tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    # Go through each sentence\n",
    "    for sentence in sentences: # complete this line\n",
    "        \n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Convert into a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de1ba75c8fc5caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green', '.'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6038ae735ca782",
   "metadata": {},
   "source": [
    "Exercise 3 - get_tokenized_data\n",
    "\n",
    "Use the two functions that you have just implemented to get the tokenized data.\n",
    "- split the data into sentences\n",
    "- tokenize those sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca5251cb1a1cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \"\"\"\n",
    "    Make a list of tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        data: String\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    # Get the sentences by splitting up the data\n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    # Get the list of lists of tokens by tokenizing the sentences\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fca4f26f0e8c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'blue', '.'],\n",
       " ['leaves', 'are', 'green'],\n",
       " ['roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your function\n",
    "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9dabd813669ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c1954838ab60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47961 data are split into 38368 train and 9593 test set\n",
      "First training sample:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
      "First test sample\n",
      "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8100be11bfbca",
   "metadata": {},
   "source": [
    "Exercise 4 - count_words\n",
    "\n",
    "You won't use all the tokens (words) appearing in the data for training.  Instead, you will use the more frequently used words.  \n",
    "- You will focus on the words that appear at least N times in the data.\n",
    "- First count how many times each word appears in the data.\n",
    "\n",
    "You will need a double for-loop, one for sentences and the other for tokens within a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b690d267e13477c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "    \n",
    "    Returns:\n",
    "        dict that maps word (str) to the frequency (int)\n",
    "    \"\"\"\n",
    "            \n",
    "    word_counts = {}\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "        # Go through each token in the sentence\n",
    "        for token in sentence:\n",
    "            # If the token is not in the dictionary yet, set the count to 1\n",
    "            if token not in word_counts:\n",
    "                word_counts[token] = 1\n",
    "            # If the token is already in the dictionary, increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6912994ea37038cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 3,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6149ef8f5f77b81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a468e45846611c2",
   "metadata": {},
   "source": [
    "Handling 'Out of Vocabulary' words\n",
    "\n",
    "If your model is performing autocomplete, but encounters a word that it never saw during training, it won't have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. \n",
    "- This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words.\n",
    "- The percentage of unknown words in the test set is called the <b> OOV </b> rate. \n",
    "\n",
    "To handle unknown words during prediction, use a special token to represent all unknown words 'unk'. \n",
    "- Modify the training data so that it has some 'unknown' words to train on.\n",
    "- Words to convert into \"unknown\" words are those that do not occur very frequently in the training set.\n",
    "- Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. \n",
    "- Convert all the other words that are not part of the closed vocabulary to the token 'unk'. \n",
    "\n",
    "Exercise 5 - get_words_with_nplus_frequency\n",
    "\n",
    "You will now create a function that takes in a text document and a threshold `count_threshold`.\n",
    "- Any word whose count is greater than or equal to the threshold `count_threshold` is kept in the closed vocabulary.\n",
    "- Returns the word closed vocabulary list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4042b5b344842c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to contain the words that\n",
    "    # appear at least 'minimum_freq' times.\n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word couts of the tokenized sentences\n",
    "    # Use the function that you defined earlier to count the words\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "#   filtered_words = None\n",
    "\n",
    "    # for each word and its count\n",
    "    for word, cnt in word_counts.items():\n",
    "    # Check that the word's count is at least as great as the minimum count\n",
    "        if cnt >= count_threshold:\n",
    "            # Append the word to the list\n",
    "            closed_vocab.append(word)\n",
    "            \n",
    "    # ekvivalentni: closed_vocab = [word for word, cnt in word_counts.items() if cnt >= count_threshold]           \n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327e4811a0ef4e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fe09c55a21359",
   "metadata": {},
   "source": [
    "Exercise 6 - replace_oov_words_by_unk\n",
    "\n",
    "The words that appear `count_threshold` times or more are in the closed vocabulary. \n",
    "- All other words are regarded as `unknown`.\n",
    "- Replace words not in the closed vocabulary with the token `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82d4a14c781c2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    # Place vocabulary into a set for faster search\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    # Initialize a list that will hold the sentences\n",
    "    # after less frequent words are replaced by the unknown token\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Initialize the list that will contain\n",
    "        # a single sentence with \"unknown_token\" replacements\n",
    "        replaced_sentence = []\n",
    "       \n",
    "        # for each token in the sentence\n",
    "        for token in sentence:\n",
    "            # Check if the token is in the closed vocabulary\n",
    "            if token in vocabulary:\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # Otherwise, append the unknown token instead\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        \n",
    "        # Append the list of tokens to the list of lists\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05a47ea5982bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "[['dogs', 'run'], ['cats', 'sleep']]\n",
      "tokenized_sentences with less frequent words converted to '<unk>':\n",
      "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "vocabulary = [\"dogs\", \"sleep\"]\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
    "print(f\"Original sentence:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8e17d7146befb",
   "metadata": {},
   "source": [
    "Exercise 7 - preprocess_data\n",
    "\n",
    "Now we are ready to process our data by combining the functions that you just implemented.\n",
    "\n",
    "1. Find tokens that appear at least count_threshold times in the training data.\n",
    "1. Replace tokens that appear less than count_threshold times by \"<unk\\>\" both for training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f41303cb8ff4b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\", get_words_with_nplus_frequency=get_words_with_nplus_frequency, replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: Words whose count is less than this are \n",
    "                      treated as unknown.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "\n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
    "\n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dde4766501231c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
    "                                                           tmp_test, \n",
    "                                                           count_threshold = 1\n",
    "                                                          )\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "421deeef71df239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fa09124ff4f1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
      "\n",
      "First preprocessed test sample:\n",
      "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n",
      "\n",
      "Size of vocabulary: 14823\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765426b8388c04c",
   "metadata": {},
   "source": [
    "#### 2 - Develop n-gram based Language Models\n",
    "\n",
    "In this section, you will develop the n-grams language model.\n",
    "- Assume the probability of the next word depends only on the previous n-gram.\n",
    "- The previous n-gram is the series of the previous 'n' words.\n",
    "\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-n}\\cdots w_{t-2}, w_{t-1}$ is:\n",
    "\n",
    "$$ P(w_t | w_{t-n}\\dots w_{t-1} ) \\tag{1}$$\n",
    "\n",
    "You can estimate this probability  by counting the occurrences of these series of words in the training data.\n",
    "- The probability can be estimated as a ratio, where\n",
    "- The numerator is the number of times word 't' appears after words t-n through t-1 appear in the training data.\n",
    "- The denominator is the number of times word t-n through t-1 appears in the training data.\n",
    "\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t)}{C(w_{t-n}\\dots w_{t-1})} \\tag{2} $$\n",
    "\n",
    "\n",
    "- The function $C(\\cdots)$ denotes the number of occurence of the given sequence. \n",
    "- $\\hat{P}$ means the estimation of $P$. \n",
    "- Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.\n",
    "\n",
    "Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.\n",
    "\n",
    "The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36c091e4ed8e2c",
   "metadata": {},
   "source": [
    "Exercise 8 - count_n_grams\n",
    "Next, you will implement a function that computes the counts of n-grams for an arbitrary number $n$.\n",
    "\n",
    "When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers \"<s\\>\" to indicate the beginning of the sentence.  \n",
    "- For example, in the tri-gram model (n=3), a sequence with two start tokens \"<s\\>\" should predict the first word of a sentence.\n",
    "- So, if the sentence is \"I like food\", modify it to be \"<s\\> <s\\> I like food\".\n",
    "- Also prepare the sentence for counting by appending an end token \"<e\\>\" so that the model can predict when to finish a sentence.\n",
    "\n",
    "Technical note: In this implementation, you will store the counts as a dictionary.\n",
    "- The key of each key-value pair in the dictionary is a **tuple** of n words (and not a list)\n",
    "- The value in the key-value pair is the number of occurrences.  \n",
    "- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is \"immutable\", so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.\n",
    "- Although for a n-gram you need to use n-1 starting markers for a sentence, you will want to prepend n starting markers in order to use them to compute the initial probability for the (n+1)-gram later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9edaf53daf72b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    # Go through each sentence in the data\n",
    "    for sentence in data:\n",
    "        # Prepend start token n times, and append the end token one time\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # Convert list to tuple\n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use 'i' to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence.\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            # Check if the n-gram is in the dictionary\n",
    "            if n_gram in n_grams:\n",
    "                # Increment the count for this n-gram\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                # Initialize this n-gram count to 1\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f7d08cc4f13c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb08f07508cca8b",
   "metadata": {},
   "source": [
    "Exercise 9 - estimate_probability\n",
    "\n",
    "Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t)}{C(w_{t-n}\\dots w_{t-1})} \\tag{2} $$\n",
    "\n",
    "This formula doesn't work when a count of an n-gram is zero..\n",
    "- Suppose we encounter an n-gram that did not occur in the training data.  \n",
    "- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\n",
    "\n",
    "A way to handle zero counts is to add k-smoothing.  \n",
    "- K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-n} \\dots w_{t-1}) = \\frac{C(w_{t-n}\\dots w_{t-1}, w_t) + k}{C(w_{t-n}\\dots w_{t-1}) + k|V|} \\tag{3} $$\n",
    "\n",
    "\n",
    "For n-grams that have a zero count, the equation (3) becomes $\\frac{1}{|V|}$.\n",
    "- This means that any n-gram with zero count has the same probability of $\\frac{1}{|V|}$.\n",
    "\n",
    "Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.\n",
    "\n",
    "- The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.\n",
    "- The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37fc284aef009a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    # Set the denominator\n",
    "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
    "    # Get its count. Otherwise set the count to zero\n",
    "    # Use the dictionary that has counts for n-grams\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "\n",
    "    # Calculate the denominator using the count of the previous n gram\n",
    "    # and apply k-smoothing\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    # Set the count to the count in the dictionary,\n",
    "    # otherwise 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "\n",
    "    # Define the numerator use the count of the n-gram plus current word,\n",
    "    # and apply smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    # Calculate the probability as the numerator divided by denominator\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5984e411b21b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", [\"a\"], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67c31066934187",
   "metadata": {},
   "source": [
    "Estimate probabilities for all words\n",
    "\n",
    "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.\n",
    "- This function is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2435d123ea0a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)    \n",
    "    \n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
    "    vocabulary_size = len(vocabulary)    \n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "                \n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f8536bd06703758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'this': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "732576f48b06656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0.18181818181818182,\n",
       " 'is': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'cat': 0.09090909090909091,\n",
       " 'this': 0.18181818181818182,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional test\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add98cabd67e2884",
   "metadata": {},
   "source": [
    "Count and probability matrices\n",
    "\n",
    "As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  \n",
    "- It can be more intuitive to present them as count or probability matrices.\n",
    "- The functions defined in the next cells return count or probability matrices.\n",
    "- This function is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47332693270cd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is omitted since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]        \n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}    \n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9fb75d15d268edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           i   is    a  cat  this  dog  like  <e>  <unk>\n",
       "(i,)     0.0  0.0  0.0  0.0   0.0  0.0   1.0  0.0    0.0\n",
       "(cat,)   0.0  0.0  0.0  0.0   0.0  0.0   0.0  2.0    0.0\n",
       "(is,)    0.0  0.0  0.0  0.0   0.0  0.0   1.0  0.0    0.0\n",
       "(a,)     0.0  0.0  0.0  2.0   0.0  0.0   0.0  0.0    0.0\n",
       "(<s>,)   1.0  0.0  0.0  0.0   1.0  0.0   0.0  0.0    0.0\n",
       "(dog,)   0.0  1.0  0.0  0.0   0.0  0.0   0.0  0.0    0.0\n",
       "(this,)  0.0  0.0  0.0  0.0   0.0  1.0   0.0  0.0    0.0\n",
       "(like,)  0.0  0.0  2.0  0.0   0.0  0.0   0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cf769af4fbc83d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               i   is    a  cat  this  dog  like  <e>  <unk>\n",
       "(i, like)    0.0  0.0  1.0  0.0   0.0  0.0   0.0  0.0    0.0\n",
       "(<s>, this)  0.0  0.0  0.0  0.0   0.0  1.0   0.0  0.0    0.0\n",
       "(dog, is)    0.0  0.0  0.0  0.0   0.0  0.0   1.0  0.0    0.0\n",
       "(is, like)   0.0  0.0  1.0  0.0   0.0  0.0   0.0  0.0    0.0\n",
       "(<s>, <s>)   1.0  0.0  0.0  0.0   1.0  0.0   0.0  0.0    0.0\n",
       "(a, cat)     0.0  0.0  0.0  0.0   0.0  0.0   0.0  2.0    0.0\n",
       "(<s>, i)     0.0  0.0  0.0  0.0   0.0  0.0   1.0  0.0    0.0\n",
       "(this, dog)  0.0  1.0  0.0  0.0   0.0  0.0   0.0  0.0    0.0\n",
       "(like, a)    0.0  0.0  0.0  2.0   0.0  0.0   0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show trigram counts\n",
    "print('\\ntrigram counts')\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b8cbb7b521e2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343308dada0b7abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                i        is         a       cat      this       dog      like  \\\n",
       "(i,)     0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(is,)    0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(a,)     0.090909  0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
       "(<s>,)   0.181818  0.090909  0.090909  0.090909  0.181818  0.090909  0.090909   \n",
       "(dog,)   0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(this,)  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(like,)  0.090909  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(i,)     0.100000  0.100000  \n",
       "(cat,)   0.272727  0.090909  \n",
       "(is,)    0.100000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(this,)  0.100000  0.100000  \n",
       "(like,)  0.090909  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2722f698ebb3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>cat</th>\n",
       "      <th>this</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    i        is         a       cat      this       dog  \\\n",
       "(i, like)    0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(<s>, this)  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(dog, is)    0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(is, like)   0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(<s>, <s>)   0.181818  0.090909  0.090909  0.090909  0.181818  0.090909   \n",
       "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(<s>, i)     0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(this, dog)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(like, a)    0.090909  0.090909  0.090909  0.272727  0.090909  0.090909   \n",
       "\n",
       "                 like       <e>     <unk>  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(<s>, this)  0.100000  0.100000  0.100000  \n",
       "(dog, is)    0.200000  0.100000  0.100000  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  \n",
       "(a, cat)     0.090909  0.272727  0.090909  \n",
       "(<s>, i)     0.200000  0.100000  0.100000  \n",
       "(this, dog)  0.100000  0.100000  0.100000  \n",
       "(like, a)    0.090909  0.090909  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"trigram probabilities\")\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a31b56bf8ad22",
   "metadata": {},
   "source": [
    "#### 3 - Perplexity\n",
    "\n",
    "In this section, you will generate the perplexity score to evaluate your model on the test set. \n",
    "- You will also use back-off when needed. \n",
    "- Perplexity is used as an evaluation metric of your language model. \n",
    "- To calculate the perplexity score of the test set on an n-gram model, use: \n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "- where $N$ is the length of the sentence.\n",
    "- $n$ is the number of words in the n-gram (e.g. 2 for a bigram).\n",
    "- In math, the numbering starts at one and not zero.\n",
    "\n",
    "In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:\n",
    "\n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be. \n",
    "- The more the n-grams tell us about the sentence, the lower the perplexity score will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886c86f0bfcc2dc",
   "metadata": {},
   "source": [
    "Exercise 10 - calculate_perplexity\n",
    "Compute the perplexity score given an N-gram count matrix and a sentence. \n",
    "\n",
    "**Note:** For the sake of simplicity, in the code below, `<s>` is included in perplexity score calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b00a900e86ba929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # prepend <s> and append <e>\n",
    "    sentence = [start_token] * n + sentence + [end_token]\n",
    "    \n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # length of sentence (after adding <s> and <e> tokens)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    # The variable p will hold the product\n",
    "    # that is calculated inside the n-root\n",
    "    # Update this in the code below\n",
    "    product_pi = 1.0\n",
    "\n",
    "    # Index t ranges from n to N - 1, inclusive on both ends\n",
    "    for t in range(n, N):\n",
    "        # get the n-gram preceding the word at position t\n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        # get the word at position t\n",
    "        word = sentence[t]\n",
    "        \n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        # using the n-gram counts, n-plus1-gram counts,\n",
    "        # vocabulary size, and smoothing constant\n",
    "        probability = estimate_probability(word, n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary_size, k=k)\n",
    "        \n",
    "        # Update the product of the probabilities\n",
    "        # This 'product_pi' is a cumulative product\n",
    "        # of the (1/P) factors that are calculated in the loop\n",
    "        product_pi *= (1 / probability)\n",
    "\n",
    "    # Take the Nth root of the product\n",
    "    perplexity = (product_pi) ** (1 / N)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Take the Nth root of the product\n",
    "    perplexity = (product_pi)**(1/N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb8b20757a24ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd63092d4f1fe0f",
   "metadata": {},
   "source": [
    "#### 4 - Build an Auto-complete System\n",
    "\n",
    "In this section, you will combine the language models developed so far to implement an auto-complete system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e853c52d8d03be0",
   "metadata": {},
   "source": [
    "Exercise 11 - suggest_a_word\n",
    "Compute probabilities for all possible next words and suggest the most likely one.\n",
    "- This function also take an optional argument `start_with`, which specifies the first few letters of the next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a41e3160c39f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C11 GRADED FUNCTION: suggest_a_word\n",
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length >= n \n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    \n",
    "    # append \"start token\" on \"previous_tokens\"\n",
    "    previous_tokens = ['<s>'] * n + previous_tokens\n",
    "    \n",
    "    # From the words that the user already typed\n",
    "    # get the most recent 'n' words as the previous n-gram\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabulary\n",
    "    # is the next word,\n",
    "    # given the previous n-gram, the dictionary of n-gram counts,\n",
    "    # the dictionary of n plus 1 gram counts, and the smoothing constant\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    # Initialize suggested word to None\n",
    "    # This will be set to the word with highest probability\n",
    "    suggestion = None\n",
    "    \n",
    "    # Initialize the highest word probability to 0\n",
    "    # this will be set to the highest probability \n",
    "    # of all words to be suggested\n",
    "    max_prob = 0\n",
    "    \n",
    "  # For each word and its probability in the probabilities dictionary:\n",
    "    for word, prob in probabilities.items():\n",
    "        # If the optional start_with string is set\n",
    "        if start_with is not None:\n",
    "            # Check if the beginning of word does not match with the letters in 'start_with'\n",
    "            if not word.startswith(start_with):\n",
    "                # if they don't match, skip this word (move onto the next word)\n",
    "                continue\n",
    "        \n",
    "        # Check if this word's probability is greater than the current maximum probability\n",
    "        if prob > max_prob:\n",
    "            # If so, save this word as the best suggestion (so far)\n",
    "            suggestion = word\n",
    "            # Save the new maximum probability\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f862f4fbfac08fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "# test your code when setting the starts_with\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d74afd95b10210",
   "metadata": {},
   "source": [
    "Get multiple suggestions\n",
    "\n",
    "The function defined below loop over varioud n-gram models to get multiple suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cc2cebea64b0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6831e3f915277d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727), ('a', 0.2), ('a', 0.2), ('a', 0.2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce6d76b87b642fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d08888b918d50187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be', 0.02766367370678687),\n",
       " ('have', 0.00013485267345425124),\n",
       " ('have', 0.00013488905375328792),\n",
       " ('i', 6.745362563237774e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74c7e6500b0facc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.014050206069689023),\n",
       " ('to', 0.004697320542507443),\n",
       " ('to', 0.0009423167530457024),\n",
       " ('to', 0.00040439441935701285)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "467442584a5eeb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.023424142254644932),\n",
       " ('you', 0.0035589578297072254),\n",
       " ('you', 0.00013489815189531904),\n",
       " ('i', 6.745362563237774e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd64db36974dc952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'re\", 0.0239720461563465),\n",
       " ('?', 0.002888086642599278),\n",
       " ('?', 0.001613228473482557),\n",
       " ('<e>', 0.00013489815189531904)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bda31bcd5df9ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('do', 0.00901999024865919),\n",
       " ('doing', 0.0016409583196586807),\n",
       " ('doing', 0.0004705249714324124),\n",
       " ('dvd', 6.744907594765952e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974d95c2062fc65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word embeddings with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcc47f92327582",
   "metadata": {},
   "source": [
    "Slova je mozno kodovat nekolika ruznymi zpusoby. Asi nejzakladnejsi, je je representovat jako cisla. Tj. mame-li slovnik\n",
    "s N slovy, pak kazde slovo je reprezentovano cislem od 1 do N. Tento zpusob je ne uplne stastny, protoze neni\n",
    "zrejme, jaky vztah maji jednotlive cisla k sobe. Proc by napriklad zebra mela byt na konci a kun uprostred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e071b283c21ef61",
   "metadata": {},
   "source": [
    "Dalsi zpusob je reprezentovat slova jako vektory. Takovym zpusobem je one-hot encoding, coz je pristup jako ke \n",
    "kategorickemu kodovani. Mame-li slovnik s N slovy, pak kazde slovo je reprezentovano vektorem o delce N, kde je jedna\n",
    "jednicka a vsechny ostatni hodnoty jsou nulove. Tento zpusob take neni uplne stastny, protoze vytvari velmi velke\n",
    "vektory. Navic je prace s takovymi vektory vypocetne narocna. A pritom nam nedava zadnou informaci o vztahu mezi slovy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96006106488c981",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/one_hot_enc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866674ac8f626a3",
   "metadata": {},
   "source": [
    "Dalsi zpusob je pouziti word embeddings, coz je opet vektorova\n",
    "reprezentace slov, ale o nizsi dimenzionalite. V podsate PCA pro slova, kde jednotlive dimenze nejsou maximalni \n",
    "variance, ale spise vztahy mezi slovy. Word embeddings se pouzivaji v mnoha NLP aplikacich, jako je napriklad sentiment analysis, machine \n",
    "translation, named entity recognition, part-of-speech tagging, automatic summarization, question answering, text \n",
    "classification, document clustering, a jine. Word embeddings se vytvari pomoci metod strojoveho uceni, jako je napriklad Word2Vec, GloVe, FastText, a jine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecd3d292ab4c34",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/word_embedding.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e72e97997d85e",
   "metadata": {},
   "source": [
    "K vytvoreni word embeddings, potrebujeme corpus textu, ktery pouzijeme k trenovani modelu a dale si musime zvolit \n",
    "nejakou embedovaci metodu. Word embedding vznika v podstate jako by-produkt pri reseni nejakeho ukolu strojoveho \n",
    "uceni. Tou ulohou je prave ta embedovaci medota. Napriklad, uloha muze byt predikce nasledujiciho slova v zavislosti na\n",
    "predchozich slovech. Nebo na slovech okolnich. \n",
    "\n",
    "Takovy ukol se nazyva \"self-supervised learning\". Znamena to, ze sice \n",
    "jako input mame neolablovany text, nicmene sama uloha je supervizovana a labely si vytvrari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543b8b5bc43d811",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/word_embedding_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8b9cf49af0374",
   "metadata": {},
   "source": [
    "Embeddingovych metod je nekolik:\n",
    "- Klasicke:\n",
    "    - Word2Vec\n",
    "    - Continous Bag Of Words (CBOW): the model learns to predict the center word given some context words.\n",
    "    - Continuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a\n",
    "     given input word.\n",
    "    - GloVe: factorizes the logarithm of the corpus's word co-occurrence matrix, similar to the count matrix youâ€™ve used \n",
    "    before.\n",
    "    - FastText: based on the skip-gram model and takes into account the structure of words by representing words as \n",
    "    an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n",
    "- Deep Learning:\n",
    "    - ELMo\n",
    "    - BERT\n",
    "    - GPT 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b93e476d5ced",
   "metadata": {},
   "source": [
    "### Continuous Bag Of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383e1eccdf4766b",
   "metadata": {},
   "source": [
    "Cilem CBOW modelu je predikovat slovo uprosted nejakeho okna v zavislosti na okolnich slovech. Tj. mame-li vetu \"I am \n",
    "happy because I am learning\", pak pro slovo \"happy\" by mohlo mit okolni slova \"I\", \"am\", \"because\", \"I\". CBOW \n",
    "model tak predikuje \"happy\" na zaklade techto slov. To, jak velke okno pouzijeme pro okolni kontextova slova, je \n",
    "hyperparametr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd7034d451e29e",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fad078e9ffaa9f",
   "metadata": {},
   "source": [
    "#### Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00959740b5825",
   "metadata": {},
   "source": [
    "- letter case : lower case\n",
    "- punctuation : remove / replace with space / replace with dot\n",
    "- numbers : remove / replace with special token <NUM>\n",
    "- special characters ($, â‚¬,ÃŸ, Ä…, ...): remove / replace with space\n",
    "- special words (:-), #nlp, ...): remove / replace with special token <URL>, <EMAIL>, <PHONE>, <CURRENCY>, <DATE>, \n",
    "<TIME>\n",
    "\n",
    "Pro nektery pripady je vhodne specialni slova, jako treba smiliky, ponechat, napriklad pro sentiment analysis, kde je \n",
    "dulezite, \n",
    "zda je v \n",
    "textu smilik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7b3da2f74603b",
   "metadata": {},
   "source": [
    "Pro cisteni textu a tokenizaci se pouzivaji knihovny jako je napriklad NLTK, Spacy, Gensim, a dalsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f2716711f0b027",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am happy because I am learning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[,!?;-]+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, corpus)\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(data)\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m [ ch\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01mif\u001b[39;00m ch\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mor\u001b[39;00m ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;129;01mor\u001b[39;00m emoji\u001b[38;5;241m.\u001b[39mget_emoji_regexp()\u001b[38;5;241m.\u001b[39msearch(ch) ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = \"I am happy because I am learning\"\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.'  or emoji.get_emoji_regexp().search(ch) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab976e990caa230a",
   "metadata": {},
   "source": [
    "#### Transforming Text to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e7250aff67e44",
   "metadata": {},
   "source": [
    "Jedna z moznosti je vzit vsechny slova, zakodovat je jako one-hot encoding a udelat jejich prumer. Tento prumer je\n",
    "potom reprezentaci contextovych slov. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed8bba7a221352",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/prumer_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566cf7fab1b91d8",
   "metadata": {},
   "source": [
    "A to slouzi jako vstup do shallow neural network, ktera predikuje slovo \n",
    "uprosted. Velikost embeddingu je velkost vnitrni skryte vrstvy teto site, tj. $N$. A je to hyperparametr. $m$ je batch_size, $V$ je vocabulary_size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfbcb9f6753d60",
   "metadata": {},
   "source": [
    "![One Hot Encoding](../pomocne_soubory/cbow_model_architect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86115ee6-db0c-4237-b66b-2212ce8bc6ea",
   "metadata": {},
   "source": [
    "Duvod proc to delame je by-product tenhle neuronky, tj. matice vah W_1 a W_2. To je to, co je onen embedding. Matice \n",
    "W_1 dobre popisuje kontextova slova a vytvari embed_dim representaci kontextu, matice W_2 zase dobre popisuje slovo \n",
    "uprostred. Bud se bere matice W_1 nebo W_2 jako embedding, nebo se bere prumer obou. Kazde ma sve pro a proti. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cfa81-918c-4691-928e-9d4175845307",
   "metadata": {},
   "source": [
    "Jak nasledne validovat, zda embedding funguje? Jsou dve skupiny evaluace: vnitrni a vnejsi. \n",
    "- Vnitrni evaluace se diva na analogie, clustering, visualizace, a dalsi. \n",
    "- Vnejsi evaluace se diva na konkretni ukoly, jako je naprikld sentiment analysis. Vyhoda je, ze rovnou testuje \n",
    "vyslednou aplikaci. Nevyhoda je, ze to trva dlouho a hure se to trouble-shootuje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d678a4d7fee3df8",
   "metadata": {},
   "source": [
    "### LAB: Word Embedding Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f912a6f664b9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmateju/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8369adcf1bd68",
   "metadata": {},
   "source": [
    "**DATA PREPARATION** \n",
    "\n",
    "In the data preparation phase, starting with a corpus of text, you will:\n",
    "\n",
    "- Clean and tokenize the corpus.\n",
    "\n",
    "- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n",
    "\n",
    "- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b16d0bf0c6d22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = 'Who â¤ï¸ \"word embeddings\" in 2020? I do!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b839a55eacafc47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who â¤ï¸ \"word embeddings\" in 2020? I do!!!\n",
      "After cleaning punctuation:  Who â¤ï¸ \"word embeddings\" in 2020. I do.\n"
     ]
    }
   ],
   "source": [
    "# Print original corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Do the substitution\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "\n",
    "# Print cleaned corpus\n",
    "print(f'After cleaning punctuation:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f0122e92815b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial string:  Who â¤ï¸ \"word embeddings\" in 2020. I do.\n",
      "After tokenization:  ['Who', 'â¤ï¸', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print cleaned corpus\n",
    "print(f'Initial string:  {data}')\n",
    "\n",
    "# Tokenize the cleaned corpus\n",
    "data = nltk.word_tokenize(data)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'After tokenization:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad05b0a889304c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial list of tokens:  ['who', 'â¤ï¸', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n",
      "After cleaning:  ['who', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenized version of the corpus\n",
    "print(f'Initial list of tokens:  {data}')\n",
    "\n",
    "# Filter tokenized corpus using list comprehension\n",
    "data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()\n",
    "         or ch == '.'\n",
    "         or not emoji.emoji_list(ch)\n",
    "       ]\n",
    "\n",
    "# Print the tokenized and filtered version of the corpus\n",
    "print(f'After cleaning:  {data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d39be3f4759148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'tokenize' function that will include the steps previously seen\n",
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or not emoji.emoji_list(ch)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec11f690ed276ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because IÂ am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# Define new corpus\n",
    "corpus = 'I am happy because IÂ am learning'\n",
    "\n",
    "# Print new corpus\n",
    "print(f'Corpus:  {corpus}')\n",
    "\n",
    "# Save tokenized version of corpus into 'words' variable\n",
    "words = tokenize(corpus)\n",
    "\n",
    "# Print the tokenized version of the corpus\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d25a3be3aff5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'your',\n",
       " 'turn',\n",
       " ':',\n",
       " 'try',\n",
       " 'with',\n",
       " 'your',\n",
       " 'own',\n",
       " 'sentence',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this with any sentence\n",
    "tokenize(\"Now it's your turn: try with your own sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e9cd6f08075e5",
   "metadata": {},
   "source": [
    "Now that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\n",
    "The `get_windows` function in the next cell was introduced in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55569f642daf769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'get_windows' function\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc17c6b475457d",
   "metadata": {},
   "source": [
    "The first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. Recall that for a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word.\n",
    "Here is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0349db201291036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d322b6ea6a9961d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', \"'s\"]\tit\n",
      "['it', 'your']\t's\n",
      "[\"'s\", 'turn']\tyour\n",
      "['your', ':']\tturn\n",
      "['turn', 'try']\t:\n",
      "[':', 'with']\ttry\n",
      "['try', 'your']\twith\n",
      "['with', 'own']\tyour\n",
      "['your', 'sentence']\town\n",
      "['own', '.']\tsentence\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
    "for x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb019b942793a5",
   "metadata": {},
   "source": [
    "To finish preparing the training set, you need to transform the context words and center words into vectors.\n",
    "The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n",
    "To create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, `get_dict`, that creates a Python dictionary that maps words to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9499f3f90124dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961838ecd60afb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'word2Ind' dictionary\n",
    "word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4108fe48a2fb97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'i':   3\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key 'i' within word2Ind dictionary\n",
    "print(\"Index of the word 'i':  \",word2Ind['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bc310a4d56f692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'Ind2word' dictionary\n",
    "Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "769519979efc4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word which has index 2:   happy\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key '2' within Ind2word dictionary\n",
    "print(\"Word which has index 2:  \",Ind2word[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2030774c67fde3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "# Save length of word2Ind dictionary into the 'V' variable\n",
    "V = len(word2Ind)\n",
    "\n",
    "# Print length of word2Ind dictionary\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235d18b203ea4ba",
   "metadata": {},
   "source": [
    "Getting one-hot word vectors\n",
    "\n",
    "Recall from the lecture that you can easily convert an integer, $n$, into a one-hot vector.\n",
    "\n",
    "Consider the word \"happy\". First, retrieve its numeric index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a5de135c4ed64ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save index of word 'happy' into the 'n' variable\n",
    "n = word2Ind['happy']\n",
    "\n",
    "# Print index of word 'happy'\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "563bf77fd859fd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vector with the same length as the vocabulary, filled with zeros\n",
    "center_word_vector = np.zeros(V)\n",
    "\n",
    "# Print vector\n",
    "center_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4acef06dc11882fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assert that the length of the vector is the same as the size of the vocabulary\n",
    "len(center_word_vector) == V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e52ccd585bdd13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace element number 'n' with a 1\n",
    "center_word_vector[n] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "751c8c70e51f7f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print vector\n",
    "center_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd55c78844a1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "918a7e1e4e215b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'word_to_one_hot_vector' function for word 'happy'\n",
    "word_to_one_hot_vector('happy', word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "669716f29e6f85f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'word_to_one_hot_vector' function for word 'learning'\n",
    "word_to_one_hot_vector('learning', word2Ind, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef036ffba955a18",
   "metadata": {},
   "source": [
    "Getting context word vectors\n",
    "\n",
    "To create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\n",
    "\n",
    "Let's start with a list of context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4c822866c6ec40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list containing context words\n",
    "context_words = ['i', 'am', 'because', 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51a644dba1586d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 1., 0.]),\n",
       " array([1., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot vectors for each context word using list comprehension\n",
    "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "\n",
    "# Print one-hot vectors for each context word\n",
    "context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "251bd4565460f195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute mean of the vectors using numpy\n",
    "np.mean(context_words_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c49e84b013303f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'context_words_to_vector' function that will include the steps previously seen\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a4186eea8e2a6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\n",
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad49acbecc06de1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\n",
    "context_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc2ce6a081d7d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print corpus\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdcbcb3fe9151271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus\n",
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f71fbbe61db3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator function 'get_training_example'\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ed2497323cd2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus using the generator function\n",
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967990ad4767d85",
   "metadata": {},
   "source": [
    "### LAB: Intro to CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de53a06a43f57d6",
   "metadata": {},
   "source": [
    "RELU:\n",
    "\n",
    "ReLU is used to calculate the values of the hidden layer, in the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
    " \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1246f657cc9ed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71320643],\n",
       "       [-4.79248051],\n",
       "       [ 1.33648235],\n",
       "       [ 2.48803883],\n",
       "       [-0.01492988]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a random seed so all random outcomes can be reproduced\n",
    "np.random.seed(10)\n",
    "\n",
    "# Define a 5X1 column vector using numpy\n",
    "z_1 = 10*np.random.rand(5, 1)-5\n",
    "\n",
    "# Print the vector\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3b37c74568684f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of vector and save it in the 'h' variable\n",
    "h = z_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc3c550086992cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which values met the criteria (this is possible because of vectorization)\n",
    "h < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54fb1cea16a3eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the array or vector. This is the same as applying ReLU to it\n",
    "h[h < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b1f6c1a47512f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71320643],\n",
       "       [0.        ],\n",
       "       [1.33648235],\n",
       "       [2.48803883],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the vector after ReLU\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc725f75f73f4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function that will include the steps previously seen\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b55c0baf84125de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "\n",
    "# Apply ReLU to it\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e511ac51124f57",
   "metadata": {},
   "source": [
    "SOFTMAX:\n",
    "\n",
    "The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n",
    "\n",
    "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n",
    "\n",
    "Let's work through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30175d7f7fba3ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9. ,  8. , 11. , 10. ,  8.5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([9, 8, 11, 10, 8.5])\n",
    "\n",
    "# Print the vector\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11c2c5bdc2ceb5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n",
       "        4914.7688403 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save exponentials of the values in a new vector\n",
    "e_z = np.exp(z)\n",
    "\n",
    "# Print the vector with the exponential values\n",
    "e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93d2644f20509441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97899.41826492078"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the sum of the exponentials\n",
    "sum_e_z = np.sum(e_z)\n",
    "\n",
    "# Print sum of exponentials\n",
    "sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7cd5f86bc31e720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08276947985173956"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print softmax value of the first element in the original vector\n",
    "e_z[0]/sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d7b6493ddb07076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'softmax' function that will include the steps previously seen\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecd4ba9cad9e22f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print softmax values for original vector\n",
    "softmax([9, 8, 11, 10, 8.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f65318d4cbe8d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assert that the sum of the softmax values is equal to 1\n",
    "np.sum(softmax([9, 8, 11, 10, 8.5])) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41c9c263658f036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n",
    "V = 5\n",
    "\n",
    "# Define vector of length V filled with zeros\n",
    "x_array = np.zeros(V)\n",
    "\n",
    "# Print vector\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5ce84eff4cc67cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print vector's shape\n",
    "x_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e64b605e97f73290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy vector\n",
    "x_column_vector = x_array.copy()\n",
    "\n",
    "# Reshape copy of vector\n",
    "x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n",
    "\n",
    "# Print vector\n",
    "x_column_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68295f99eb9bf492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print vector's shape\n",
    "x_column_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5051fa19685b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the word embedding vectors and save it in the variable 'N'\n",
    "N = 3\n",
    "\n",
    "# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\n",
    "V = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751756bbe229e7fa",
   "metadata": {},
   "source": [
    "Initializing weights and biases\n",
    "\n",
    "Before you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\n",
    "\n",
    "In the assignment you will implement a function to do this yourself using `numpy.random.rand`. In this notebook, we've pre-populated these matrices and vectors for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ca9c0eec590191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5dbd617a2df2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (5, 3) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W2.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "725b025f642e0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "# Define the 'get_windows' function as seen in a previous notebook\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Define the 'context_words_to_vector' function as seen in a previous notebook\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    return context_words_vectors\n",
    "\n",
    "# Define the generator function 'get_training_example' as seen in a previous notebook\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f5c076bbd985ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generator object in the 'training_examples' variable with the desired arguments\n",
    "training_examples = get_training_example(words, 2, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "381c478edd26cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first values from generator\n",
    "x_array, y_array = next(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95f74e4023f7fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print context words vector\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3630bc9f359840b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print one hot vector of center word\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d56c6ce08075bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y:\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Copy vector\n",
    "x = x_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "x.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'x:\\n{x}\\n')\n",
    "\n",
    "# Copy vector\n",
    "y = y_array.copy()\n",
    "\n",
    "# Reshape it\n",
    "y.shape = (V, 1)\n",
    "\n",
    "# Print it\n",
    "print(f'y:\\n{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dd969c654a70bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function as seen in the previous lecture notebook\n",
    "def relu(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "# Define the 'softmax' function as seen in the previous lecture notebook\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec1cf5edf74fc46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute z1 (values of first hidden layer before applying the ReLU function)\n",
    "z1 = np.dot(W1, x) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed5dbb654882e410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36483875],\n",
       "       [ 0.63710329],\n",
       "       [-0.3236647 ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print z1\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca2b7a38ea3c78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36483875],\n",
       "       [0.63710329],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute h (z1 after applying ReLU function)\n",
    "h = relu(z1)\n",
    "\n",
    "# Print h\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b466e6a0d2414",
   "metadata": {},
   "source": [
    "Values of the output layer\n",
    "\n",
    "Here are the formulas you need to calculate the values of the output layer, represented by the vector $\\mathbf{\\hat y}$:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "**First, calculate $\\mathbf{z_2}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7797587851022ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31973737],\n",
       "       [-0.28125477],\n",
       "       [-0.09838369],\n",
       "       [-0.33512159],\n",
       "       [-0.19919612]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute z2 (values of the output layer before applying the softmax function)\n",
    "z2 = np.dot(W2, h) + b2\n",
    "\n",
    "# Print z2\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "765428f4acc9fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute y_hat (z2 after applying softmax function)\n",
    "y_hat = softmax(z2)\n",
    "\n",
    "# Print y_hat\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5f16d92ab9cbb",
   "metadata": {},
   "source": [
    "Cross-entropy loss\n",
    "\n",
    "Now that you have the network's prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n",
    "\n",
    "> Remember that you are working on a single training example, not on a batch of examples, which is why you are using *loss* and not *cost*, which is the generalized form of loss.\n",
    "\n",
    "First let's recall what the prediction was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "957256954d4fa92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print prediction\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c4c2d8b6eabea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print target value\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc1a00fb9d2a93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    # Fill the loss variable with your code\n",
    "    loss = np.sum(-np.log(y_hat)*y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be51ed25404c021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print value of cross entropy loss for prediction and target value\n",
    "cross_entropy_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea877bc2361f7f8a",
   "metadata": {},
   "source": [
    "This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.\n",
    "\n",
    "The actual learning will start during the next phase: backpropagation.\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "The formulas that you will implement for backpropagation are the following.\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n",
    "\\end{align}\n",
    "\n",
    "> Note: these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.\n",
    "\n",
    "Let's start with an easy one.\n",
    "\n",
    "**Calculate the partial derivative of the loss function with respect to $\\mathbf{b_2}$, and store the result in `grad_b2`.**\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32e453feda0d57ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18519074],\n",
       "       [ 0.19245626],\n",
       "       [-0.76892554],\n",
       "       [ 0.18236353],\n",
       "       [ 0.20891502]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute vector with partial derivatives of loss function with respect to b2\n",
    "grad_b2 = y_hat - y\n",
    "\n",
    "# Print this vector\n",
    "grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fcc0304cce85e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06756476,  0.11798563,  0.        ],\n",
       "       [ 0.0702155 ,  0.12261452,  0.        ],\n",
       "       [-0.28053384, -0.48988499,  0.        ],\n",
       "       [ 0.06653328,  0.1161844 ,  0.        ],\n",
       "       [ 0.07622029,  0.13310045,  0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute matrix with partial derivatives of loss function with respect to W2\n",
    "grad_W2 = np.dot(y_hat - y, h.T)\n",
    "\n",
    "# Print matrix\n",
    "grad_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cf2994d444962f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.17045858]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute vector with partial derivatives of loss function with respect to b1\n",
    "grad_b1 = relu(np.dot(W2.T, y_hat - y))\n",
    "\n",
    "# Print vector\n",
    "grad_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6453a7a3ab8ec6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute matrix with partial derivatives of loss function with respect to W1\n",
    "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n",
    "\n",
    "# Print matrix\n",
    "grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9960f21e6a6e6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (5, 3) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W2.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65a2340cafe689",
   "metadata": {},
   "source": [
    "**Gradient descent**\n",
    "\n",
    "During the gradient descent phase, you will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n",
    " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
    " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
    " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
    "\\end{align}\n",
    "\n",
    "First, let set a value for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd3c331de71ca6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha\n",
    "alpha = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da880d99c871853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute updated W1\n",
    "W1_new = W1 - alpha * grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c0eb8a27a52e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "\n",
      "new value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "print('old value of W1:')\n",
    "print(W1)\n",
    "print()\n",
    "print('new value of W1:')\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27972af8672f3a4",
   "metadata": {},
   "source": [
    "The difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\n",
    "\n",
    "**Now calculate the new values of $\\mathbf{W_2}$ (to be stored in `W2_new`), $\\mathbf{b_1}$ (in `b1_new`), and $\\mathbf{b_2}$ (in `b2_new`).**\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
    " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
    " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b94cdb234306922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2_new\n",
      "[[-0.22384758 -0.43362588  0.13310965]\n",
      " [ 0.08265956  0.0775535   0.1772054 ]\n",
      " [ 0.19557112 -0.04637608 -0.1790735 ]\n",
      " [ 0.06855622 -0.02363691  0.36107434]\n",
      " [ 0.33251813 -0.3982269  -0.43959196]]\n",
      "\n",
      "b1_new\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27875802]]\n",
      "\n",
      "b2_new\n",
      "[[ 0.02964508]\n",
      " [-0.36970753]\n",
      " [-0.10468778]\n",
      " [-0.35349417]\n",
      " [-0.0764456 ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute updated W2\n",
    "W2_new = W2 - alpha * grad_W2\n",
    "\n",
    "# Compute updated b1\n",
    "b1_new = b1 - alpha * grad_b1\n",
    "\n",
    "# Compute updated b2\n",
    "b2_new = b2 - alpha * grad_b2\n",
    "\n",
    "\n",
    "print('W2_new')\n",
    "print(W2_new)\n",
    "print()\n",
    "print('b1_new')\n",
    "print(b1_new)\n",
    "print()\n",
    "print('b2_new')\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac851a36839870",
   "metadata": {},
   "source": [
    "### LAB: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b37b9f648891fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenized version of the corpus\n",
    "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
    "\n",
    "# Define V. Remember this is the size of the vocabulary\n",
    "V = 5\n",
    "\n",
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)\n",
    "\n",
    "\n",
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edce04b6254c1",
   "metadata": {},
   "source": [
    "**Extracting word embedding vectors**\n",
    "\n",
    "Once you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n",
    "\n",
    "Option 1: extract embedding vectors from $\\mathbf{W_1}$\n",
    "\n",
    "The first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n",
    "\n",
    "> Note: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here's how you would proceed after the training process is complete.\n",
    "\n",
    "For example $\\mathbf{W_1}$ is this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "957f3303cf86b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
       "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
       "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print W1\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2d76a8e51ebcf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# Print corresponding word for each index within vocabulary's range\n",
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d36e84b38eebc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.26637602]\n",
      "because: [ 0.08854191  0.22795148 -0.23846886]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.11399446]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42199056c78354d3",
   "metadata": {},
   "source": [
    "Option 2: extract embedding vectors from $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4bff88e6d076dfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
       "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
       "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print transposed W2\n",
    "W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b820c97f1e040101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.22182064 -0.43008631  0.13310965]\n",
      "because: [0.08476603 0.08123194 0.1772054 ]\n",
      "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      "i: [ 0.07055222 -0.02015138  0.36107434]\n",
      "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b75e4e5b4588",
   "metadata": {},
   "source": [
    "Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21658f2f7783571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
       "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
       "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute W3 as the average of W1 and W2 transposed\n",
    "W3 = (W1+W2.T)/2\n",
    "\n",
    "# Print W3\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e68c283660ff0aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.09752647 -0.05136565  0.19974284]\n",
      "because: [ 0.08665397  0.15459171 -0.03063173]\n",
      "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
      "i: [0.1768788  0.19580601 0.12353994]\n",
      "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # Extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    # Print word alongside word embedding vector\n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacc1416be4f4fb",
   "metadata": {},
   "source": [
    "### LAB: Assignment CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3581ef92d8c7664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelmateju/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils2) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pomocne_soubory.utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "\n",
    "nltk.download('punkt')\n",
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9dd00ff955308c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60976 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
     ]
    }
   ],
   "source": [
    "# Load, tokenize and process the data\n",
    "import re                                                           #  Load the Regex-modul\n",
    "with open('./pomocne_soubory/shakespeare.txt') as f:\n",
    "    data = f.read()                                                 #  Read in the data\n",
    "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
    "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "718db2ffad1b4d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d4945d9c9ce87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73c840ec82a9b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   2744\n",
      "Word which has index 2743:   kinds\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9232d50325908b",
   "metadata": {},
   "source": [
    "#### 2 - Training the Model\n",
    "\n",
    "<a name='2.1'></a>\n",
    " 2.1 - Initializing the Model\n",
    "\n",
    "You will now initialize two matrices and two vectors. \n",
    "- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n",
    "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
    "- Vector $b_1$ has dimensions $N\\times 1$\n",
    "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
    "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
    "\n",
    "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n",
    "\n",
    "Exercise 1 - initialize_model\n",
    "Please use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\n",
    "\n",
    "**Note:** In the next cell you will encounter a random seed. Please **DO NOT** modify this seed so your solution can be tested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a051ea91f2c615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N, V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "def638d2fa547b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test your function example.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2071e403ce953e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    # Calculate yhat (softmax)\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "708e6fc0174a577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.73105858, 0.88079708],\n",
       "       [0.5       , 0.26894142, 0.11920292]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp = np.array([[1,2,3],\n",
    "                [1,1,1]\n",
    "               ])\n",
    "tmp_sm = softmax(tmp)\n",
    "display(tmp_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b2e7a5bdcaab109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "    \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h = np.maximum(0, h)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d214a7b0737f87c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "call forward_prop\n",
      "\n",
      "z has shape (3, 1)\n",
      "z has values:\n",
      "[[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]]\n",
      "\n",
      "h has shape (2, 1)\n",
      "h has values:\n",
      "[[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "#print(tmp_x)\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# call function\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"h has shape {tmp_h.shape}\")\n",
    "print(\"h has values:\")\n",
    "print(tmp_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ebb2eafae6fc5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c233998e28588992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_x.shape (5775, 4)\n",
      "tmp_y.shape (5775, 4)\n",
      "tmp_W1.shape (50, 5775)\n",
      "tmp_W2.shape (5775, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5775, 1)\n",
      "tmp_z.shape: (5775, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "tmp_yhat.shape: (5775, 4)\n",
      "call compute_cost\n",
      "tmp_cost 10.4074\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "print(\"call compute_cost\")\n",
    "print(f\"tmp_cost {tmp_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b8c88661894497b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    \n",
    "    # Compute z1 as \"W1â‹…x + b1\"\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    \n",
    "  \n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "    \n",
    "    # if z1 < 0, then l1 = 0\n",
    "    # otherwise l1 = l1\n",
    "    # (this is already implemented for you)\n",
    "    l1[z1 < 0] = 0 # use \"l1\" to compute gradients below\n",
    "    \n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "    \n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    # compute gradient for b2\n",
    "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dcf382e9e025c91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "tmp_x.shape (5775, 4)\n",
      "tmp_y.shape (5775, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 5775)\n",
      "tmp_W2.shape (5775, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (5775, 1)\n",
      "\n",
      "Forwad prop to get z and h\n",
      "tmp_z.shape: (5775, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "\n",
      "Get yhat by calling softmax\n",
      "tmp_yhat.shape: (5775, 4)\n",
      "\n",
      "call back_prop\n",
      "tmp_grad_W1.shape (50, 5775)\n",
      "tmp_grad_W2.shape (5775, 50)\n",
      "tmp_grad_b1.shape (50, 1)\n",
      "tmp_grad_b2.shape (5775, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "\n",
    "# get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "print(\"get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Forwad prop to get z and h\")\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Get yhat by calling softmax\")\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_m = (2*tmp_C)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "print()\n",
    "print(\"call back_prop\")\n",
    "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6323052b286b7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
    "                     random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "    batch_size = 128\n",
    "#    batch_size = 512\n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    \n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):            \n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        \n",
    "        if ((iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "        \n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "defaebde1a62aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 8.586233\n",
      "iters: 20 cost: 4.143100\n",
      "iters: 30 cost: 16.994681\n",
      "iters: 40 cost: 1.915174\n",
      "iters: 50 cost: 10.884540\n",
      "iters: 60 cost: 7.958590\n",
      "iters: 70 cost: 5.037293\n",
      "iters: 80 cost: 10.089375\n",
      "iters: 90 cost: 10.565950\n",
      "iters: 100 cost: 13.304523\n",
      "iters: 110 cost: 3.570108\n",
      "iters: 120 cost: 4.042038\n",
      "iters: 130 cost: 11.994114\n",
      "iters: 140 cost: 6.098708\n",
      "iters: 150 cost: 2.167120\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "365ef72f37a640b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) [2744, 3949, 2960, 3022, 5672, 1452, 5671, 4189, 2315, 4276]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "         'rich','happy','sad']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15d37df6323a7e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"412.113835pt\" height=\"297.190125pt\" viewBox=\"0 0 412.113835 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-31T21:58:03.359356</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 297.190125 \n",
       "L 412.113835 297.190125 \n",
       "L 412.113835 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 401.965312 273.312 \n",
       "L 401.965312 7.2 \n",
       "L 44.845313 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m937d18b9ec\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pcdfc440aba)\">\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"211.934078\" y=\"261.216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"308.577449\" y=\"131.492114\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"385.732585\" y=\"155.751777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"105.933621\" y=\"139.524962\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"219.360041\" y=\"112.754034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"220.130281\" y=\"19.296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"61.07804\" y=\"167.96109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"163.500144\" y=\"177.897075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"325.963663\" y=\"140.06341\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m937d18b9ec\" x=\"324.427678\" y=\"207.836239\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"me5e86aabc3\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"54.648035\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- âˆ’1.00 -->\n",
       "      <g transform=\"translate(39.325379 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"99.151966\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- âˆ’0.75 -->\n",
       "      <g transform=\"translate(83.82931 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"143.655897\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- âˆ’0.50 -->\n",
       "      <g transform=\"translate(128.33324 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"188.159827\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- âˆ’0.25 -->\n",
       "      <g transform=\"translate(172.837171 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"232.663758\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(221.530945 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"277.167689\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(266.034876 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"321.671619\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(310.538807 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me5e86aabc3\" x=\"366.17555\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(355.042737 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m1f29c9d080\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"270.279532\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- âˆ’1.00 -->\n",
       "      <g transform=\"translate(7.2 274.078751) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"240.554467\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- âˆ’0.75 -->\n",
       "      <g transform=\"translate(7.2 244.353686) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"210.829401\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- âˆ’0.50 -->\n",
       "      <g transform=\"translate(7.2 214.62862) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"181.104336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- âˆ’0.25 -->\n",
       "      <g transform=\"translate(7.2 184.903554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"151.37927\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(15.579688 155.178489) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"121.654204\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(15.579688 125.453423) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"91.929139\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(15.579688 95.728358) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"62.204073\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(15.579688 66.003292) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f29c9d080\" x=\"44.845313\" y=\"32.479008\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(15.579688 36.278226) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 44.845313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 401.965312 273.312 \n",
       "L 401.965312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 401.965312 273.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 44.845313 7.2 \n",
       "L 401.965312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- king -->\n",
       "    <g transform=\"translate(211.934078 261.216) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"57.910156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"85.693359\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"149.072266\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- queen -->\n",
       "    <g transform=\"translate(308.577449 131.492114) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"188.378906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"249.902344\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- lord -->\n",
       "    <g transform=\"translate(385.732585 155.751777) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"128.328125\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- man -->\n",
       "    <g transform=\"translate(105.933621 139.524962) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"97.412109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"158.691406\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- woman -->\n",
       "    <g transform=\"translate(219.360041 112.754034) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
       "L 844 3500 \n",
       "L 1563 769 \n",
       "L 2278 3500 \n",
       "L 2956 3500 \n",
       "L 3675 769 \n",
       "L 4391 3500 \n",
       "L 4966 3500 \n",
       "L 4050 0 \n",
       "L 3372 0 \n",
       "L 2619 2869 \n",
       "L 1863 0 \n",
       "L 1184 0 \n",
       "L 269 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"142.96875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"240.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"301.660156\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_23\">\n",
       "    <!-- dog -->\n",
       "    <g transform=\"translate(220.130281 19.296) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-64\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"124.658203\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_24\">\n",
       "    <!-- wolf -->\n",
       "    <g transform=\"translate(61.07804 167.96109) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"142.96875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-66\" x=\"170.751953\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- rich -->\n",
       "    <g transform=\"translate(163.500144 177.897075) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-72\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"41.113281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"68.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"123.876953\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_26\">\n",
       "    <!-- happy -->\n",
       "    <g transform=\"translate(325.963663 140.06341) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-68\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-70\" x=\"124.658203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-70\" x=\"188.134766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"251.611328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_27\">\n",
       "    <!-- sad -->\n",
       "    <g transform=\"translate(324.427678 207.836239) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"52.099609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"113.378906\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pcdfc440aba\">\n",
       "   <rect x=\"44.845313\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1cdb3f702fced341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"424.013835pt\" height=\"297.190125pt\" viewBox=\"0 0 424.013835 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-05-31T21:58:03.499733</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 297.190125 \n",
       "L 424.013835 297.190125 \n",
       "L 424.013835 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 401.965312 273.312 \n",
       "L 401.965312 7.2 \n",
       "L 44.845313 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"mc13f71a41d\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pe5b929f0a7)\">\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"242.470155\" y=\"261.216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"142.847279\" y=\"131.492114\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"233.212629\" y=\"155.751777\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"350.203154\" y=\"139.524962\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"88.593196\" y=\"112.754034\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"187.305151\" y=\"19.296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"211.689142\" y=\"167.96109\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"61.07804\" y=\"177.897075\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"385.732585\" y=\"140.06341\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#mc13f71a41d\" x=\"119.334223\" y=\"207.836239\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mab56cda1af\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"71.308682\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- âˆ’0.6 -->\n",
       "      <g transform=\"translate(59.167276 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"114.95464\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- âˆ’0.4 -->\n",
       "      <g transform=\"translate(102.813234 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"158.600598\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- âˆ’0.2 -->\n",
       "      <g transform=\"translate(146.459192 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"202.246556\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(194.294993 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"245.892513\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(237.940951 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"289.538471\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(281.586908 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"333.184429\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(325.232866 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mab56cda1af\" x=\"376.830386\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(368.878824 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"md7193582b3\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"270.279532\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- âˆ’1.00 -->\n",
       "      <g transform=\"translate(7.2 274.078751) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"240.554467\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- âˆ’0.75 -->\n",
       "      <g transform=\"translate(7.2 244.353686) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"210.829401\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- âˆ’0.50 -->\n",
       "      <g transform=\"translate(7.2 214.62862) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"181.104336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- âˆ’0.25 -->\n",
       "      <g transform=\"translate(7.2 184.903554) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"151.37927\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(15.579688 155.178489) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"121.654204\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(15.579688 125.453423) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"91.929139\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(15.579688 95.728358) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"62.204073\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(15.579688 66.003292) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md7193582b3\" x=\"44.845313\" y=\"32.479008\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(15.579688 36.278226) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 44.845313 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 401.965312 273.312 \n",
       "L 401.965312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 44.845313 273.312 \n",
       "L 401.965312 273.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 44.845313 7.2 \n",
       "L 401.965312 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- king -->\n",
       "    <g transform=\"translate(242.470155 261.216) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6b\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"57.910156\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"85.693359\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"149.072266\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- queen -->\n",
       "    <g transform=\"translate(142.847279 131.492114) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-71\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"126.855469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"188.378906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"249.902344\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- lord -->\n",
       "    <g transform=\"translate(233.212629 155.751777) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"88.964844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"128.328125\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- man -->\n",
       "    <g transform=\"translate(350.203154 139.524962) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"97.412109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"158.691406\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_22\">\n",
       "    <!-- woman -->\n",
       "    <g transform=\"translate(88.593196 112.754034) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n",
       "L 844 3500 \n",
       "L 1563 769 \n",
       "L 2278 3500 \n",
       "L 2956 3500 \n",
       "L 3675 769 \n",
       "L 4391 3500 \n",
       "L 4966 3500 \n",
       "L 4050 0 \n",
       "L 3372 0 \n",
       "L 2619 2869 \n",
       "L 1863 0 \n",
       "L 1184 0 \n",
       "L 269 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"142.96875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"240.380859\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"301.660156\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_23\">\n",
       "    <!-- dog -->\n",
       "    <g transform=\"translate(187.305151 19.296) scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-64\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"63.476562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"124.658203\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_24\">\n",
       "    <!-- wolf -->\n",
       "    <g transform=\"translate(211.689142 167.96109) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"81.787109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"142.96875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-66\" x=\"170.751953\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_25\">\n",
       "    <!-- rich -->\n",
       "    <g transform=\"translate(61.07804 177.897075) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-72\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"41.113281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"68.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-68\" x=\"123.876953\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_26\">\n",
       "    <!-- happy -->\n",
       "    <g transform=\"translate(385.732585 140.06341) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-68\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"63.378906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-70\" x=\"124.658203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-70\" x=\"188.134766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"251.611328\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_27\">\n",
       "    <!-- sad -->\n",
       "    <g transform=\"translate(119.334223 207.836239) scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"52.099609\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"113.378906\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe5b929f0a7\">\n",
       "   <rect x=\"44.845313\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f1eb4a68443bf",
   "metadata": {},
   "source": [
    "### Continuous skip-gram / Skip-gram with negative sampling (SGNS) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e4ed79e60c4e6",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bdd7bf8e8a996",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e08e6c9a985208",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca687d9740568d2a",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8500f265ccc67c4",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a4d69bbd34e67",
   "metadata": {},
   "source": [
    "### GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95674d88fc92ea5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9638b5a425f633cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
